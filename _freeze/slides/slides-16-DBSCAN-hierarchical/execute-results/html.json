{
  "hash": "12181f2ccd774de63e50fae9a62acda6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'CPSC 330 Lecture 16: DBSCAN, Hierarchical Clustering'\ndescription: \"Unsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset,  importance of input data representation in clustering.\"\nformat:\n    revealjs:\n        html-math-method: plain\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\neditor:\n  render-on-save: true\n---\n\n\n## Announcements \n\n- HW5 extension: Was due yesterday \n- HW6 is due next week Wednesday. \n  - Computationally intensive \n  - You need to install many packages \n\n::: {#b5d68c8d .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-2-output-1.png){width=254 height=444}\n:::\n:::\n\n\n## iClicker Exercise 16.1 \n\n**Select all of the following statements which are TRUE.**\n\n- (A) Similar to K-nearest neighbours, K-Means is a non parametric model.\n- (B) The meaning of $K$ in K-nearest neighbours and K-Means clustering is very similar. \n- (C) Scaling of input features is crucial in clustering.  \n- (D) In clustering, it's almost always a good idea to find equal-sized clusters. \n\n# K-means Limitations\n\n## Shape of clusters\n- Good for spherical clusters of more or less equal sizes \n![](img/kmeans_boundaries.png)\n\n## K-Means: failure case 1\n\n- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). \n\n::: {#1c1a5e18 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-3-output-1.png){width=826 height=375}\n:::\n:::\n\n\n## K-Means: failure case 2\n\n- Again, K-Means is unable to capture complex cluster shapes. \n\n::: {#a8cdd876 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-4-output-1.png){width=818 height=375}\n:::\n:::\n\n\n## K-Means: failure case 3\n\n- It assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters. \n\n::: {#c79989db .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-5-output-1.png){width=807 height=375}\n:::\n:::\n\n\n# Can we do better? \n\n## DBSCAN\n\n- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise\n- A density-based clustering algorithm\n\n::: {#0069d63a .cell execution_count=5}\n``` {.python .cell-code}\nX, y = make_moons(n_samples=200, noise=0.08, random_state=42)\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(X)\nplot_original_clustered(X, dbscan, dbscan.labels_)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-6-output-1.png){width=974 height=375}\n:::\n:::\n\n\n## How does it work?\n![](img/DBSCAN_search.gif)\n\n## DBSCAN Analogy\n\nConsider DBSCAN in a social context: \n\n- Social butterflies (ü¶ã): Core points\n- Friends of social butterflies who are not social butterflies: Border points\n- Lone wolves (üê∫): Noise points  \n\n## Two main hyperparameters\n- `eps`: determines what it means for points to be \"close\"\n- `min_samples`: determines the number of **neighboring points** we require to consider in order for a point to be part of a cluster\n\n## DBSCAN: failure cases\n\n- Let's consider this dataset with three clusters of varying densities.  \n- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n\n::: {#8b61b9e0 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-7-output-2.png){width=1390 height=375}\n:::\n:::\n\n\n## Hierarchical clustering \n\n::: {#854c4603 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-8-output-1.png){width=1073 height=378}\n:::\n:::\n\n\n## Dendrogram \n\n\n\n## Flat clusters\n\n- This is good but how can we get cluster labels from a dendrogram? \n- We can bring the clustering to a \"flat\" format use `fcluster`\n\n::: {#cb53e3d9 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-16-DBSCAN-hierarchical_files/figure-revealjs/cell-9-output-1.png){width=948 height=378}\n:::\n:::\n\n\n## Linkage criteria {.smaller}\n- When we create a dendrogram, we need to calculate distance between clusters. How do we measure distances between clusters? \n- The **linkage criteria** determines how to find similarity between clusters:\n- Some example linkage criteria are: \n    - Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n    - Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n    - Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n    - Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters\n\n## Activity\n\n- Fill in the table below in this [Google doc]()\n\n| **Clustering Method**  | **KMeans**                                           | **DBSCAN**                                          | **Hierarchical Clustering**                            |\n|------------------------|------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------|\n| **Approach**           | | |\n| **Hyperparameters**    | | |\n| **Shape of clusters**  | | |\n| **Handling noise**     | | | \n| **Examples**           | | |\n\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_16-dbscan-hierarchical.ipynb)\n\n",
    "supporting": [
      "slides-16-DBSCAN-hierarchical_files"
    ],
    "filters": [],
    "includes": {}
  }
}