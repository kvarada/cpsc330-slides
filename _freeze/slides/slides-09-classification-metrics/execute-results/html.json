{
  "hash": "65933672bf297022d20c57c5ef20d638",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 9: Classification Metrics\" \nauthor: \"Varada Kolhatkar\"\ndescription: \"Metrics for classification\"\ndescription-short: \"confusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance\" \nformat:\n  revealjs:\n    html-math-method: mathjax\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Focus on the breath!\n\n![](img/inukshuk.jpeg){.nostretch fig-align=\"center\" width=\"600px\"}\n\n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/mekbcze4gyber/post/162\n  - **Good news for you: You'll have access to our course notes in the midterm!**\n- HW4 was due on Monday, Oct 6th 11:59 pm. \n- HW5 has been released. It's a project-type assignment and you get till Oct 27th to work on it.  \n\n\n\n## ML workflow \n![](img/ml-workflow.png)\n\n## Accuracy\n\n- So far, we‚Äôve been measuring model performance using **Accuracy**.  \n- **Accuracy** is the proportion of all predictions that were correct ‚Äî whether *positive* or *negative*.  \n\n$$\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{total classifications}}\n$$\n\n- But is **accuracy** always the right metric to evaluate a model? ü§î  \n\n## A fraud classification example\n\n::: {#2850a85d .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n(199364, 30)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Time</th>\n      <th>Amount</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>...</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>64454</th>\n      <td>0</td>\n      <td>51150.0</td>\n      <td>1.00</td>\n      <td>-3.538816</td>\n      <td>3.481893</td>\n      <td>-1.827130</td>\n      <td>-0.573050</td>\n      <td>2.644106</td>\n      <td>-0.340988</td>\n      <td>2.102135</td>\n      <td>...</td>\n      <td>-1.509991</td>\n      <td>1.345904</td>\n      <td>0.530978</td>\n      <td>-0.860677</td>\n      <td>-0.201810</td>\n      <td>-1.719747</td>\n      <td>0.729143</td>\n      <td>-0.547993</td>\n      <td>-0.023636</td>\n      <td>-0.454966</td>\n    </tr>\n    <tr>\n      <th>37906</th>\n      <td>0</td>\n      <td>39163.0</td>\n      <td>18.49</td>\n      <td>-0.363913</td>\n      <td>0.853399</td>\n      <td>1.648195</td>\n      <td>1.118934</td>\n      <td>0.100882</td>\n      <td>0.423852</td>\n      <td>0.472790</td>\n      <td>...</td>\n      <td>0.810267</td>\n      <td>-0.192932</td>\n      <td>0.687055</td>\n      <td>-0.094586</td>\n      <td>0.121531</td>\n      <td>0.146830</td>\n      <td>-0.944092</td>\n      <td>-0.558564</td>\n      <td>-0.186814</td>\n      <td>-0.257103</td>\n    </tr>\n    <tr>\n      <th>79378</th>\n      <td>0</td>\n      <td>57994.0</td>\n      <td>23.74</td>\n      <td>1.193021</td>\n      <td>-0.136714</td>\n      <td>0.622612</td>\n      <td>0.780864</td>\n      <td>-0.823511</td>\n      <td>-0.706444</td>\n      <td>-0.206073</td>\n      <td>...</td>\n      <td>0.258815</td>\n      <td>-0.178761</td>\n      <td>-0.310405</td>\n      <td>-0.842028</td>\n      <td>0.085477</td>\n      <td>0.366005</td>\n      <td>0.254443</td>\n      <td>0.290002</td>\n      <td>-0.036764</td>\n      <td>0.015039</td>\n    </tr>\n    <tr>\n      <th>245686</th>\n      <td>0</td>\n      <td>152859.0</td>\n      <td>156.52</td>\n      <td>1.604032</td>\n      <td>-0.808208</td>\n      <td>-1.594982</td>\n      <td>0.200475</td>\n      <td>0.502985</td>\n      <td>0.832370</td>\n      <td>-0.034071</td>\n      <td>...</td>\n      <td>-1.009429</td>\n      <td>-0.040448</td>\n      <td>0.519029</td>\n      <td>1.429217</td>\n      <td>-0.139322</td>\n      <td>-1.293663</td>\n      <td>0.037785</td>\n      <td>0.061206</td>\n      <td>0.005387</td>\n      <td>-0.057296</td>\n    </tr>\n    <tr>\n      <th>60943</th>\n      <td>0</td>\n      <td>49575.0</td>\n      <td>57.50</td>\n      <td>-2.669614</td>\n      <td>-2.734385</td>\n      <td>0.662450</td>\n      <td>-0.059077</td>\n      <td>3.346850</td>\n      <td>-2.549682</td>\n      <td>-1.430571</td>\n      <td>...</td>\n      <td>0.157993</td>\n      <td>-0.430295</td>\n      <td>-0.228329</td>\n      <td>-0.370643</td>\n      <td>-0.211544</td>\n      <td>-0.300837</td>\n      <td>-1.174590</td>\n      <td>0.573818</td>\n      <td>0.388023</td>\n      <td>0.161782</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n## `DummyClassifier`\nLet's try a DummyClassifier, which makes predictions without learning any patterns.\n\n::: {#1a10b340 .cell execution_count=3}\n``` {.python .cell-code}\ndummy = DummyClassifier()\npd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True)).mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nfit_time       0.017052\nscore_time     0.002711\ntest_score     0.998300\ntrain_score    0.998300\ndtype: float64\n```\n:::\n:::\n\n\n- The accuracy looks surprisingly high!\n- Should we be happy with this model and deploy it?\n\n## Problem: Class imbalance \n\n- In many real-world problems, some classes are much rarer than others.\n\n- A model that always predicts \"no fraud\" could still achieve >99% accuracy!\n- This is why accuracy can be misleading in imbalanced datasets.\n- We need metrics that differentiate types of errors.\n\n## Fraud Confusion matrix\n\nWhich types of errors would be most critical for the bank to address?\n\n- Missing a fraud case?\n\n- Or flagging a legitimate transaction as fraud?\n\n![](img/fraud-confusion-matrix.png)\n\n\n## Understanding the confusion matrix\n:::: {.columns}\n:::{.column width=\"80%\"}\n![](img/tp-fp-tn-fn-fraud.png)\n:::\n\n:::{.column width=\"20%\"}\n- TN $\\rightarrow$ True negatives \n- FP $\\rightarrow$ False positives \n- FN $\\rightarrow$ False negatives\n- TP $\\rightarrow$ True positives \n:::\n::::\n\n# Practice: confusion matrix terminology\n\n## Confusion matrix questions \n\nImagine a spam filter model where emails labeled **1 = spam, 0 = not spam**. \n\nIf a spam email is incorrectly classified as not spam, what kind of error is this?\n\n- (A) A false positive\n- (B) A true positive\n- (C) A false negative\n- (D) A true negative\n\n## Confusion matrix questions\n\nIn an intrusion detection system, **1 = intrusion, 0 = safe**. \n\nIf the system misses an actual intrusion and classifies it as safe, this is a:\n\n- (A) A false positive\n- (B) A true positive\n- (C) A false negative\n- (D) A true negative\n\n## Confusion matrix questions\n\nIn a medical test for a disease, **1 = diseased, 0 = healthy**. \n\nIf a healthy patient is incorrectly diagnosed as diseased, that's a:\n\n- (A) A false positive\n- (B) A true positive\n- (C) A false negative\n- (D) A true negative\n\n## Metrics other than accuracy \n\nNow that we understand the different types of errors, we can explore metrics that better capture model performance when **accuracy falls short**, especially for **imbalanced datasets**.\n\nWe'll start with three key ones:\n\n- **Precision**\n- **Recall**\n- **F1-score**\n\n- Precision\n- Recall\n- F1-score \n\n## Precision and recall \n\nLet's revisit our fraud detection scenario. The circle below represents **all transactions predicted as fraud** by an **imaginary toy model** designed to detect fraudulent activity.\n\n![](img/precision-recall.png){.nostretch fig-align=\"center\" width=\"600px\"}\n![](img/fraud-precision-recall.png){.nostretch fig-align=\"center\" width=\"600px\"}\n\n## Intuition behind the two metrics\n\n- **Precision** answers:  \n  > *Of all the transactions predicted as fraud, how many were actually fraud?*  \n  High precision $\\rightarrow$ few false alarms (low false positives).\n\n- **Recall** answers:  \n  > *Of all the actual fraud cases, how many did the model catch?*  \n  High recall $\\rightarrow$ few missed frauds (low false negatives).\n\n## Trade-off between precision and recall\n\n- Increasing **recall** often decreases **precision**, and vice versa.  \n- Example:  \n  - Predict *‚Äúfraud‚Äù* for every transaction $rightarrow$ perfect recall, terrible precision.  \n  - Predict *‚Äúfraud‚Äù* only when 100% sure $rightarrow$ high precision, low recall.\n\n**The right balance depends on the application and cost of errors.**\n\n## F1-score\n\n- Sometimes, we want a **single metric** that balances precision and recall.  \n- The **F1-score** is the **harmonic mean** of the two:\n\n$$\nF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n- High **F1** means both precision and recall are strong.  \n- Useful when we care about both false positives **and** false negatives.\n\n## Summary\n\n| Metric | What it measures | High value means |\n|:--------|:----------------|:------------------|\n| **Accuracy** | Overall correctness | Model gets most predictions right |\n| **Precision** | Quality of positive predictions | Few false alarms |\n| **Recall** | Quantity of true positives caught | Few missed positives |\n| **F1-score** | Balance of precision & recall | Both precision and recall are high |\n\n## iClicker Exercise 9.1\n\n**Select all of the following statements which are TRUE.**\n\n- (A) In medical diagnosis, false positives are more damaging than false negatives (assume \"positive\" means the person has a disease, \"negative\" means they don't).\n- (B) In spam classification, false positives are more damaging than false negatives (assume \"positive\" means the email is spam, \"negative\" means they it's not).\n- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.\n- (D) If method A gets a higher accuracy than method B, that means its recall is also higher.\n\n## Counter examples\n\nMethod A - higher accuracy but lower precision\n\n| Negative | Positive\n| -------- |:-------------:|\n| 90      | 5|\n| 5      | 0|\n\nMethod B - lower accuracy but higher precision\n\n| Negative | Positive\n| -------- |:-------------:|\n| 80      | 15|\n| 0      | 5|\n\n## Thresholding \n\n- The above metrics assume a fixed threshold. \n- We use thresholding to get the binary prediction. \n- A typical threshold is 0.5.\n    - A prediction of 0.90 $\\rightarrow$ a high likelihood that the transaction is fraudulent and we predict **fraud**\n    - A prediction of 0.20 $\\rightarrow$ a low likelihood that the transaction is non-fraudulent and we predict **Non fraud**\n- **What happens if the predicted score is equal to the chosen threshold?**\n\n- [Play with classification thresholds](https://developers.google.com/machine-learning/crash-course/classification/thresholding)\n\n\n## iClicker Exercise 9.2\n\n**Select all of the following statements which are TRUE.**\n\n- (A) If we increase the classification threshold, both true and false positives are likely to decrease.\n- (B) If we increase the classification threshold, both true and false negatives are likely to decrease.\n- (C) Lowering the classification threshold generally increases the model‚Äôs recall.  \n- (D) Raising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives.\n\n\n## PR curve\n- Calculate precision and recall (TPR) at every possible threshold and graph them. \n- Better choice for highly imbalanced datasets \n\n![](img/pr-curve-example.png)\n\n\n## ROC curve \n- Calculate the true positive rate (TPR) and false positive rate (FPR) at every possible thresholding and graph TPR over FPR. \n- Good choice when the datasets are roughly balanced. \n![](img/roc-curve-example.png)\n\n## AUC \n- The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.\n\n\n## ROC AUC questions\n\nConsider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?\n\n:::: {.columns}\n\n:::{.column width=\"50%\"}\n![](img/auc_abc)\n:::\n\n:::{.column width=\"50%\"}\n\n- (A) If false positives (false alarms) are highly costly\n- (B) If false positives are cheap and false negatives (missed true positives) highly costly\n- (C) If the costs are roughly equivalent\n:::\n::::\n\n[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n\n",
    "supporting": [
      "slides-09-classification-metrics_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}