{
  "hash": "ad76a493f2a0662c970ef54af8b477f8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 7: Linear models\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Linear regression, logistic regression\"\ndescription-short: \"Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Where to find slides? \n  - https://kvarada.github.io/cpsc330-slides/lecture.html\n- HW3 is due next week Tuesday, Oct 1st, 11:59 pm. \n  - You can work in pairs for this assignment. \n\n\n\n## Recap: Dealing with text features \n- Preprocessing text to fit into machine learning models using text vectorization.\n- Bag of words representation \n![](img/bag-of-words.png)\n\n## Recap: `sklearn` `CountVectorizer`\n- Use `scikit-learn`’s `CountVectorizer` to encode text data\n- `CountVectorizer`: Transforms text into a matrix of token counts\n- Important parameters:\n  - `max_features`: Control the number of features used in the model \n  - `max_df`, `min_df`: Control document frequency thresholds\n  - `ngram_range`: Defines the range of n-grams to be extracted\n  - `stop_words`: Enables the removal of common words that are typically uninformative in most applications, such as “and”, “the”, etc.\n\n## Recap: Incorporating text features in a machine learning pipeline\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipeline = make_pipeline(\n    CountVectorizer(),\n    SVC()\n)\n```\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_05-06-preprocessing.ipynb)\n\n\n## (iClicker) Exercise 6.2\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) `handle_unknown=\"ignore\"` would treat all unknown categories equally.\n- (B) As you increase the value for `max_features` hyperparameter of `CountVectorizer` the training score is likely to go up.\n- (C) Suppose you are encoding text data using `CountVectorizer`. If you encounter a word in the validation or the test split that's not available in the training data, we'll get an error.\n- (D) In the code below, inside `cross_validate`, each fold might have slightly different number of features (columns) in the fold.\n\n```python\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)\n```\n\n## Linear models \n\n:::: {.columns}\n\n:::{.column width=\"45%\"}\n- Linear models make an assumption that the relationship between `X` and `y` is linear. \n- In this case, with only one feature, our model is a straight line.\n- What do we need to represent a line?\n  - Slope ($w_1$): Determines the angle of the line.\n  - Y-intercept ($w_0$): Where the line crosses the y-axis.\n\n:::\n\n\n::: {.column width=\"55%\"}\n\n::: {#4db3c28a .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-07-linear-models_files/figure-revealjs/cell-3-output-1.png){width=677 height=399}\n:::\n:::\n\n\n- Making predictions\n  - $\\hat{y} = w_1 \\times \\text{# hours studied} + w_0$\n\n:::\n\n::::\n\n## `Ridge` vs. `LinearRegression`\n\n## Logistic Regression \n\n- Suppose your target is binary: pass or fail \n\n::: {#52be8438 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-07-linear-models_files/figure-revealjs/cell-4-output-1.png){width=684 height=473}\n:::\n:::\n\n\n## Sigmoid \n\n## Parametric vs. non-Parametric models \n\n## (iClicker) Exercise 7.1\n\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) Increasing the hyperparameter alpha of Ridge is likely to decrease model complexity.\n- (B) Ridge can be used with datasets that have multiple features.\n- (C) With Ridge, we learn one coefficient per training example.\n- (D) If you train a linear regression model on a 2-dimensional problem (2 features), the model will learn 3 parameters: one for each feature and one for the bias term.\n\n\n## (iClicker) Exercise 7.2\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) Increasing logistic regression’s `C` hyperparameter increases model complexity.\n- (B) The raw output score can be used to calculate the probability score for a given prediction.\n- (C) For linear classifier trained on $d$ features, the decision boundary is a $d-1$-dimensional hyperparlane.\n- (D) A linear model is likely to be uncertain about the data points close to the decision boundary.\n\n# [Class demo]()\n\n",
    "supporting": [
      "slides-07-linear-models_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}