{
  "hash": "7ddc02a6fb0f506e85af3257f1b676da",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 18: Introduction to deep learning and computer vision\"\nauthor: \"Varada Kolhatkar\"\nformat: \n    revealjs:\n      html-math-method: mathjax\n      slide-number: true\n      center: true\n      logo: img/UBC-CS-logo.png\n      resources:\n        - data/\n        - img/        \n---\n\n\n\n## Announcements\n\n- HW7 was due yesterday \n- HW8 has been released (due next week Monday)\n  - Almost there! Hang in there üòä\n- Midterm 2 grading is in progress. \n\n# iClicker 18.0\n\n**Select all of the following statements which are TRUE for you.**\n\n- (A) I found the multiple-choice questions challenging.\n- (B) The coding questions took a lot of time.\n- (C) I didn't like the format of the midterm.\n- (D) I appreciated the mix of coding, conceptual, and multiple-choice questions.\n- (E) I felt the midterm was a good reflection of what we cover in the lectures and homework assignments.\n\n## iClicker 18.1 {.smaller}\n\n**Select all of the following statements which are TRUE.**\n\n- (A) It's possible to use word2vec embedding representations for text classification instead of bag-of-words representation. \n- (B) The topic model approach we used in the last lecture, Latent Dirichlet Allocation (LDA), is an unsupervised approach. \n- (C) In an LDA topic model, the same word can be associated with two different topics with high probability.\n- (D) In an LDA topic model, a document is a mixture of multiple topics. \n- (E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output. \n\n# Multiclass classification \n\n## What is multiclass classification? \n\n- So far, we've focused on binary classification (two classes).\n- But many real-world problems have more than two classes:\n  - Classifying types of emotions in text\n  - Identifying species of flowers\n  - Recognizing objects in an image\n\n**Goal: assign each example to exactly one of $K$ possible classes.**\n\n## How to handle multiple classes? \n\nConsider this dataset with multiple classes. \n\n```python\n\ntrain_df[\"target\"].value_counts(normalize=True)\n\ntarget\naffection           0.342571\nachievement         0.300799\nbonding             0.127238\nenjoy_the_moment    0.105694\nleisure             0.090927\nnature              0.018307\nexercise            0.014463\n```\n\n- ‚ùì Can we use Decision Trees or KNNs for multiclass classification without any significant modifications?\n\n## Extending logistic regression {.smaller}\n\n- What about logistic regression? The binary version works only for two classes.\n- For multiclass problems, we use the multinomial (softmax) logistic regression model.\n- It learns a separate weight vector and bias for each class:\n  - Parameters per class: $d$ weights + 1 bias\n\t- Total parameters: $(d + 1) \\times K$\n\n**Example:**\n\nIf we have 10,000 features (e.g., vocabulary size) and 7 moment classes, the model learns $(10{,}000 + 1) \\times 7$ parameters.\n\n## Softmax function\n\n- In binary logistic regression, we use the sigmoid function to map scores to probabilities.\n- In multiclass, we need:\n\t- Probabilities that are positive, and\n\t- Sum to 1 across all classes.\n\nThe softmax function does exactly that.\n\n## (Optional) Softmax function \n\nGiven an input, the probability that it belongs to class $j \\in \\{1, 2, \\dots, K\\}$ is calculated using the **softmax function**:\n\n$P(y = j \\mid x_i) = \\frac{e^{w_j^\\top x_i + b_j}}{\\sum_{k=1}^{K} e^{w_k^\\top x_i + b_k}}$\n\n- $x_i$ is the $i^{th}$ example \n- $w_j$ is the weight vector for class $j$.\n- $b_j$ is the bias term for class $j$.\n- $K$ is the total number of classes.\n\n\n## Making predictions\n\n1. **Compute Probabilities**:  \n   For each class $j$, compute the probability $P(y = j \\mid x_i)$ using the softmax function.\n\n2. **Select the Class with the Highest Probability**:  \n   The predicted class $\\hat{y}$ is:  \n   $\\hat{y} = \\arg \\max_{j \\in \\{1, \\dots, K\\}} P(y = j \\mid x_i)$\n\n## Example: Softmax output {.smaller}\n\n**Query: ‚ÄúI love my students!‚Äù**\n\n| Class              | Raw Score (Logit) | Exponentiated | Normalized Probability |\n|:-------------------|------------------:|---------------:|-----------------------:|\n| ‚ù§Ô∏è affection        | 2.1  | 8.166 | 0.4684 |\n| üèÜ achievement      | 1.8  | 6.050 | 0.3472 |\n| ü§ù bonding          | 1.0  | 2.718 | 0.1559 |\n| üåÖ enjoy_the_moment | 0.3  | 1.350 | 0.0775 |\n| üéÆ leisure          | -0.2 | 0.819 | 0.0470 |\n| üåø nature           | -1.0 | 0.368 | 0.0211 |\n| üèÉ exercise         | -1.5 | 0.223 | 0.0128 |\n| **Total**           | ‚Äî    | ‚Äî     | **1.000** |\n\n\n\n## Binary vs multinomial logistic regression {.smaller}\n\n|   **Aspect**                       | **Binary Logistic Regression**              | **Multinomial Logistic Regression**  |\n|--------------------------|---------------------------------------------|--------------------------------------|\n| **Target variable**      | 2 classes (binary)                          | More than 2 classes (multi-class)    |\n| **Getting probabilities**  | Sigmoid                                   | Softmax                              |\n| parameters               | $d$ weights, one per feature and the bias term | $d$ weights and a bias term per class |  \n| **Output**               | Single probability                          | Probability distribution over classes |\n| **Use case**             | Binary classification (e.g., spam detection) | Multi-class classification (e.g., flower species) |\n\n\n# Deep Learning \n\n## Remember this picture we saw earlier? \n\n- **Deep Learning (DL)** is a subset of machine learning \n\n![](img/ai-ml-dl.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n## Image classification\n\\\n\nHave you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\nThis can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n\n## Image classification\n\\\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n\n![](img/cat_variation.png)\n<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n\n## Neural networks {.smaller}\n\\\n\n- Neural networks are perfect for these types of problems where local structures are important. \n- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n  - [ImageNet Classification with Deep Convolutional\nNeural Networks](https://dl.acm.org/doi/10.1145/3065386)\n  - Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n- Let's go over the basics of a neural network.\n\n## A graphical view of a linear model\n:::: {.columns}\n\n:::{.column width=\"30%\"}\n\n::: {#979fd33e .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](slides-18-computer-vision_files/figure-revealjs/cell-3-output-1.svg){}\n:::\n:::\n\n\n:::\n\n:::{.column width=\"70%\"}\n- Remember this graphical view of linear models? \n- We have 4 features: x[0], x[1], x[2], x[3]\n- The output is calculated as $y = x[0]w[0] + x[1]w[1] + x[2]w[2] + x[3]w[3]$\n- For simplicity, we are ignoring the bias term. \n:::\n::::\n\n\n## Introduction to neural networks\n\\\n\n- Neural networks can be viewed a generalization of linear models where we apply a series of transformations.\n- Below we are adding one \"layer\" of transformations in between features and the target. \n- We are repeating the the process of computing the weighted sum multiple times.  \n- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n\n::: {#7ebb9c1e .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](slides-18-computer-vision_files/figure-revealjs/cell-4-output-1.svg){}\n:::\n:::\n\n\n## One more layer of transformations \n\\\n\n- Now we are adding one more layer of transformations. \n\n::: {#885206fd .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](slides-18-computer-vision_files/figure-revealjs/cell-5-output-1.svg){}\n:::\n:::\n\n\n## Neural networks {.smaller}\n\\\n\n- With a neural net, you specify the number of features after each transformation.\n  - In the above, it goes from 4 to 3 to 3 to 1.\n\n- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n- Neural network = neural net\n- Deep learning ~ using neural networks\n\n## Neural networks example and terminology\n\n![](img/nn-10.png)\n\n## (Optional) How does training work in neural networks? (High-Level) {.smaller}\n\nTraining a neural network has **two main steps:**\n\n- **Forward pass:**  \n  Feed the input through the network to calculate the predicted output using the current parameter values (weights).\n\n- **Backward pass:**  \n  - Measure how far the predicted output is from the actual target (this is the **loss**).\n  - Calculate how to adjust each parameter to improve future predictions (**gradients**).\n  - Update the parameters using these gradients ‚Äî this helps the model improve.\n\n- Summary\n  - **Input $\\rightarrow$ Forward Pass $\\rightarrow$ Loss $\\rightarrow$ Backward Pass $\\rightarrow$ Parameter Update $\\rightarrow$ Repeat**\n  - The model repeats this process many times, gradually improving its predictions.\n\n## Why neural networks? {.smaller}\n\n- They can learn very complex functions.\n  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n  - More layers / bigger layers --> more complex model.\n  - You can generally get a model that will not underfit. \n\n- They work really well for structured data:\n  - 1D sequence, e.g. timeseries, language\n  - 2D image\n  - 3D image or video\n- They've had some incredible successes in the last 12 years.\n- Transfer learning (coming later today) is really useful.  \n\n## Why not neural networks? {.smaller}\n\n- Often they require a lot of data.\n- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n- They have huge numbers of hyperparameters\n  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n  - Being slow compounds this problem.\n- They are not interpretable.\n- I don't recommend training them on your own without further training\n- Good news\n    - You don't have to train your models from scratch in order to use them.\n    - I'll show you some ways to use neural networks without training them yourselves. \n\n## Deep learning software\n\nThe current big players are:\n\n1. [PyTorch](http://pytorch.org)\n2. [TensorFlow](https://www.tensorflow.org)\n\nBoth are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n\n<br><br>\n\n## Introduction to computer vision\n\\\n\n- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. \n- In the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n\n## Introduction to computer vision {.smaller}\n\\\n\n- image classification: is this a cat or a dog?\n- object localization: where is the cat in this image?\n- object detection: What are the various objects in the image? \n- instance segmentation: What are the shapes of these various objects in the image? \n- and much more...\n\n![](img/vision-apps.jpeg)\n\n<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n\n## Convolutional Neural Networks (CNNs) {.smaller}\n- Neural networks come in different shapes depending on the type of data and the task.\n- CNNs are commonly used in **image classification, object detection, and medical imaging.**\n- What's the problem if we use the above feedforward architecture to learn patterns from images? \n\n![](img/cnn-1.png){.nostretch fig-align=\"center\" width=\"400px\"}\n\n## Filters \n\n- Uses filters that \"slide\" over the input to detect local patterns.\n\n![](img/cnn-4.png)\n\n- Play around with filters: https://setosa.io/ev/image-kernels/\n\n## CNNs \n\n![](img/cnn-5.gif)\n\n## CNNs big picture\n\n![](img/cnn_big_picture.png)\n\n---\n\n## Combining CNN and NN {.smaller}\n\n- Sometimes you'll want to combine different types of data in a single network\n- The most common case is combining tabular data with image data, for example, using both real estate data and images of a house to predict its sale price:\n\n![](img/multi-input.png)\n\nSource: \"[House](https://www.flickr.com/photos/68089229@N06/17458373552)\" by [oatsy40](https://www.flickr.com/photos/68089229@N06), \"[House in Vancouver](https://www.flickr.com/photos/17573364@N00/433449690)\" by [pnwra](https://www.flickr.com/photos/17573364@N00), \"[House](https://www.flickr.com/photos/21098413@N04/5405425139)\" by [noona11](https://www.flickr.com/photos/21098413@N04) all licensed under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=rich).\n\n\n## Pre-trained models {.smaller}\n\\\n\n- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n- It refers to using a model already trained on one task as a starting point for learning to perform another task.\n\n## Pre-trained models out-of-the-box \n\\\n\n![](img/cnn-ex.png)\n\n<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n\n- Let's first apply one of these pre-trained models to our own problem right out of the box. \n\n\n## Pre-trained models out-of-the-box {.smaller}\n\\\n\n- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n- We preprocess the given image. \n- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n\n## Pre-trained models out-of-the-box {.scrollable}\n\n- Let's predict labels with associated probabilities for unseen images\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#fbe301e4 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-6-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-6-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-6-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-6-output-7.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n\n## Pre-trained models out-of-the-box {.smaller}\n\\\n\n- We got these predictions without \"doing the ML ourselves\".\n- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n\n## Pre-trained models out-of-the-box {.smaller}\n\\\n\n- Let's try some images which are unlikely to be there in ImageNet. \n- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#54505367 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-7.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-9.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-7-output-11.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n## Pre-trained models out-of-the-box\n\\\n\n- Here we used pre-trained models out-of-the-box. \n- Can we use pre-trained models for our own classification problem with our classes? \n- Yes!! We have two options here:\n    1. Add some extra layers to the pre-trained network to suit our particular task\n    2. Pass training data through the network and save the output to use as features for training some other model\n\n\n## Pre-trained models to extract features {.smaller}\n\\\n\n- Let's use pre-trained models to extract features.\n- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n- You can think of each layer a transformer applying some transformations on the input received to that later. \n\n![](img/cnn-ex.png){.nostretch fig-align=\"center\" width=\"600px\"}\n\n\n## Pre-trained models to extract features \n\\\n\n- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n- Let's try this out. \n- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n\n## Pre-trained models to extract features \n\\\n\nLet's look at some sample images in the dataset. \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#58e66a16 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-8-output-1.png){}\n:::\n:::\n\n\n:::\n\n## Dataset statistics\n\\\n\nHere is the stat of our toy dataset. \n\n::: {#f0bd73e5 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)\n```\n:::\n:::\n\n\n## Pre-trained models to extract features \n\\\n\n- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n\n\n\n## Shape of the feature vector {.smaller}\n\\\n\n- Now we have extracted feature vectors for all examples. What's the shape of these features?\n\n::: {#9133b4a3 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntorch.Size([283, 1024])\n```\n:::\n:::\n\n\n- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n\n![](img/densenet-architecture.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n\n## A feature vector given by densenet \n\\ \n\n- Let's examine the feature vectors. \n\n::: {#541acdb9 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1014</th>\n      <th>1015</th>\n      <th>1016</th>\n      <th>1017</th>\n      <th>1018</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000290</td>\n      <td>0.003821</td>\n      <td>0.005015</td>\n      <td>0.001307</td>\n      <td>0.052690</td>\n      <td>0.063403</td>\n      <td>0.000626</td>\n      <td>0.001850</td>\n      <td>0.256254</td>\n      <td>0.000223</td>\n      <td>...</td>\n      <td>0.229935</td>\n      <td>1.046375</td>\n      <td>2.241259</td>\n      <td>0.229641</td>\n      <td>0.033674</td>\n      <td>0.742792</td>\n      <td>1.338698</td>\n      <td>2.130880</td>\n      <td>0.625475</td>\n      <td>0.463088</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000407</td>\n      <td>0.005973</td>\n      <td>0.003206</td>\n      <td>0.001932</td>\n      <td>0.090702</td>\n      <td>0.438523</td>\n      <td>0.001513</td>\n      <td>0.003906</td>\n      <td>0.166081</td>\n      <td>0.000286</td>\n      <td>...</td>\n      <td>0.910680</td>\n      <td>1.580815</td>\n      <td>0.087191</td>\n      <td>0.606904</td>\n      <td>0.436106</td>\n      <td>0.306456</td>\n      <td>0.940102</td>\n      <td>1.159818</td>\n      <td>1.712705</td>\n      <td>1.624753</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000626</td>\n      <td>0.005090</td>\n      <td>0.002887</td>\n      <td>0.001299</td>\n      <td>0.091715</td>\n      <td>0.548537</td>\n      <td>0.000491</td>\n      <td>0.003587</td>\n      <td>0.266537</td>\n      <td>0.000408</td>\n      <td>...</td>\n      <td>0.465152</td>\n      <td>0.678276</td>\n      <td>0.946387</td>\n      <td>1.194697</td>\n      <td>2.537747</td>\n      <td>1.642383</td>\n      <td>0.701200</td>\n      <td>0.115620</td>\n      <td>0.186433</td>\n      <td>0.166605</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000169</td>\n      <td>0.006087</td>\n      <td>0.002489</td>\n      <td>0.002167</td>\n      <td>0.087537</td>\n      <td>0.623212</td>\n      <td>0.000427</td>\n      <td>0.000226</td>\n      <td>0.460680</td>\n      <td>0.000388</td>\n      <td>...</td>\n      <td>0.394083</td>\n      <td>0.700158</td>\n      <td>0.105200</td>\n      <td>0.856323</td>\n      <td>0.038457</td>\n      <td>0.023948</td>\n      <td>0.131838</td>\n      <td>1.296370</td>\n      <td>0.723323</td>\n      <td>1.915215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000286</td>\n      <td>0.005520</td>\n      <td>0.001906</td>\n      <td>0.001599</td>\n      <td>0.186034</td>\n      <td>0.850148</td>\n      <td>0.000835</td>\n      <td>0.003025</td>\n      <td>0.036309</td>\n      <td>0.000142</td>\n      <td>...</td>\n      <td>3.313760</td>\n      <td>0.565744</td>\n      <td>0.473564</td>\n      <td>0.139446</td>\n      <td>0.029283</td>\n      <td>1.165938</td>\n      <td>0.442319</td>\n      <td>0.227593</td>\n      <td>0.884266</td>\n      <td>1.592698</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 1024 columns</p>\n</div>\n```\n:::\n:::\n\n\n- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n\n## Logistic regression with the extracted features \n\\\n\n- Let's try out logistic regression on these extracted features. \n\n::: {#cd14e8e9 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score:  1.0\n```\n:::\n:::\n\n\n::: {#14b1891e .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation score:  0.835820895522388\n```\n:::\n:::\n\n\n- This is great accuracy for so little data and little effort!!!\n\n\n## Sample predictions\n\\\n\nLet's examine some sample predictions on the validation set.  \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#0de3f1f2 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-15-output-1.png){}\n:::\n:::\n\n\n:::\n\n\n## Object detection {.smaller}\n\\\n\n- Another useful task and tool to know is object detection using YOLO model. \n- Let's identify objects in a sample image using a pretrained model called YOLO8. \n- List the objects present in this image.\n\n![](data/yolo_test/3356700488_183566145b.jpg){.nostretch fig-align=\"center\" width=\"400px\"}\n\n## Object detection using [YOLO](https://docs.ultralytics.com/)\n\\\n\nLet's try this out using a pre-trained model. \n\n::: {#92d51047 .cell execution_count=15}\n``` {.python .cell-code}\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n\nyolo_input = \"data/yolo_test/3356700488_183566145b.jpg\"\nyolo_result = \"data/yolo_result.jpg\"\n# Run batched inference on a list of images\nresult = model(yolo_input)  # return a list of Results objects\nresult[0].save(filename=yolo_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nimage 1/1 /Users/kvarada/CS/2025-26/330/cpsc330-slides/website/slides/data/yolo_test/3356700488_183566145b.jpg: 512x640 4 persons, 2 cars, 1 stop sign, 61.9ms\nSpeed: 2.4ms preprocess, 61.9ms inference, 7.8ms postprocess per image at shape (1, 3, 512, 640)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n'data/yolo_result.jpg'\n```\n:::\n:::\n\n\n## Object detection output \n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#626ff733 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](slides-18-computer-vision_files/figure-revealjs/cell-17-output-1.png){}\n:::\n:::\n\n\n:::\n\n## Summary {.smaller}\n\\\n\n- Neural networks are a flexible class of models.\n  - They are particular powerful for structured input like images, videos, audio, etc.\n  - They can be challenging to train and often require significant computational resources.\n- The good news is we can use pre-trained neural networks.\n  - This saves us a huge amount of time/cost/effort/resources.\n  - We can use these pre-trained networks directly or use them as feature transformers. \n\n",
    "supporting": [
      "slides-18-computer-vision_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}