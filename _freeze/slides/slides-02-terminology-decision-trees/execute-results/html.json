{
  "hash": "02f6b482d6dab7f7498355e1833d66bb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Lecture 2: Terminology, Baselines, Decision Trees'\nauthor: \"Varada Kolhatkar\"\ndescription: Terminology, decision Trees\n\nformat:\n    revealjs:\n        html-math-method: mathjax\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\n\n\neditor:\n  render-on-save: true\n---\n\n\n\n## ðŸŽ¯ Learning Outcomes {.smaller}\n\nBy the end of this lesson, you will be able to:\n\n- Define key machine learning terminology:  \n  *features, targets, predictions, training, error, classification vs. regression, supervised vs. unsupervised learning, hyperparameters vs. parameters, baselines, decision boundaries*  \n\n- Build a simple machine learning model in **scikit-learn**, explaining the `fit`â€“`predict` workflow and evaluating performance with the `score` method  \n\n- Describe at a high level how decision trees are trained (fitting) and how they make predictions  \n\n- Implement and visualize decision trees in scikit-learn using `DecisionTreeClassifier` and `DecisionTreeRegressor`  \n\n## Announcements \n\n- Things due this week \n    - Homework 1 (hw1): Due Sept 09 11:59pm \n- Homework 2 (hw2) has been released (Due: Sept 15, 11:59pm)\n    - There is some autograding in this homework. \n- You can find the tentative due dates for all deliverables [here](https://ubc-cs.github.io/cpsc330-2025W1/README.html#deliverable-due-dates-tentative).\n- Please monitor Piazza (especially pinned posts and instructor posts) for announcements. \n- I'll assume that you've watched the pre-lecture videos. \n\n## Recap: What is ML? \n\n- ML uses data to build models that find patterns, make predictions, or generate content.\n- It helps computers learn from data to make decisions.\n- No one model works for every situation.\n\n## iClicker 2.1: ML or not {.smaller}\n\niClicker join link: https://join.iclicker.com/FZMQ\n\n**Select all of the following statements which are suitable problems for machine learning.**\n\n- (A) Identifying objects within digital images, such as facial recognition in security systems or categorizing images based on content.\n- (B) Determining if individuals meet the necessary criteria for government or financial services based on strict guidelines.\n- (C) Identifying unusual patterns that may indicate fraudulent transactions in banking and finance.\n- (D) Automatically analyzing images from MRIs, CT scans, or X-rays to detect abnormalities like tumors or fractures.\n- (E) Addressing mental health issues where human empathy, understanding, and adaptability are key.\n\n## [Therapists using ChatGPT secretly](https://www.technologyreview.com/2025/09/02/1122871/therapists-using-chatgpt-secretly/) ðŸ˜”\n\n![](img/therapists-using-ChatGPT.png)\n\n\n## Recap: When is ML suitable?\n\n- ML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\n- Rule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making. \n- Human experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence.\n\n## Recap: Supervised learning {.smaller}\n\n- We wish to find a model function $f$ that relates $X$ to $y$.\n- We use the model function to predict targets of new examples. \n\n![](img/sup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\nIn the first part of this course, we'll focus on supervised machine learning. \n\n\n## Unsupervised learning {.smaller}\n- Training data consists of observations $X$ without any corresponding targets.\n- Unsupervised learning could be used to group similar things together in $X$ or to find underlying structure in the data.\n\n![](img/unsup-learning.png){.nostretch fig-align=\"center\" width=\"700px\"}\n\n## iClicker 2.2: Supervised vs unsupervised {.smaller}\n\nClicker cloud join link: \n\nSelect all of the following statements which are examples of supervised machine learning\n\n- (A) Finding groups of similar properties in a real estate data set.\n- (B) Predicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n- (C) Grouping articles on different topics from different news sources (something like the Google News app).\n- (D) Detecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n- (E) Given some measure of employee performance, identify the key factors which are likely to influence their performance.\n\n\n## iClicker 2.3: Classification vs. Regression {.smaller}\n\nClicker cloud join link: \n \nSelect all of the following statements which are examples of regression problems\n\n- (A) Predicting the price of a house based on features such as number of bedrooms and the year built.\n- (B) Predicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n- (C) Predicting percentage grade in CPSC 330 based on past grades.\n- (D) Predicting whether you should bicycle tomorrow or not based on the weather forecast.\n- (E) Predicting appropriate thermostat temperature based on the wind speed and the number of people in a room.\n\n\n## Today's focus \n\n- ML Terminology\n- Using sklearn to build a simple supervised ML model\n- Intuition of Decision Trees\n\n## Framework\n\n- There are many frameworks to do do machine learning. \n- We'll mainly be using [`scikit-learn` framework](https://scikit-learn.org/stable/). \n\n::: {#3351be1c .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <iframe\n            width=\"1000\"\n            height=\"650\"\n            src=\"https://scikit-learn.org\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\n## Running example {.smaller}\n\nImagine you're in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people. \n\n- Can you think of relevant features for this problem? \n\n## Toy job happinees dataset {.smaller}\n\nHere are the first few rows of a toy dataset.\n\n::: {#8e952030 .cell execution_count=3}\n``` {.python .cell-code}\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Terminology {.smaller}\n\n## Features, target, example\n::: panel-tabset\n### Terminology \n\n- What are the **features** $X$? \n  - features = inputs = predictors = explanatory variables = regressors = independent variables = covariates \n- What's the target $y$?\n  - target = output = outcome = response variable = dependent variable = labels \n- What is an example?\n\n### Data {.smaller}\n\n::: {#cb94df30 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n\n## Classification vs. Regression\n- Is this a **classification** problem or a **regression** problem?  \n\n::: {#53e6f218 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## (Optional) Inference vs. Prediction {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n- **Inference** asks: *Why does something happen?*  \n- Goal: **understand** and **quantify** the relationship between variables  \n- Often involves estimating model parameters and testing hypotheses  \n- Example: *Which factors influence happiness, and by how much?*  \n:::\n\n::: {.column width=\"40%\"}\n- **Prediction** asks: *What will happen?*  \n- Goal: **accurately predict the target** without needing to fully explain the relationships  \n- Example: *Will you be happy in a particular job?*  \n:::\n::::\n\nOf course these goals are related, and in many situations we need both. \n\n## Training {.smaller}\n- In supervised ML, the goal is to learn a function that maps input features ($X$) to a target ($y$).\n- The relationship between $X$ and $y$ is often complex, making it difficult to  define mathematically.\n- We use algorithms to approximate this complex relationship between $X$ and $y$.\n- **Training** is the process of applying an algorithm to learn the best function (or model) that maps $X$ to $y$. \n- In this course, I'll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline.\n\n## Error and accuracy\n\n- Machine learning models are **not perfect**â€”they will make mistakes.  \n- To judge whether a model is **useful**, we need to track its performance.  \n- For classification problems, the most common (and default in `sklearn`) metric is **accuracy**:  \n\n$$\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of examples}}\n$$\n\n## Separating $X$ and $y$\n\n- In order to train a model we need to separate $X$ and $y$ from the dataframe. \n\n::: {#b0243b60 .cell execution_count=6}\n``` {.python .cell-code}\nX = toy_happiness_df.drop(columns=[\"happy?\"]) # Extract the feature set by removing the target column \"happy?\"\ny = toy_happiness_df[\"happy?\"] # Extract the target variable \"happy?\"\n```\n:::\n\n\n## Baseline  {.smaller}\n- Let's try a simplest algorithm of predicting the most popular target! \n\n::: {#cb051f8e .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.dummy import DummyClassifier\nmodel = DummyClassifier(strategy=\"most_frequent\") # Initialize the DummyClassifier to always predict the most frequent class\nmodel.fit(X, y) # Train the model on the feature set X and target variable y\ntoy_happiness_df['dummy_predictions'] = model.predict(X) # Add the predicted values as a new column in the dataframe\ntoy_happiness_df\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n      <th>dummy_predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n      <td>Happy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Decision trees intuition {.smaller}\n- One intuitive way to build a model is by asking a series of **yes/no questions**, forming a tree.  \n- Which question would help you best separate the **happy** and **unhappy** examples?\n\n![](img/happy_unhappy_faces.png)\n\n::: {#8824fe43 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Which question is more effective? {.smaller}\n::: panel-tabset\n### Visual {.smaller}\n\n![](img/decision_tree_intuition.png)\n\n### Data {.smaller}\n\n::: {#e9385c2b .cell execution_count=9}\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n## What are we trying to learn? {.smaller}\n\n- We want to learn which questions to ask and in what order.\n- How many possible questions could we ask with these features?\n\n::: {#dac54829 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>supportive_colleagues</th>\n      <th>salary</th>\n      <th>free_coffee</th>\n      <th>boss_vegan</th>\n      <th>happy?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>60000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>110000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>120000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Happy</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>150000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Unhappy</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Decision tree Training (high level) {.smaller}\n\n- Training a decision tree is a search process: we look for the \"best\" tree among many possible ones.\n- There are different algorithms for learning trees. [Check this out.](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) \n- At each step, we evaluate candidate questions using measures such as:\n  - Information gain\n  - Gini index\n- The goal is to split the data into groups with greater certainty (more homogeneous outcomes).\n\n## Decision tree with `sklearn`\nLet's train a simple decision tree on our toy dataset using `sklearn` \n\n::: {#ad5a65c1 .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1) # Create a class object\nmodel.fit(X, y)\nplot_tree(model, filled=True, feature_names = X.columns, class_names=[\"Happy\", \"Unhappy\"], impurity = False, fontsize=12);\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-02-terminology-decision-trees_files/figure-revealjs/cell-12-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Prediction {.smaller}\n- Given a new example, how does a decision tree predict the class of this example?  \n- What would be the prediction for the example below using the tree above? \n  - supportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,  \n\n::: {#9e020a48 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-terminology-decision-trees_files/figure-revealjs/cell-13-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Prediction with `sklearn` {.smaller}\n- What would be the prediction for the example below using the tree above? \n  - supportive_colleagues = 1, salary = 60000, free_coffee = 0, vegan_boss = 1,  \n\n::: {#8ebb3350 .cell execution_count=13}\n``` {.python .cell-code}\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\nplot_tree(model, filled=True, feature_names = X.columns, class_names = [\"Happy\", \"Unhappy\"], impurity = False, fontsize=9);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel prediction:  ['Unhappy']\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-02-terminology-decision-trees_files/figure-revealjs/cell-14-output-2.png){width=763 height=389}\n:::\n:::\n\n\n## Parameters vs. Hyperparameters \n- Parameters \n  - The questions (features and thresholds) used to split the data at each node.\n  - Example: salary <= 75000 at the root node  \n- Hyperparameters\n  - Settings that control tree growth, like `max_depth`, which limits how deep the tree can go.\n\n## Decision boundary\n\n- A decision boundary is the **line, curve, or surface** that separates classes.\n- Points on one side $\\rightarrow$ Model predicts Class Happy  \n- Points on the other side $\\rightarrow$ Model predicts Class Unhappy\n\n## Decision boundary with `max_depth=1`\n\n::: {#89cdf3e9 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-terminology-decision-trees_files/figure-revealjs/cell-15-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## Decision boundary with `max_depth=2`\n\n::: {#dfe99266 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](slides-02-terminology-decision-trees_files/figure-revealjs/cell-16-output-1.png){width=1236 height=517}\n:::\n:::\n\n\n## iClicker 2.4: Baselines and Decision trees {.smaller}\n\niClicker cloud join link:  https://join.iclicker.com/FZMQ\n\nSelect all of the following statements which are TRUE.\n\n- (A) Change in features (i.e., binarizing features above) would change DummyClassifier predictions.\n- (B) predict takes only X as argument whereas fit and score take both X and y as arguments.\n- (C) For the decision tree algorithm to work, the feature values must be binary.\n- (D) The prediction in a decision tree works by routing the example from the root to the leaf.\n\n## Summary\n\n- Terminology\n- `sklearn` basic steps\n- Decision tree intuition \n\n",
    "supporting": [
      "slides-02-terminology-decision-trees_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}