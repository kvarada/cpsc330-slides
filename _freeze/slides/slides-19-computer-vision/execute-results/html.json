{
  "hash": "80e152ef3dc3f4cc7e98b6993e9bd0b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 19: Introduction to deep learning and computer vision\"\nauthor: \"Varada Kolhatkar\"\nformat: \n    revealjs:\n      embed-resources: true\n      slide-number: true\n      smaller: true\n      center: true\n      logo: img/UBC-CS-logo.png\n      resources:\n        - data/\n        - img/        \n---\n\n\n\n## Announcements\n\n- HW7 was due yesterday \n- HW8 has been released (due next week Monday)\n  - Almost there! Hang in there ðŸ˜Š\n- Midterm 2 grading is in progress. \n\n# iClicker 19.0\n\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\n**Select all of the following statements which are TRUE for you.**\n\n- (A) I found the multiple-choice questions challenging.\n- (B) The coding questions took a lot of time.\n- (C) I didn't like the format of the midterm.\n- (D) I appreciated the mix of coding, conceptual, and multiple-choice questions.\n- (E) I felt the midterm was a good reflection of what we cover in the lectures and homework assignments.\n\n## iClicker 19.1\n\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\n**Select all of the following statements which are TRUE.**\n\n- (A) It's possible to use word2vec embedding representations for text classification instead of bag-of-words representation. \n- (B) The topic model approach we used in the last lecture, Latent Dirichlet Allocation (LDA), is an unsupervised approach. \n- (C) In an LDA topic model, the same word can be associated with two different topics with high probability.\n- (D) In an LDA topic model, a document is a mixture of multiple topics. \n- (E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output. \n\n## Multiclass classification\n\n- So far we have been talking about binary classification \n- Can we use these classifiers when there are more than two classes? \n    - [\"ImageNet\" computer vision competition](http://www.image-net.org/challenges/LSVRC/), for example, has 1000 classes \n- Can we use decision trees or KNNs for multi-class classification?\n- What about logistic regression?\n\n# Multinomial logistic regression\n\n## Softmax Function for Probabilities\n\nGiven an input, the probability that it belongs to class $j \\in \\{1, 2, \\dots, K\\}$ is calculated using the **softmax function**:\n\n$P(y = j \\mid x_i) = \\frac{e^{w_j^\\top x_i + b_j}}{\\sum_{k=1}^{K} e^{w_k^\\top x_i + b_k}}$\n\n- $x_i$ is the $i^{th}$ example \n- $w_j$ is the weight vector for class $j$.\n- $b_j$ is the bias term for class $j$.\n- $K$ is the total number of classes.\n\n\n## Making Predictions\n\n1. **Compute Probabilities**:  \n   For each class $j$, compute the probability $P(y = j \\mid x_i)$ using the softmax function.\n\n2. **Select the Class with the Highest Probability**:  \n   The predicted class \\(\\hat{y}\\) is:  \n   $\\hat{y} = \\arg \\max_{j \\in \\{1, \\dots, K\\}} P(y = j \\mid x_i)$\n\n\n\n## Binary vs multinomial logistic regression \n\n|   **Aspect**                       | **Binary Logistic Regression**              | **Multinomial Logistic Regression**  |\n|--------------------------|---------------------------------------------|--------------------------------------|\n| **Target variable**      | 2 classes (binary)                          | More than 2 classes (multi-class)    |\n| **Getting probabilities**  | Sigmoid                                   | Softmax                              |\n| parameters               | $d$ weights, one per feature and the bias term | $d$ weights and a bias term per class |  \n| **Output**               | Single probability                          | Probability distribution over classes |\n| **Use case**             | Binary classification (e.g., spam detection) | Multi-class classification (e.g., image classification) |\n\n\n## Image classification\n\\\n\nHave you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\nThis can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n\n## Image classification\n\\\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n\n![](img/cat_variation.png)\n<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n\n## Neural networks\n\\\n\n- Neural networks are perfect for these types of problems where local structures are important. \n- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n  - [ImageNet Classification with Deep Convolutional\nNeural Networks](https://dl.acm.org/doi/10.1145/3065386)\n  - Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n- Let's go over the basics of a neural network.\n\n## Introduction to neural networks\n\\\n\n- Neural networks can be viewed a generalization of linear models where we apply a series of transformations.\n- Here is graphical representation of a logistic regression model.\n- We have 4 features: x[0], x[1], x[2], x[3]\n\n::: {#9beaa7b9 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](slides-19-computer-vision_files/figure-revealjs/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Adding a layer of transformations \n\\\n\n- Below we are adding one \"layer\" of transformations in between features and the target. \n- We are repeating the the process of computing the weighted sum multiple times.  \n- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n\n::: {#bd1234d5 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](slides-19-computer-vision_files/figure-revealjs/cell-4-output-1.svg){}\n:::\n:::\n\n\n## One more layer of transformations \n\\\n\n- Now we are adding one more layer of transformations. \n\n::: {#1811dac5 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](slides-19-computer-vision_files/figure-revealjs/cell-5-output-1.svg){}\n:::\n:::\n\n\n## Neural networks \n\\\n\n- With a neural net, you specify the number of features after each transformation.\n  - In the above, it goes from 4 to 3 to 3 to 1.\n\n- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n- Neural network = neural net\n- Deep learning ~ using neural networks\n\n## Why neural networks?\n\\\n\n- They can learn very complex functions.\n  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n  - More layers / bigger layers --> more complex model.\n  - You can generally get a model that will not underfit. \n\n- They work really well for structured data:\n  - 1D sequence, e.g. timeseries, language\n  - 2D image\n  - 3D image or video\n- They've had some incredible successes in the last 12 years.\n- Transfer learning (coming later today) is really useful.  \n\n## Why not neural networks?\n\\\n\n- Often they require a lot of data.\n- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n- They have huge numbers of hyperparameters\n  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n  - Being slow compounds this problem.\n- They are not interpretable.\n- I don't recommend training them on your own without further training\n- Good news\n    - You don't have to train your models from scratch in order to use them.\n    - I'll show you some ways to use neural networks without training them yourselves. \n\n## Deep learning software\n\\\n\nThe current big players are:\n\n1. [PyTorch](http://pytorch.org)\n2. [TensorFlow](https://www.tensorflow.org)\n\nBoth are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n\n<br><br>\n\n## Introduction to computer vision\n\\\n\n- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. \n- In the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n\n## Introduction to computer vision\n\\\n\n- image classification: is this a cat or a dog?\n- object localization: where is the cat in this image?\n- object detection: What are the various objects in the image? \n- instance segmentation: What are the shapes of these various objects in the image? \n- and much more...\n\n![](img/vision-apps.jpeg)\n<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n\n\n## Pre-trained models\n\\\n\n- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n- It refers to using a model already trained on one task as a starting point for learning to perform another task.\n\n## Pre-trained models out-of-the-box \n\\\n\n![](img/cnn-ex.png)\n\n<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n\n- Let's first apply one of these pre-trained models to our own problem right out of the box. \n\n\n## Pre-trained models out-of-the-box \n\\\n\n- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n- We preprocess the given image. \n- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n\n## Pre-trained models out-of-the-box {.scrollable}\n\n- Let's predict labels with associated probabilities for unseen images\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#e03a7c35 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-6-output-1.png){width=434 height=325}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-6-output-3.png){width=324 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-6-output-5.png){width=309 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-6-output-7.png){width=299 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n\n## Pre-trained models out-of-the-box \n\\\n\n- We got these predictions without \"doing the ML ourselves\".\n- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n\n## Pre-trained models out-of-the-box {.smaller}\n\\\n\n- Let's try some images which are unlikely to be there in ImageNet. \n- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#375a1763 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-1.png){width=549 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-3.png){width=702 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-5.png){width=611 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-7.png){width=428 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-9.png){width=331 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-7-output-11.png){width=607 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n## Pre-trained models out-of-the-box\n\\\n\n- Here we are Pre-trained models out-of-the-box. \n- Can we use pre-trained models for our own classification problem with our classes? \n- Yes!! We have two options here:\n    1. Add some extra layers to the pre-trained network to suit our particular task\n    2. Pass training data through the network and save the output to use as features for training some other model\n\n\n## Pre-trained models to extract features \n\\\n\n- Let's use pre-trained models to extract features.\n- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n- You can think of each layer a transformer applying some transformations on the input received to that later. \n\n![](img/cnn-ex.png)\n\n\n## Pre-trained models to extract features \n\\\n\n- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n- Let's try this out. \n- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n\n## Pre-trained models to extract features \n\\\n\nLet's look at some sample images in the dataset. \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#b82602b6 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-8-output-1.png){width=611 height=631}\n:::\n:::\n\n\n:::\n\n## Dataset statistics\n\\\n\nHere is the stat of our toy dataset. \n\n::: {#e527e20d .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)\n```\n:::\n:::\n\n\n## Pre-trained models to extract features \n\\\n\n- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n\n\n\n## Shape of the feature vector\n\\\n\n- Now we have extracted feature vectors for all examples. What's the shape of these features?\n\n::: {#e1babe6f .cell execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntorch.Size([283, 1024])\n```\n:::\n:::\n\n\n- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n\n![](img/densenet-architecture.png)\n\n[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n\n## A feature vector given by densenet \n\\ \n\n- Let's examine the feature vectors. \n\n::: {#13f2181c .cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1014</th>\n      <th>1015</th>\n      <th>1016</th>\n      <th>1017</th>\n      <th>1018</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000290</td>\n      <td>0.003821</td>\n      <td>0.005015</td>\n      <td>0.001307</td>\n      <td>0.052690</td>\n      <td>0.063403</td>\n      <td>0.000626</td>\n      <td>0.001850</td>\n      <td>0.256254</td>\n      <td>0.000223</td>\n      <td>...</td>\n      <td>0.229935</td>\n      <td>1.046375</td>\n      <td>2.241259</td>\n      <td>0.229641</td>\n      <td>0.033674</td>\n      <td>0.742792</td>\n      <td>1.338698</td>\n      <td>2.130880</td>\n      <td>0.625475</td>\n      <td>0.463088</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000407</td>\n      <td>0.005973</td>\n      <td>0.003206</td>\n      <td>0.001932</td>\n      <td>0.090702</td>\n      <td>0.438523</td>\n      <td>0.001513</td>\n      <td>0.003906</td>\n      <td>0.166081</td>\n      <td>0.000286</td>\n      <td>...</td>\n      <td>0.910680</td>\n      <td>1.580815</td>\n      <td>0.087191</td>\n      <td>0.606904</td>\n      <td>0.436106</td>\n      <td>0.306456</td>\n      <td>0.940102</td>\n      <td>1.159818</td>\n      <td>1.712705</td>\n      <td>1.624753</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000626</td>\n      <td>0.005090</td>\n      <td>0.002887</td>\n      <td>0.001299</td>\n      <td>0.091715</td>\n      <td>0.548537</td>\n      <td>0.000491</td>\n      <td>0.003587</td>\n      <td>0.266537</td>\n      <td>0.000408</td>\n      <td>...</td>\n      <td>0.465152</td>\n      <td>0.678276</td>\n      <td>0.946387</td>\n      <td>1.194697</td>\n      <td>2.537747</td>\n      <td>1.642383</td>\n      <td>0.701200</td>\n      <td>0.115620</td>\n      <td>0.186433</td>\n      <td>0.166605</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000169</td>\n      <td>0.006087</td>\n      <td>0.002489</td>\n      <td>0.002167</td>\n      <td>0.087537</td>\n      <td>0.623212</td>\n      <td>0.000427</td>\n      <td>0.000226</td>\n      <td>0.460680</td>\n      <td>0.000388</td>\n      <td>...</td>\n      <td>0.394083</td>\n      <td>0.700158</td>\n      <td>0.105200</td>\n      <td>0.856323</td>\n      <td>0.038457</td>\n      <td>0.023948</td>\n      <td>0.131838</td>\n      <td>1.296370</td>\n      <td>0.723323</td>\n      <td>1.915215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000286</td>\n      <td>0.005520</td>\n      <td>0.001906</td>\n      <td>0.001599</td>\n      <td>0.186034</td>\n      <td>0.850148</td>\n      <td>0.000835</td>\n      <td>0.003025</td>\n      <td>0.036309</td>\n      <td>0.000142</td>\n      <td>...</td>\n      <td>3.313760</td>\n      <td>0.565744</td>\n      <td>0.473564</td>\n      <td>0.139446</td>\n      <td>0.029283</td>\n      <td>1.165938</td>\n      <td>0.442319</td>\n      <td>0.227593</td>\n      <td>0.884266</td>\n      <td>1.592698</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 1024 columns</p>\n</div>\n```\n:::\n:::\n\n\n- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n\n## Logistic regression with the extracted features \n\\\n\n- Let's try out logistic regression on these extracted features. \n\n::: {#67932d49 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score:  1.0\n```\n:::\n:::\n\n\n::: {#0730e86e .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation score:  0.835820895522388\n```\n:::\n:::\n\n\n- This is great accuracy for so little data and little effort!!!\n\n\n## Sample predictions\n\\\n\nLet's examine some sample predictions on the validation set.  \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#509e2b76 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-15-output-1.png){width=1151 height=1901}\n:::\n:::\n\n\n:::\n\n\n## Object detection \n\\\n\n- Another useful task and tool to know is object detection using YOLO model. \n- Let's identify objects in a sample image using a pretrained model called YOLO8. \n- List the objects present in this image.\n\n![](data/yolo_test/3356700488_183566145b.jpg)\n\n## Object detection using [YOLO](https://docs.ultralytics.com/)\n\\\n\nLet's try this out using a pre-trained model. \n\n::: {#a854f3e8 .cell execution_count=15}\n``` {.python .cell-code}\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n\nyolo_input = \"data/yolo_test/3356700488_183566145b.jpg\"\nyolo_result = \"data/yolo_result.jpg\"\n# Run batched inference on a list of images\nresult = model(yolo_input)  # return a list of Results objects\nresult[0].save(filename=yolo_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nimage 1/1 /Users/kvarada/CS/2024-25/330/cpsc330-slides/website/slides/data/yolo_test/3356700488_183566145b.jpg: 512x640 4 persons, 2 cars, 1 stop sign, 114.0ms\nSpeed: 2.3ms preprocess, 114.0ms inference, 6.5ms postprocess per image at shape (1, 3, 512, 640)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n'data/yolo_result.jpg'\n```\n:::\n:::\n\n\n## Object detection output \n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#6d127d64 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-computer-vision_files/figure-revealjs/cell-17-output-1.png){width=950 height=401}\n:::\n:::\n\n\n:::\n\n\n## Summary \n\\\n\n- Neural networks are a flexible class of models.\n  - They are particular powerful for structured input like images, videos, audio, etc.\n  - They can be challenging to train and often require significant computational resources.\n- The good news is we can use pre-trained neural networks.\n  - This saves us a huge amount of time/cost/effort/resources.\n  - We can use these pre-trained networks directly or use them as feature transformers. \n\n",
    "supporting": [
      "slides-19-computer-vision_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}