{
  "hash": "80ec997a9d54ceb6f74be60f8b5bc11d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 19: Time series\"\nauthor: \"Varada Kolhatkar\"\nformat: \n    revealjs:\n      html-math-method: mathjax    \n      embed-resources: true\n      slide-number: true\n      center: true\n      logo: img/UBC-CS-logo.png\n      resources:\n        - data/\n        - img/        \n---\n\n\n\n## Focus on the breath!\n\n![](img/inukshuk.jpeg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n## Announcements\n\n- HW8 has been released (due next week Monday)\n  - Almost there! You‚Äôve got this! üòä\n- Midterm 2 grading is in progress. \n\n## Recap: iClicker questions {.smaller}\n- (A) In multinomial logistic regression, the model learns a separate weight vector and bias for each class.\n- (B) Neural networks are powerful models, so it's usually a good idea to start with them on any new machine learning problem.\n- (C) The main reason we add hidden layers is to allow the model to learn increasingly complex representations.\n- (D) Convolutional neural networks (CNNs) use filters that slide over the image to detect local patterns.\n- (E) Using a pre-trained network as a feature extractor typically requires less data than training a deep network from scratch.\n\n## Today's lecture goals\n- What is time series? \n- How do we know a problem is a time series problem?\n- Why do standard ML models struggle with time-dependent data?\n- How can we adapt ML models to handle time series?\n\n## What type of model would be appropriate? {.smaller}\n\n| Scenario |  Model/Method |\n|-------------------------------------------------------|-------|\n| You have user‚Äìitem ratings (e.g., movie ratings) and want to predict missing ratings. | ? |\n| You have a collection of documents without any labels and want to group them into themes. | ? |\n| You want to classify the emotion of a set of text messages, but you do not have any labeled data. | ? |\n| You have a small dataset with ~500 images containing pictures and names of 20 different Computer Science faculty members from UBC. Your goal is to develop a reasonably accurate multi-class classification model for this task.| ? |\n\n## Loan default prediction (tabular data) {.scrollable}\n\nYou work for a financial institution and have a dataset where each row represents a customer applying for a loan. What type of model would you use? \n\n| customer_id | income_k | credit_utilization | late_payments | employment_length | employment_type | home_ownership | loan_purpose        | default |\n|----|---|---|---|---|---|---|---|---|\n| 1           | 95       | 22                 | 0             | 9                 | salaried        | mortgage       | home_improvement    | 0       |\n| 2           | 45       | 78                 | 3             | 2                 | contract        | rent           | debt_consolidation  | 1       |\n| 3           | 120      | 30                 | 1             | 7                 | salaried        | own            | car                 | 0       |\n| 4           | 60       | 65                 | 2             | 3                 | self_employed   | rent           | debt_consolidation  | 1       |\n| 5           | 85       | 40                 | 0             | 10                | salaried        | mortgage       | education           | 0       |\n| 6           | 55       | 90                 | 4             | 1                 | contract        | rent           | debt_consolidation  | 1       |\n| 7           | 130      | 28                 | 0             | 6                 | salaried        | own            | car                 | 0       |\n| 8           | 40       | 82                 | 2             | 1                 | self_employed   | rent           | debt_consolidation  | 1       |\n\n- Rows are independent $\\rightarrow$ order does not matter $\\rightarrow$ time does not matter\n\n## citibike dataset\n\n- You have bike rental counts every three hours for one station in New York City over a month. You want to predict demand for the **next** three-hour period.  \n\n:::: {.columns}\n\n:::{.column width=\"40%\"}\n\n::: {#a4f86d74 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nstarttime\n2015-08-01 00:00:00     3\n2015-08-01 03:00:00     0\n2015-08-01 06:00:00     9\n2015-08-01 09:00:00    41\n2015-08-01 12:00:00    39\n2015-08-01 15:00:00    27\n2015-08-01 18:00:00    12\n2015-08-01 21:00:00     4\n2015-08-02 00:00:00     3\n2015-08-02 03:00:00     4\n2015-08-02 06:00:00     6\n2015-08-02 09:00:00    30\n2015-08-02 12:00:00    46\n2015-08-02 15:00:00    27\n2015-08-02 18:00:00    28\n2015-08-02 21:00:00     6\n2015-08-03 00:00:00     3\n2015-08-03 03:00:00     2\n2015-08-03 06:00:00    21\n2015-08-03 09:00:00     9\nFreq: 3h, Name: one, dtype: int64\n```\n:::\n:::\n\n\n:::\n\n:::{.column width=\"60%\"}\n![](img/citibike.jpg)\n:::\n::::\n\n## citibike data\n\n:::: {.columns}\n\n:::{.column width=\"40%\"}\n\n::: {#7c667c90 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nstarttime\n2015-08-01 00:00:00     3\n2015-08-01 03:00:00     0\n2015-08-01 06:00:00     9\n2015-08-01 09:00:00    41\n2015-08-01 12:00:00    39\n2015-08-01 15:00:00    27\n2015-08-01 18:00:00    12\n2015-08-01 21:00:00     4\n2015-08-02 00:00:00     3\n2015-08-02 03:00:00     4\n2015-08-02 06:00:00     6\n2015-08-02 09:00:00    30\n2015-08-02 12:00:00    46\n2015-08-02 15:00:00    27\n2015-08-02 18:00:00    28\n2015-08-02 21:00:00     6\n2015-08-03 00:00:00     3\n2015-08-03 03:00:00     2\n2015-08-03 06:00:00    21\n2015-08-03 09:00:00     9\nFreq: 3h, Name: one, dtype: int64\n```\n:::\n:::\n\n\n:::\n\n:::{.column width=\"60%\"}\n- Only feature: datetime (e.g., 2015-08-01 00:00:00)\n- The data is collected at regular intervals (every three hours) \n- Target: rentals in the next 3-hour period (e.g., 9 rentals between 2015-08-01 06:00:00 and 2015-08-01 09:00:00) \n- Goal: Given past rental counts, predict the number of rentals at a specific future time.\n:::\n::::\n\n**Using only the tools in your current toolbox, what model would you choose, and what challenges might you run into?**\n\n## Why different treatement?\n\n:::: {.columns}\n\n:::{.column width=\"40%\"}\n\n::: {#d20964aa .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nstarttime\n2015-08-01 00:00:00     3\n2015-08-01 03:00:00     0\n2015-08-01 06:00:00     9\n2015-08-01 09:00:00    41\n2015-08-01 12:00:00    39\n2015-08-01 15:00:00    27\n2015-08-01 18:00:00    12\n2015-08-01 21:00:00     4\n2015-08-02 00:00:00     3\n2015-08-02 03:00:00     4\n2015-08-02 06:00:00     6\n2015-08-02 09:00:00    30\n2015-08-02 12:00:00    46\n2015-08-02 15:00:00    27\n2015-08-02 18:00:00    28\n2015-08-02 21:00:00     6\n2015-08-03 00:00:00     3\n2015-08-03 03:00:00     2\n2015-08-03 06:00:00    21\n2015-08-03 09:00:00     9\nFreq: 3h, Name: one, dtype: int64\n```\n:::\n:::\n\n\n:::\n:::{.column width=\"60%\"}\n- This type of data is distinctive because it is inherently sequential, with an intrinsic order based on time.\n- The number of bikes available at a station at one point in time is often related to the number of bikes at earlier times. \n- This is a **time-series forecasting** problem.\n:::\n::::\n\n## Models for time series\n\nThe ML models we've used so far **do not have a built-in concept of time**. There are **two broad strategies** for modeling time series:\n\n- Use models designed for sequential data which explicitly capture temporal dependencies (e.g., Hidden Markov Models, Recurrent Neural Networks, transformer architectures  \n- Use tabular ML models with engineered temporal features (e.g., Linear models, Random Forests, Gradient Boosted Trees)  \n  - Requires creating features that capture temporal information. \n  - This allows us to reuse the familiar models in our toolbox.\n\n## citibike data visualization\n\n::: {#823c15a1 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nStart date: 2015-08-01 00:00:00\nEnd date: 2015-08-31 21:00:00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-6-output-2.png){width=963 height=528}\n:::\n:::\n\n\n- Do you see any daily patterns? Weekly patterns? Noise? \n\n## ‚õîÔ∏è Incorrect data splitting\n\n::: {#2259e40b .cell execution_count=6}\n``` {.python .cell-code}\ntrain_df, test_df = train_test_split(citibike, test_size=0.2, random_state=123)\nprint('Train largest date: ', train_df.index.max())\nprint('Test smallest date: ', test_df.index.min())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain largest date:  2015-08-31 21:00:00\nTest smallest date:  2015-08-01 12:00:00\n```\n:::\n:::\n\n\n::: {#7a401dcf .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-8-output-1.png){width=793 height=346}\n:::\n:::\n\n\n‚õîÔ∏è We should never train on the future to predict the past!\n\n## ‚úÖ Correct data splitting\n\nIn time series, the simplest split is: \n\n- earlier data $\\rightarrow$ training and later data $\\rightarrow$ testing\n\n::: {#ae814b47 .cell execution_count=8}\n``` {.python .cell-code}\n# Example split\nn_train = 184\ntrain_df = citibike[:184]\ntest_df = citibike[184:]\n```\n:::\n\n\n::: {#0c30a062 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-10-output-1.png){width=942 height=419}\n:::\n:::\n\n\n# Feature engineering for time series \n\n## Motivation \n\n- In this toy data, we just have a single feature: the date time feature. \n- Note that ML models do not have a built-in concept of time. We have to give it to them.\n- We will explore different ways to extract informative features from time.\n\n## POSIX time feature\n- Let's start with our worst but simplest encoding. \n- A common way that dates are stored on computers is using POSIX time, which is the number of seconds since January 1970 00:00:00 (this is beginning of Unix time). \n- Let's start with encoding feature as a single integer representing this POSIX time. \n\n::: {#927ef606 .cell execution_count=10}\n``` {.python .cell-code}\n# convert to POSIX time by dividing by 10**9\nX = (\n    citibike.index.astype(\"int64\").values.reshape(-1, 1) // 10**9\n)  # convert to POSIX time by dividing by 10**9\ny = citibike.values\nX[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[1438387200],\n       [1438398000],\n       [1438408800],\n       [1438419600],\n       [1438430400],\n       [1438441200],\n       [1438452000],\n       [1438462800],\n       [1438473600],\n       [1438484400]])\n```\n:::\n:::\n\n\n## Random forest on posix features \n\n::: {#087d59c9 .cell execution_count=11}\n``` {.python .cell-code}\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\neval_on_features(X, y, regressor, xticks, feat_names=\"POSIX time\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.85\nTest-set R^2: -0.04\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-12-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n- The predictions on the training data and training score are pretty good \n- But for the test data, a constant line is predicted ...\n- What's going on?\n\n## Trees cannot extrapolate! {.smaller}\n- Tree-based models (Decision Trees, Random Forests, Gradient Boosted Trees) only make predictions within the range of values they've seen during training.\n- They are excellent interpolators but terrible extrapolators because \n  - Trees partition the feature space into fixed regions and predictions inside each region are averages of training labels.\n  - If your future timestamps are larger than the ones in the training set trees cannot \"see beyond\" the training range and they will flatline or behave unpredictably.\n\nThis is exactly what happens with POSIX time encoded as a single numeric feature! \n\n\n## Extracting date and time information\n\n- Note that our index is of this special type: [`DateTimeIndex`](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html). We can extract all kinds of interesting information from it.   \n\n::: {#da2da97b .cell execution_count=12}\n``` {.python .cell-code}\nprint(citibike.index[0])\nprint(citibike.index[0].month_name())\nprint(citibike.index[0].dayofweek)\nprint(citibike.index[0].hour)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2015-08-01 00:00:00\nAugust\n5\n0\n```\n:::\n:::\n\n\n## Time of the day \n- We noted before that the time of the day and day of the week seem quite important. \n- Let's start with time of the day. \n\n::: {#173ab67d .cell execution_count=13}\n``` {.python .cell-code}\nX_hour = citibike.index.hour.values.reshape(-1, 1)\nX_hour[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([[ 0],\n       [ 3],\n       [ 6],\n       [ 9],\n       [12],\n       [15],\n       [18],\n       [21],\n       [ 0],\n       [ 3]], dtype=int32)\n```\n:::\n:::\n\n\n## Random forest with time of the day\n\n::: {#d369a0f6 .cell execution_count=14}\n``` {.python .cell-code}\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\neval_on_features(X_hour, y, regressor, xticks, feat_names=\"Hour of the day\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.50\nTest-set R^2: 0.60\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-15-output-2.png){width=1001 height=403}\n:::\n:::\n\n\nThe scores are better when we add time of the day feature! \n\n## Time of the day + Day of the week \nNow let's add day of the week along with time of the day. \n\n::: {#e86b05da .cell execution_count=15}\n``` {.python .cell-code}\nX_hour_week = np.hstack(\n    [\n        citibike.index.dayofweek.values.reshape(-1, 1),\n        citibike.index.hour.values.reshape(-1, 1),\n    ]\n)\nX_hour_week\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([[ 5,  0],\n       [ 5,  3],\n       [ 5,  6],\n       [ 5,  9],\n       [ 5, 12],\n       [ 5, 15],\n       [ 5, 18],\n       [ 5, 21],\n       [ 6,  0],\n       [ 6,  3],\n       [ 6,  6],\n       [ 6,  9],\n       [ 6, 12],\n       [ 6, 15],\n       [ 6, 18],\n       [ 6, 21],\n       [ 0,  0],\n       [ 0,  3],\n       [ 0,  6],\n       [ 0,  9],\n       [ 0, 12],\n       [ 0, 15],\n       [ 0, 18],\n       [ 0, 21],\n       [ 1,  0],\n       [ 1,  3],\n       [ 1,  6],\n       [ 1,  9],\n       [ 1, 12],\n       [ 1, 15],\n       [ 1, 18],\n       [ 1, 21],\n       [ 2,  0],\n       [ 2,  3],\n       [ 2,  6],\n       [ 2,  9],\n       [ 2, 12],\n       [ 2, 15],\n       [ 2, 18],\n       [ 2, 21],\n       [ 3,  0],\n       [ 3,  3],\n       [ 3,  6],\n       [ 3,  9],\n       [ 3, 12],\n       [ 3, 15],\n       [ 3, 18],\n       [ 3, 21],\n       [ 4,  0],\n       [ 4,  3],\n       [ 4,  6],\n       [ 4,  9],\n       [ 4, 12],\n       [ 4, 15],\n       [ 4, 18],\n       [ 4, 21],\n       [ 5,  0],\n       [ 5,  3],\n       [ 5,  6],\n       [ 5,  9],\n       [ 5, 12],\n       [ 5, 15],\n       [ 5, 18],\n       [ 5, 21],\n       [ 6,  0],\n       [ 6,  3],\n       [ 6,  6],\n       [ 6,  9],\n       [ 6, 12],\n       [ 6, 15],\n       [ 6, 18],\n       [ 6, 21],\n       [ 0,  0],\n       [ 0,  3],\n       [ 0,  6],\n       [ 0,  9],\n       [ 0, 12],\n       [ 0, 15],\n       [ 0, 18],\n       [ 0, 21],\n       [ 1,  0],\n       [ 1,  3],\n       [ 1,  6],\n       [ 1,  9],\n       [ 1, 12],\n       [ 1, 15],\n       [ 1, 18],\n       [ 1, 21],\n       [ 2,  0],\n       [ 2,  3],\n       [ 2,  6],\n       [ 2,  9],\n       [ 2, 12],\n       [ 2, 15],\n       [ 2, 18],\n       [ 2, 21],\n       [ 3,  0],\n       [ 3,  3],\n       [ 3,  6],\n       [ 3,  9],\n       [ 3, 12],\n       [ 3, 15],\n       [ 3, 18],\n       [ 3, 21],\n       [ 4,  0],\n       [ 4,  3],\n       [ 4,  6],\n       [ 4,  9],\n       [ 4, 12],\n       [ 4, 15],\n       [ 4, 18],\n       [ 4, 21],\n       [ 5,  0],\n       [ 5,  3],\n       [ 5,  6],\n       [ 5,  9],\n       [ 5, 12],\n       [ 5, 15],\n       [ 5, 18],\n       [ 5, 21],\n       [ 6,  0],\n       [ 6,  3],\n       [ 6,  6],\n       [ 6,  9],\n       [ 6, 12],\n       [ 6, 15],\n       [ 6, 18],\n       [ 6, 21],\n       [ 0,  0],\n       [ 0,  3],\n       [ 0,  6],\n       [ 0,  9],\n       [ 0, 12],\n       [ 0, 15],\n       [ 0, 18],\n       [ 0, 21],\n       [ 1,  0],\n       [ 1,  3],\n       [ 1,  6],\n       [ 1,  9],\n       [ 1, 12],\n       [ 1, 15],\n       [ 1, 18],\n       [ 1, 21],\n       [ 2,  0],\n       [ 2,  3],\n       [ 2,  6],\n       [ 2,  9],\n       [ 2, 12],\n       [ 2, 15],\n       [ 2, 18],\n       [ 2, 21],\n       [ 3,  0],\n       [ 3,  3],\n       [ 3,  6],\n       [ 3,  9],\n       [ 3, 12],\n       [ 3, 15],\n       [ 3, 18],\n       [ 3, 21],\n       [ 4,  0],\n       [ 4,  3],\n       [ 4,  6],\n       [ 4,  9],\n       [ 4, 12],\n       [ 4, 15],\n       [ 4, 18],\n       [ 4, 21],\n       [ 5,  0],\n       [ 5,  3],\n       [ 5,  6],\n       [ 5,  9],\n       [ 5, 12],\n       [ 5, 15],\n       [ 5, 18],\n       [ 5, 21],\n       [ 6,  0],\n       [ 6,  3],\n       [ 6,  6],\n       [ 6,  9],\n       [ 6, 12],\n       [ 6, 15],\n       [ 6, 18],\n       [ 6, 21],\n       [ 0,  0],\n       [ 0,  3],\n       [ 0,  6],\n       [ 0,  9],\n       [ 0, 12],\n       [ 0, 15],\n       [ 0, 18],\n       [ 0, 21],\n       [ 1,  0],\n       [ 1,  3],\n       [ 1,  6],\n       [ 1,  9],\n       [ 1, 12],\n       [ 1, 15],\n       [ 1, 18],\n       [ 1, 21],\n       [ 2,  0],\n       [ 2,  3],\n       [ 2,  6],\n       [ 2,  9],\n       [ 2, 12],\n       [ 2, 15],\n       [ 2, 18],\n       [ 2, 21],\n       [ 3,  0],\n       [ 3,  3],\n       [ 3,  6],\n       [ 3,  9],\n       [ 3, 12],\n       [ 3, 15],\n       [ 3, 18],\n       [ 3, 21],\n       [ 4,  0],\n       [ 4,  3],\n       [ 4,  6],\n       [ 4,  9],\n       [ 4, 12],\n       [ 4, 15],\n       [ 4, 18],\n       [ 4, 21],\n       [ 5,  0],\n       [ 5,  3],\n       [ 5,  6],\n       [ 5,  9],\n       [ 5, 12],\n       [ 5, 15],\n       [ 5, 18],\n       [ 5, 21],\n       [ 6,  0],\n       [ 6,  3],\n       [ 6,  6],\n       [ 6,  9],\n       [ 6, 12],\n       [ 6, 15],\n       [ 6, 18],\n       [ 6, 21],\n       [ 0,  0],\n       [ 0,  3],\n       [ 0,  6],\n       [ 0,  9],\n       [ 0, 12],\n       [ 0, 15],\n       [ 0, 18],\n       [ 0, 21]], dtype=int32)\n```\n:::\n:::\n\n\n## Random forest with Time of the day + Day of the week \n\n::: {#068528dd .cell execution_count=16}\n``` {.python .cell-code}\neval_on_features(X_hour_week, y, regressor, xticks, feat_names = \"hour of day + day of week\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.89\nTest-set R^2: 0.84\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-17-output-2.png){width=1001 height=403}\n:::\n:::\n\n\nThe time of the day and day of the week features are clearly helping. \n\n## Linear model \nLet's try an interpretable linear model `Ridge` with these features. \n\n::: {#ec1fd38b .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.16\nTest-set R^2: 0.13\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-18-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n- Why is `Ridge` performing poorly on the training data as well as test data?\n\n## Encoding time and day with OHE \n\n::: {#0bc28f6a .cell execution_count=18}\n``` {.python .cell-code}\nenc = OneHotEncoder()\nX_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\nhour = [\"%02d:00\" % i for i in range(0, 24, 3)]\nday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nfeatures = day + hour\npd.DataFrame(X_hour_week_onehot, columns=features).head(6)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mon</th>\n      <th>Tue</th>\n      <th>Wed</th>\n      <th>Thu</th>\n      <th>Fri</th>\n      <th>Sat</th>\n      <th>Sun</th>\n      <th>00:00</th>\n      <th>03:00</th>\n      <th>06:00</th>\n      <th>09:00</th>\n      <th>12:00</th>\n      <th>15:00</th>\n      <th>18:00</th>\n      <th>21:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Linear model with OHE day and time \nLet's try an interpretable linear model `Ridge` with these features. \n\n::: {#86298562 .cell execution_count=19}\n``` {.python .cell-code}\neval_on_features(X_hour_week_onehot, y, Ridge(), xticks, feat_names=\"hour of day OHE + day of week OHE\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.53\nTest-set R^2: 0.62\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-20-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n- The scores are a bit better! \n- Can we improve them further? \n\n\n## Add interaction features \n\n::: {#07f9a3fe .cell execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mon</th>\n      <th>Tue</th>\n      <th>Wed</th>\n      <th>Thu</th>\n      <th>Fri</th>\n      <th>Sat</th>\n      <th>Tue 00:00</th>\n      <th>Tue 03:00</th>\n      <th>Tue 06:00</th>\n      <th>Tue 09:00</th>\n      <th>Tue 18:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Linear model with OHE day and time + interaction feats\n\n::: {#370d1cc5 .cell execution_count=21}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.87\nTest-set R^2: 0.85\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-22-output-2.png){width=1001 height=403}\n:::\n:::\n\n\nThe scores are much better now!! \n\n## Interpretation \n\nSince we are using a linear model, we can examine the coefficients learned by `Ridge`. \n\n::: {#af3b61d8 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sat 09:00</th>\n      <td>15.196739</td>\n    </tr>\n    <tr>\n      <th>Wed 06:00</th>\n      <td>15.005809</td>\n    </tr>\n    <tr>\n      <th>Sat 12:00</th>\n      <td>13.437684</td>\n    </tr>\n    <tr>\n      <th>Sun 12:00</th>\n      <td>13.362009</td>\n    </tr>\n    <tr>\n      <th>Thu 06:00</th>\n      <td>10.907595</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Sat 21:00</th>\n      <td>-6.085150</td>\n    </tr>\n    <tr>\n      <th>00:00</th>\n      <td>-11.693898</td>\n    </tr>\n    <tr>\n      <th>03:00</th>\n      <td>-12.111220</td>\n    </tr>\n    <tr>\n      <th>Sat 06:00</th>\n      <td>-13.757591</td>\n    </tr>\n    <tr>\n      <th>Sun 06:00</th>\n      <td>-18.033267</td>\n    </tr>\n  </tbody>\n</table>\n<p>71 rows √ó 1 columns</p>\n</div>\n```\n:::\n:::\n\n\n**Do these coefficients make sense?**\n\n\n## Interim summary {.smaller}\n\n- Success in time-series analysis heavily relies on the appropriate choice of models and features.\n- Tree-based models cannot extrapolate; caution is needed when using them with linear integer features.\n- Linear models struggle with cyclic patterns in numeric features (e.g., numerically encoded time of the day feature) because these patterns are inherently non-linear.\n- Applying one-hot encoding on such features transforms cyclic temporal features into a format where their impact on the target variable can be independently and linearly modeled, enabling linear models to effectively capture and use these cyclic patterns. \n\n## Lag-based features\n\n:::: {.columns}\n\n:::{.column width=\"40%\"}\n\n::: {#49be1ccd .cell execution_count=23}\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_rentals</th>\n    </tr>\n    <tr>\n      <th>starttime</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-08-01 00:00:00</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 03:00:00</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 06:00:00</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 09:00:00</th>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 12:00:00</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 15:00:00</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 18:00:00</th>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 21:00:00</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2015-08-02 00:00:00</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2015-08-02 03:00:00</th>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n:::{.column width=\"60%\"}\n- In time series data there is temporal dependence; observations close in time tend to be correlated.\n- Currently we're using current time to predict the number of bike rentals in the next three hours.\n- But, what if the number of bike rentals is also related to bike rentals three hours ago or 6 hours ago and so on?\n:::\n:::: \n\nSuch features are called lagged features.\n\n## Creating lag features\n\n::: {#d479ccdd .cell execution_count=24}\n``` {.python .cell-code}\ndef create_lag_df(df, lag, cols):\n    return df.assign(\n        **{f\"{col}-{n}\": df[col].shift(n) for n in range(1, lag + 1) for col in cols}\n    )\nrentals_lag5 = create_lag_df(rentals_df, 5, ['n_rentals'] )\nrentals_lag5.head(8)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_rentals</th>\n      <th>n_rentals-1</th>\n      <th>n_rentals-2</th>\n      <th>n_rentals-3</th>\n      <th>n_rentals-4</th>\n      <th>n_rentals-5</th>\n    </tr>\n    <tr>\n      <th>starttime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-08-01 00:00:00</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 03:00:00</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 06:00:00</th>\n      <td>9</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 09:00:00</th>\n      <td>41</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 12:00:00</th>\n      <td>39</td>\n      <td>41.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 15:00:00</th>\n      <td>27</td>\n      <td>39.0</td>\n      <td>41.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 18:00:00</th>\n      <td>12</td>\n      <td>27.0</td>\n      <td>39.0</td>\n      <td>41.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2015-08-01 21:00:00</th>\n      <td>4</td>\n      <td>12.0</td>\n      <td>27.0</td>\n      <td>39.0</td>\n      <td>41.0</td>\n      <td>9.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Linear model with lag features\n\n::: {#cddd5749 .cell execution_count=25}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.25\nTest-set R^2: 0.37\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-26-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n## Random Forest with lag features\n\n::: {#3a87e09f .cell execution_count=26}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.94\nTest-set R^2: 0.69\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-27-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n## Random Forest with time and day + lag features\n\n::: {#f1a7799f .cell execution_count=27}\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain-set R^2: 0.95\nTest-set R^2: 0.78\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-28-output-2.png){width=1001 height=403}\n:::\n:::\n\n\n## Cross-validation with time series\n\n- We can't do regular cross-validation if we don't want to be predicting the past.\n\n- There is [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for time series data. \n\n::: {#2cdcf434 .cell execution_count=28}\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 1 2] [3]\n[0 1 2 3] [4]\n[0 1 2 3 4] [5]\n```\n:::\n:::\n\n\n# Forecasting further into the future \n\n## Problem\n\n::: {#88c44922 .cell execution_count=29}\n``` {.python .cell-code}\ncitibike.index.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\nTimestamp('2015-08-31 21:00:00')\n```\n:::\n:::\n\n\n- So far, our lag features let us predict 3 hours ahead. \n- What if we want to predict 15 hours in the future? In other words, what if we want to predict number of rentals for the next three hours at time 2015-09-01 12:00:00?  \n- Problem: We do not yet know the rental counts for the required lag timestamps:\n    - 2015-09-01 00:00:00\n    - 2015-09-01 03:00:00\n    - 2015-09-01 06:00:00\n    - 2015-09-01 09:00:00\n- Without those values, our lag features break üò¢\n\n## Approach 1: Iterative forecasting\n\n- Train one model that predicts 3 hours ahead.\n- At prediction time, move forward step by step, using your own predictions as future lag inputs.\n\n**Example:**\n\n- Predict rentals at 00:00 on 2015-09-01.\n- Use that prediction as the lag to predict rentals at 03:00.\n- Use both predictions to predict rentals at 06:00.\n- Continue until you reach 12:00.\n\nThis method works, but errors accumulate as we step forward. The longer the horizon, the more uncertainty grows. \n\n## Approach 2: Direct forecasting (multiple horizons)\n\n- Train separate models for each horizon:\n  - Model 1 $\\rightarrow$ predict 3 hours ahead\n  - Model 2 $\\rightarrow$ predict 6 hours ahead\n  - Model 3 $\\rightarrow$ predict 9 hours ahead\n\nEach model uses lag features that match the required horizon.\n\n\n## (Optional) Approach 3: Multi-output models (joint forecasting)\n\n- Train one model that predicts several future steps at once, e.g.:\n\n  ```y = [rentals_in_3h, rentals_in_6h, rentals_in_9h, ...]```\n\n- These models learn relationships across future time steps.\n- Note: Multi-output forecasting is powerful, but outside the scope of CPSC 330.\n\n# Seasonality and trends \n\n\n## Trends \nLet's consider another time series dataset, [Retail Sales of Clothing and Clothing Accessory Stores dataset](https://fred.stlouisfed.org/series/MRTSSM448USN). \n\n::: {#b7b2d375 .cell execution_count=30}\n\n::: {.cell-output .cell-output-display}\n![](slides-19-time-series_files/figure-revealjs/cell-31-output-1.png){width=823 height=434}\n:::\n:::\n\n\n- It looks like there's a **trend** here; the sales are going up over time. \n- How can we encode this information in the model?\n\n\n## \n\n- One idea is to create a feature such as \"Days_since\"\n\n::: {#9dbaf152 .cell execution_count=31}\n\n::: {.cell-output .cell-output-display execution_count=31}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>sales</th>\n      <th>sales-1</th>\n      <th>sales-2</th>\n      <th>sales-3</th>\n      <th>sales-4</th>\n      <th>sales-5</th>\n      <th>Days_since</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>6938</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-02-01</td>\n      <td>7524</td>\n      <td>6938.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-03-01</td>\n      <td>8475</td>\n      <td>7524.0</td>\n      <td>6938.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-04-01</td>\n      <td>9401</td>\n      <td>8475.0</td>\n      <td>7524.0</td>\n      <td>6938.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-05-01</td>\n      <td>9558</td>\n      <td>9401.0</td>\n      <td>8475.0</td>\n      <td>7524.0</td>\n      <td>6938.0</td>\n      <td>NaN</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1992-06-01</td>\n      <td>9182</td>\n      <td>9558.0</td>\n      <td>9401.0</td>\n      <td>8475.0</td>\n      <td>7524.0</td>\n      <td>6938.0</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1992-07-01</td>\n      <td>9103</td>\n      <td>9182.0</td>\n      <td>9558.0</td>\n      <td>9401.0</td>\n      <td>8475.0</td>\n      <td>7524.0</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1992-08-01</td>\n      <td>10513</td>\n      <td>9103.0</td>\n      <td>9182.0</td>\n      <td>9558.0</td>\n      <td>9401.0</td>\n      <td>8475.0</td>\n      <td>213</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1992-09-01</td>\n      <td>9573</td>\n      <td>10513.0</td>\n      <td>9103.0</td>\n      <td>9182.0</td>\n      <td>9558.0</td>\n      <td>9401.0</td>\n      <td>244</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1992-10-01</td>\n      <td>10254</td>\n      <td>9573.0</td>\n      <td>10513.0</td>\n      <td>9103.0</td>\n      <td>9182.0</td>\n      <td>9558.0</td>\n      <td>274</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_20_time-series.ipynb) \n\n",
    "supporting": [
      "slides-19-time-series_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}