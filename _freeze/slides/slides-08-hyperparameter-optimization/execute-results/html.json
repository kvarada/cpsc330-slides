{
  "hash": "1f5e3dc4f946a4f057274c7c3d7060d6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 8: Hyperparameter Optimization\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Linear regression, logistic regression\"\ndescription-short: \"Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/m01ukubppof625/post/249\n- Change of my office hours\n  - Thursdays from 2 to 3 in my office ICCS 237\n- HW3 is due today 11:59 pm. \n- HW4 has been released \n\n\n\n## Recap: Logistic regression\n- A **linear model used for binary classification** tasks. \n  - There is a variant of logistic regression called multinomial logistic regression for multiclass classification.\n- Parameters: \n  - Coefficients (Weights): The model learns a coefficient or a weight associated with each feature that represents its importance.\n  - Bias (Intercept): A constant term added to the linear combination of features and their coefficients.\n\n## Recap: Logistic regression \n- The model computes a weighted sum of the input featuresâ€™ values, adjusted by their respective coefficients and the bias term.\n- This weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the \"positive\" class.\n\n\\begin{equation}\nP_{hat} = \\sigma\\left(\\sum_{i=1}^d w_i x_i + b\\right) \n\\end{equation}\n\n- $P_{hat}$ is the predicted probability of the example belonging to the positive class. \n- $w_i$ is the learned weight associated with feature $i$\n- $x_i$ is the value of the input feature $i$\n- $b$ is the bias term \n\n## Recap: Logistic regression\n\n- For a dataset with $d$ features, the decision boundary that \nseparates the classes is a $d-1$ dimensional hyperplane.  \n- Complexity hyperparameter: `C` in `sklearn`. \n  - Higher `C` $\\rightarrow$ more complex model meaning larger coefficients\n  - Lower `C` $\\rightarrow$ less complex model meaning smaller coefficients\n\n\n## Recap: `CountVectorizer` input \n\n- Primarily designed to accept either a `pandas.Series` of text data or a 1D `numpy` array. It can also process a list of string data directly.\n- Unlike many transformers that handle multiple features (`DataFrame` or 2D `numpy` array), `CountVectorizer` a single text column at a time.\n- If your dataset contains multiple text columns, you will need to instantiate separate `CountVectorizer` objects for each text feature.\n- This approach ensures that the unique vocabulary and tokenization processes are correctly applied to each specific text column without interference.\n\n## Hyperparameter optimization motivation\n\n![](img/hyperparam-optimization.png)\n\n\n## Data\n\n::: {#8ebb6684 .cell execution_count=2}\n``` {.python .cell-code}\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3130</th>\n      <td>spam</td>\n      <td>LookAtMe!: Thanks for your purchase of a video...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>ham</td>\n      <td>Aight, I'll hit you up when I get some cash</td>\n    </tr>\n    <tr>\n      <th>4697</th>\n      <td>ham</td>\n      <td>Don no da:)whats you plan?</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>ham</td>\n      <td>Going to take your babe out ?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Model building \n\n- Let's define a pipeline \n\n::: {#99d3363f .cell execution_count=3}\n``` {.python .cell-code}\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n```\n:::\n\n\n- Suppose we want to try out different hyperparameter values. \n\n::: {#e287462c .cell execution_count=4}\n``` {.python .cell-code}\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}\n```\n:::\n\n\n## Hyperparameter optimization with loops\n\n- Define a parameter space.\n- Iterate through possible combinations.\n- Evaluate model performance.\n- What are some limitations of this approach? \n\n\n## `sklearn` methods \n\n- `sklearn` provides two main methods for hyperparameter optimization\n  - Grid Search\n  - Random Search\n\n## Grid Search \n\n- Covers all possible combinations from the provided grid. \n- Can be parallelized easily.\n- Integrates cross-validation.\n\n\n## Grid search example \n\n::: {#eb021c62 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(pipe_svm, \n                  param_grid = param_grid, \n                  n_jobs=-1, \n                  return_train_score=True\n                 )\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.9782606272997375\n```\n:::\n:::\n\n\n## Random Search \n- More efficient than grid search when dealing with large hyperparameter spaces.\n- Samples a given number of parameter settings from distributions.\n\n![](img/randomsearch_bergstra.png)\n\n## Random search example \n\n::: {#d63c4e40 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(pipe_svm,                                    \n                  param_distributions = param_dist, \n                  n_iter=10, \n                  n_jobs=-1, \n                  return_train_score=True)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0.9840446723701153\n```\n:::\n:::\n\n\n## Optimization bias \n\n- Why do we need separate validation and test datasets? \n\n![](img/optimization-bias.png)\n\n## Mitigating optimization bias.\n  - Cross-validation\n  - Ensembles \n  - Regularization and choosing a simpler model  \n\n\n## (iClicker) Exercise 8.1\n\niClicker cloud join link: **https://join.iclicker.com/VYFJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) If you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n- (B) Grid search is guaranteed to find the best hyperparameter values.\n- (C) It is possible to get different hyperparameters in different runs of RandomizedSearchCV.\n\n## Questions for you\n\n- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n  - Probably\n  - Probably not\n\n\n## Questions for class discussion\n\n- Suppose you have 10 hyperparameters, each with 4 possible values. If you run `GridSearchCV` with this parameter grid, how many cross-validation experiments will be carried out? \n- Suppose you have 10 hyperparameters and each takes 4 values. If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`, how many cross-validation experiments will be carried out? \n\n# [Class Demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_08_hyperparameter-optimization.ipynb)\n\n",
    "supporting": [
      "slides-08-hyperparameter-optimization_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}