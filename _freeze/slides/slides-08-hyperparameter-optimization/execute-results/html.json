{
  "hash": "eb70a963222b7b603259727ed79b5792",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 8: Hyperparameter Optimization\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Linear regression, logistic regression\"\ndescription-short: \"Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients\"\nformat:\n  revealjs:\n    html-math-method: mathjax\n    embed-resources: true\n    slide-number: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n\n## Focus on the breath! \n\n::: {.center}\n![](img/inukshuk.jpeg){height=80%}\n:::\n\n## Check-in\n\n**How are you feeling today?**\n\n- (A) Things are more or less under control! \n\n- (B) Excited about hyperparameter optimization! \n\n- (C) Lost üòû\n- (D) Tired / Sleepy üò¥\n- (E) Secretly thinking of lunch ü•ó \n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/mekbcze4gyber/post/162\n- HW3 was due on Monday, Sept 29th 11:59 pm. \n- HW4 has been released \n\n## Recap: iClicker Logistic Regression 1 \n\nWhich of the following are **True**?\n\n- (A) Logistic regression can be used for binary as well as multi-class classification tasks.  \n- (B) Logistic regression computes a weighted sum of features and applies the sigmoid function.  \n- (C) The sigmoid function ensures outputs between 0 and 1, interpreted as probabilities.  \n- (D) The decision boundary in logistic regression is linear, even though the sigmoid is applied. \n- (E) When the weighted sum is 0, $\\hat{p}$ = 0.5.  \n\n## Recap: iClicker Logistic Regression 1\n\nWhich of the following are **True**?\n\n- (A) Logistic regression coefficients always have to be positive.  \n- (B) Larger coefficients (in absolute value) indicate stronger feature influence on the prediction.  \n- (C) For $d$ features, the decision boundary is a $d-1$ dimensional hyperplane.  \n- (D) In `sklearn`, very small `C` value shrinks coefficients, often leading to underfitting.  \n- (E) A larger `C` value allows larger coefficients and a more complex model.  \n\n\n## Recap: Logistic regression {.smaller}\n- A **linear model used for binary classification** tasks. \n  - (Optional) There is am extension of logistic regression called multinomial logistic regression for multiclass classification.\n- Parameters: \n  - **Coefficients (Weights)**: The model learns a coefficient or a weight associated with each feature that represents its importance.\n  - **Bias (Intercept)**: A constant term added to the linear combination of features and their coefficients.\n\n## Recap: Logistic regression {.smaller}\n- The model computes a weighted sum of the input features‚Äô values, adjusted by their respective coefficients and the bias term.\n- This weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the \"positive\" class.\n\n$$ \\hat{p} = \\sigma\\left(\\sum_{i=1}^d w_i x_i + b\\right) $$\n\n- $P_{hat}$ is the predicted probability of the example belonging to the positive class. \n- $w_i$ is the learned weight associated with feature $i$\n- $x_i$ is the value of the input feature $i$\n- $b$ is the bias term \n\n## Recap: Logistic regression\n\n- For a dataset with $d$ features, the decision boundary that \nseparates the classes is a $d-1$ dimensional hyperplane.  \n- Complexity hyperparameter: `C` in `sklearn`. \n  - Higher `C` $\\rightarrow$ more complex model meaning larger coefficients\n  - Lower `C` $\\rightarrow$ less complex model meaning smaller coefficients\n\n## Data\n\n::: {#89a3661f .cell execution_count=2}\n``` {.python .cell-code}\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3130</th>\n      <td>spam</td>\n      <td>LookAtMe!: Thanks for your purchase of a video...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>ham</td>\n      <td>Aight, I'll hit you up when I get some cash</td>\n    </tr>\n    <tr>\n      <th>4697</th>\n      <td>ham</td>\n      <td>Don no da:)whats you plan?</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>ham</td>\n      <td>Going to take your babe out ?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Model building \n\n- Let's define a pipeline \n\n::: {#aa885a2e .cell execution_count=3}\n``` {.python .cell-code}\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n```\n:::\n\n\n- Suppose we want to try out different hyperparameter values. \n\n::: {#f15eae47 .cell execution_count=4}\n``` {.python .cell-code}\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}\n```\n:::\n\n\n## Hyperparameter optimization with loops\n\n- Define a parameter space.\n- Iterate through possible combinations.\n- Evaluate model performance.\n- What are some limitations of this approach? \n\n\n## `sklearn` methods \n\n- `sklearn` provides two main methods for hyperparameter optimization\n  - Grid Search\n  - Random Search\n\n## Grid Search \n\n- Covers all possible combinations from the provided grid. \n- Can be parallelized easily.\n- Integrates cross-validation.\n\n\n## Grid search example \n\n::: {#2af6989f .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(pipe_svm, \n                  param_grid = param_grid, \n                  n_jobs=-1, \n                  return_train_score=True\n                 )\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.9782606272997375\n```\n:::\n:::\n\n\n## Random Search \n- More efficient than grid search when dealing with large hyperparameter spaces.\n- Samples a given number of parameter settings from distributions.\n\n![](img/randomsearch_bergstra.png)\n\n## Random search example \n\n::: {#ff4a0d1b .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(pipe_svm,                                    \n                  param_distributions = param_dist, \n                  n_iter=10, \n                  n_jobs=-1, \n                  return_train_score=True)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0.9808534476411086\n```\n:::\n:::\n\n\n# Optimization bias \n\n## Pizza baking competition example\n\nImagine that you participate in pizza baking competition. \n\n:::: {columns}\n::: {.column width=\"50%\"}\n\n![](img/margherita-pizza.jpeg)\n:::\n\n::: {.column width=\"50%\"}\n- **Training phase**: Collecting recipes and practicing at home  \n- **Validation phase**: Inviting a group of friends for tasting and feedback  \n- **Test phase (competition day)**: Serving the judges  \n:::\n::::\n\n## Overfitting on the validation set\n:::: {columns}\n::: {.column width=\"50%\"}\n- Your friends loved your pineapple pizza. \n- You fine-tune your recipe for the same group of friends, perfecting it for their tastes.\n\n<img src=\"img/pineapple-pizza.png\" width=\"60%\" alt=\"Pineapple Pizza\">\n:::\n\n::: {.column width=\"50%\"}\n- On the competition day, you confidently present your perfected pineapple pizza.  \n- Judges are not impressed: *‚ÄúThis doesn‚Äôt appeal to a broad audience.‚Äù*  \n![](img/eva-sad.png)\n\n:::\n::::\n\nThis is similar to reusing the same validation set again and again to perfect the model for it! \n\n## Lesson: Overfitting on the validation set {.smaller}\n\n- You tailored your recipe too closely to your friends‚Äô tastes.  \n- They were **not representative** of the broader audience (the judges).  \n- The pizza, while perfect for your validation group, failed to **generalize**.  \n- Over many iterations, the validation set no longer gives an **unbiased estimate** of performance.  \n- That's why we need a **separate test set** (like a group of tasters who never influenced your pizza).  \n\n## Optimization bias\n- Why do we need separate validation and test datasets? \n![](img/optimization-bias.png)\n\n## Mitigating optimization bias.\n  - Cross-validation\n  - Ensembles \n  - Regularization and choosing a simpler model  \n\n## (iClicker) Exercise 8.1\n\nSelect all of the following statements which are TRUE.\n\n- (A) If you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n- (B) Grid search is guaranteed to find the best hyperparameter values.\n- (C) It is possible to get different hyperparameters in different runs of RandomizedSearchCV.\n\n## Questions for you\n\n- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n  - Probably\n  - Probably not\n\n\n## Questions for class discussion\n\n- Suppose you have 10 hyperparameters, each with 4 possible values. If you run `GridSearchCV` with this parameter grid, how many cross-validation experiments will be carried out? \n- Suppose you have 10 hyperparameters and each takes 4 values. If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`, how many cross-validation experiments will be carried out? \n\n# [Class Demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_08-hyperparameter-optimization.ipynb)\n\n",
    "supporting": [
      "slides-08-hyperparameter-optimization_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}