{
  "hash": "0554ad8669a870b8c52efed774334362",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 10: Regression Metrics\" \nauthor: \"Varada Kolhatkar\"\ndescription: \"Metrics for Regression\"\ndescription-short: \"Ridge, \" \nformat:\n  revealjs:\n    html-math-method: mathjax\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n\n## Focus on the breath!\n\n![](img/inukshuk.jpeg){.nostretch fig-align=\"center\" width=\"500px\"}\n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/mekbcze4gyber/post/162\n  - **Good news for you: You'll have access to our course notes in the midterm!**\n- HW4 was due on Monday, Oct 6th 11:59 pm. \n- HW5 has been released. It's a project-type assignment and you get till Oct 27th to work on it.   \n\n## iClicker OH \nI'm planning to hold an in-person midterm review office hour. Which time works best for you?\n\n- (A) Friday, October 10th 2pm \n- (B) Tuesday, October 14th 2pm \n- (C) Tuesday, October 14th 4pm \n\n\n## Which metric fits best? {.smaller}\n\n| Scenario | Data Imbalance | Main Concern | Best Metric(s) / Curve |\n|-----------|----------------|---------------|------------------------|\n| **Email Spam Detection** | 10% spam | Avoid false positives| |\n| **Disease Screening** | 1 in 10,000 | Avoid false negatives | |\n| **Credit Card Fraud** | 0.1% fraud | Focus on rare positive class | |\n| **Customer Churn** | 20% churn | Balance FP & FN | |\n| **Sentiment Analysis** | 50/50 balanced | Overall correctness | |\n| **Face Recognition** | Balanced pairs | Trade-off FP vs FN | |\n\n## Summary: Choosing the right metric {.smaller}\n\n| Metric / Plot | When to Use | Why |\n|---------|----------|----------|\n| **Precision, Recall, F1** | When you care about *specific error types* (FP vs FN) or a *fixed threshold*. | Focus on particular tradeoffs. |\n| **PR Curve & AP Score** | When the dataset is **highly imbalanced** (rare positives). | Ignores TNs; focuses on positives. |\n| **ROC Curve & AUC** | When classes are **moderately imbalanced**. | Measures ranking ability across thresholds. |\n\n\n## Questions for you \n\n- What's the difference between the average precision (AP) score and F1-score? \n- Which model would you pick? \n\n![](img/pr-curve-which-model.png)\n\n## ROC of a baseline model\n:::: {.columns}\n:::{.column width=\"50%\"}\n![](img/roc-baseline.png) \n:::\n:::{.column width=\"50%\"}\n**AUC–ROC** measures the probability that a randomly chosen **positive example** receives a **higher score** than a randomly chosen **negative example**.  \n\n- **Perfect model:** (AUC = 1.0). Always ranks positives above negatives.  \n- **Random model** (AUC = 0.5): No discriminative ability (equivalent to random guessing).  \n:::\n::::\n\n[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n\n## Questions for you {.smaller}\n\n:::: {.columns}\n:::{.column width=\"60%\"}\n![](img/roc-curve-which-model.png)\n:::\n:::{.column width=\"40%\"}\n- Which model would you pick? \n:::\n::::\n\n## Dealing with class imbalance\n- Under sampling \n- Oversampling \n- `class weight=\"balanced\"` (preferred method for this course)\n- SMOTE\n\n## Handling imbalance by chaning class weights\n\n- We can specify class_weight=\"balanced\" to give more importance to rare examples during training.\n\n::: {#28903f26 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-10-regression-metrics_files/figure-revealjs/cell-3-output-1.png){}\n:::\n:::\n\n\n::: {.notes}\n- This sets the weights so that the classes are \"equal\".\n- We have reduced false negatives but we have many more false positives now ...\n  - Decreases false negatives, which improves recall.\n  - Increases false positives, which might lower precision.\n:::\n\n# [Regression metrics class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_10-regression-metrics.ipynb)\n\n## Ridge and RidgeCV\n- **Ridge Regression**: `alpha` hyperparameter controls model complexity.\n- **RidgeCV**: Ridge regression with built-in cross-validation to find the optimal `alpha`.\n\n## `alpha` hyperparameter\n- **Role of `alpha`**:\n  - Controls model complexity\n  - Higher `alpha`: Simpler model, smaller coefficients.\n  - Lower `alpha`: Complex model, larger coefficients.\n\n## Regression metrics: MSE, RMSE, MAPE, r2_score\n\n- **Mean Squared Error (MSE)**: Average of the squares of the errors.\n- **Root Mean Squared Error (RMSE)**: Square root of MSE, same units as the target variable.\n- **r2** measures how much of the variation in the target variable your model can explain.-\n- **Mean Absolute Percentage Error (MAPE)**: Average of the absolute percentage errors.\n\n\n## Applying log transformation to the targets\n\n- Suitable when the target has a wide range and spans several orders of magnitude \n  - Example: counts data such as social media likes or price data\n- Helps manage skewed data, making patterns more apparent and regression models more effective.\n- `TransformedTargetRegressor`\n  - Wraps a regression model and applies a transformation to the target values.\n\n## iClicker Exercise 10.1\n\n**Select all of the following statements which are TRUE.**\n\n- (A) Price per square foot would be a good feature to add in our `X`. \n- (B) The `alpha` hyperparameter of `Ridge` has similar interpretation of `C` hyperparameter of `LogisticRegression`; higher `alpha` means more complex model. \n- (C) In `Ridge`, smaller alpha means bigger coefficients whereas bigger alpha means smaller coefficients.  \n\n\n## iClicker Exercise 10.2\n\n**Select all of the following statements which are TRUE.**\n\n- (A) We can still use precision and recall for regression problems but now we have other metrics we can use as well.\n- (B) In `sklearn` for regression problems, using `r2_score()` and `.score()` (with default values) will produce the same results.\n- (C) RMSE is always going to be non-negative.\n- (D) MSE does not directly provide the information about whether the model is underpredicting or overpredicting.\n- (E) We can pass multiple scoring metrics to `GridSearchCV` or `RandomizedSearchCV` for regression as well as classification problems. \n\n## Which metric fits the scenario? {.smaller}\n\n| Scenario | What matters most? | Best metric(s)? |\n|-----------|-------------------|----------|\n| Predicting **house prices** ranging from \\$60K–\\$800K.  | A \\$30K error is huge for a \\$60K house but small for a \\$500K house. |  |\n| Predicting **exam scores** (0–100).  | You want an interpretable measure of average error in **points**. |  |\n| Predicting **energy consumption** in a large industrial system. | Large errors are very costly and should be penalized heavily. |  |\n| Predicting **insurance claim amounts**. | You want to compare how well different models explain the variation in claims. |  |\n\n\n## Which metric fits the scenario? \n\n- **For interpretability:** prefer RMSE or MAPE\n- **When you want to discourage large error:** MSE is common\n- **For fair comparison:** r2 provides a normalized score similar to accuracy in classification.  \n- **For imbalanced scales:** MAPE helps when proportional error matters more than absolute error.\n\n",
    "supporting": [
      "slides-10-regression-metrics_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}