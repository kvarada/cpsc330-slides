{
  "hash": "238bee611131c97332c7c9029c2be3a6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 10: Regression Metrics\" \nauthor: \"Varada Kolhatkar\"\ndescription: \"Metrics for Regression\"\ndescription-short: \"Ridge, \" \nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Important information about midterm 1\n  - https://piazza.com/class/m01ukubppof625/post/249\n  - Good news for you: You'll have access to our course notes in the midterm! \n- HW5 will be released today. It's a project-type assignment and you get till Oct 28th to work on it.  \n\n\n\n## Recap: Confusion matrix\n:::: {.columns}\n:::{.column width=\"80%\"}\n![](img/tp-fp-tn-fn-fraud.png)\n:::\n\n:::{.column width=\"20%\"}\n- TN $\\rightarrow$ True negatives \n- FP $\\rightarrow$ False positives \n- FN $\\rightarrow$ False negatives\n- TP $\\rightarrow$ True positives \n:::\n::::\n\n## Recap: Precision, Recall, F1-Score\n:::: {.columns}\n:::{.column width=\"70%\"}\n![](img/fraud-precision-recall.png)\n:::\n:::{.column width=\"30%\"}\n- $$Precision = \\frac{TP}{TP + FP} = ?$$\n- $$Recall = \\frac{TP}{TP + FN} = ?$$\n:::\n\n\n## Recap: PR curve\n- Calculate precision and recall (TPR) at every possible threshold and graph them. \n- Better choice for highly imbalanced datasets because it focuses on the performance of the positive class. \n\n![](img/pr-curve-example.png)\n\n\n## Questions for you \n\n- What's the difference between the average precision (AP) score and F1-score? \n\n## Recap: ROC curve \n- Calculate the true positive rate (TPR) and false positive rate (FPR) ($\\frac{FP}{FP + TN}$) at every possible thresholding and graph TPR over FPR. \n- Good choice when the datasets are roughly balanced. \n![](img/roc-curve-example.png)\n\n## ROC Curve\nNot a great choice when there is an extreme imbalance because FPR can remain relatively low even if the number of false positives is high, simply because the number of negatives is very large.  \n\n## AUC \n- The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.\n\n$$ FPR  = \\frac{FP}{FP + TN}$$ \n\n",
    "supporting": [
      "slides-10-regression-metrics_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}