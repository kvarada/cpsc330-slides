{
  "hash": "28adf1911beb1acb241e706bcd4022ab",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"DSCI 571 Lecture 6: Hyperparameter Optimization\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Hyperparameter optimization and optimization bias\"\ndescription-short: \"motivation for hyperparameter optimization, hyperparameter optimization using sklearnâ€™s GridSearchCV and RandomizedSearchCV, optimization bias\" \nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n\n## Recap: `CountVectorizer` input \n\n- Primarily designed to accept either a `pandas.Series` of text data or a 1D `numpy` array. It can also process a list of string data directly.\n- Unlike many transformers that handle multiple features (`DataFrame` or 2D `numpy` array), `CountVectorizer` a single text column at a time.\n- If your dataset contains multiple text columns, you will need to instantiate separate `CountVectorizer` objects for each text feature.\n- This approach ensures that the unique vocabulary and tokenization processes are correctly applied to each specific text column without interference.\n\n## Hyperparameter optimization motivation\n\n![](img/hyperparam-optimization.png)\n\n\n## Data\n\n::: {#aeb86514 .cell execution_count=2}\n``` {.python .cell-code}\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3130</th>\n      <td>spam</td>\n      <td>LookAtMe!: Thanks for your purchase of a video...</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>ham</td>\n      <td>Aight, I'll hit you up when I get some cash</td>\n    </tr>\n    <tr>\n      <th>4697</th>\n      <td>ham</td>\n      <td>Don no da:)whats you plan?</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>ham</td>\n      <td>Going to take your babe out ?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Model building \n\n- Let's define a pipeline \n\n::: {#20a8c7dd .cell execution_count=3}\n``` {.python .cell-code}\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n```\n:::\n\n\n- Suppose we want to try out different hyperparameter values. \n\n::: {#4a3315a0 .cell execution_count=4}\n``` {.python .cell-code}\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}\n```\n:::\n\n\n## Hyperparameter optimization with loops\n\n- Define a parameter space.\n- Iterate through possible combinations.\n- Evaluate model performance.\n- What are some limitations of this approach? \n\n\n## `sklearn` methods \n\n- `sklearn` provides two main methods for hyperparameter optimization\n  - Grid Search\n  - Random Search\n\n## Grid Search \n\n- Covers all possible combinations from the provided grid. \n- Can be parallelized easily.\n- Integrates cross-validation.\n\n\n## Grid search example \n\n::: {#d713e96c .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(pipe_svm, \n                  param_grid = param_grid, \n                  n_jobs=-1, \n                  return_train_score=True\n                 )\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nnp.float64(0.9782606272997375)\n```\n:::\n:::\n\n\n## Random Search \n- More efficient than grid search when dealing with large hyperparameter spaces.\n- Samples a given number of parameter settings from distributions.\n\n![](img/randomsearch_bergstra.png)\n\n## Random search example \n\n::: {#97992a2d .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(pipe_svm,                                    \n                  param_distributions = param_dist, \n                  n_iter=10, \n                  n_jobs=-1, \n                  return_train_score=True)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nnp.float64(0.9804550420594504)\n```\n:::\n:::\n\n\n# Optimization bias \n\n## Pizza baking competition example\n\nImagine that you participate in pizza baking competition. \n\n![](img/margherita-pizza.jpeg)\n\n- Training phase: Collecting recipes and practicing \n- Validation phase: Inviting a group of friends and getting feedback \n\n## Overfitting on the validation set\n:::: {columns}\n::: {.column width=\"50%\"}\n- Your friends love your pineapple pizza you hesitantly tried out. \n- Encouraged by their enthusiasm, you decide to focus on perfecting this recipe, believing it to be a crowd-pleaser\n\n<img src=\"img/pineapple-pizza.png\" width=\"70%\" alt=\"Pineapple Pizza\">\n:::\n\n::: {.column width=\"50%\"}\n### Competition day! \n- You confidently present your perfected pineapple pizza, expecting it to be a hit\n- The judges are not impressed. They criticize the choice of pineapple, pointing out that it might not appeal to a general audience.\n\n![](img/eva-sad.png)\n:::\n::::\n\n## Overfitting on the validation set \n\n- By focusing solely on the positive feedback from your pineapple-loving friends, you've overfitted your pizza to their tastes. This group, however, was not representative of the broader preferences of the competition judges or the general public.\n- The pizza, while perfect for your validation group, failed to generalize across a broader range of tastes, leading to disappointing results in the competition where diverse preferences were expected.\n\n## Optimization bias\n- Why do we need separate validation and test datasets? \n![](img/optimization-bias.png)\n\n## Mitigating optimization bias.\n  - Cross-validation\n  - Ensembles \n  - Regularization and choosing a simpler model  \n\n\n## (iClicker) Exercise 6.1\n\niClicker cloud join link: **https://join.iclicker.com/YWOJ**\n\nSelect all of the following statements which are TRUE.\n\n- (A) If you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n- (B) Grid search is guaranteed to find the best hyperparameter values.\n- (C) It is possible to get different hyperparameters in different runs of RandomizedSearchCV.\n\n## Questions for you\n\n- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n  - Probably\n  - Probably not\n\n\n## Questions for class discussion\n\n- Suppose you have 10 hyperparameters, each with 4 possible values. If you run `GridSearchCV` with this parameter grid, how many cross-validation experiments will be carried out? \n- Suppose you have 10 hyperparameters and each takes 4 values. If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`, how many cross-validation experiments will be carried out? \n\n# Class Demo\n\n",
    "supporting": [
      "slides-06-hyperparameter-optimization_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}