{
  "hash": "ad113d62a0df2eb1d08fd7cee5a8b924",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"CPSC 330 Lecture 7: Linear models\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Linear regression, logistic regression\"\ndescription-short: \"Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Important information about midterm 1\n- Change of my office hours\n- Where to find slides? \n  - https://kvarada.github.io/cpsc330-slides/lecture.html\n- HW3 is due next week Tuesday, Sept 29th, 11:59 pm. \n\n\n\n## Recap: Dealing with text features \n- Preprocessing text to fit into machine learning models using text vectorization.\n- Bag of words representation \n![](img/bag-of-words.png)\n\n## Recap: `sklearn` `CountVectorizer`\n- Use `scikit-learn`’s `CountVectorizer` to encode text data\n- `CountVectorizer`: Transforms text into a matrix of token counts\n- Important parameters:\n  - `max_features`: Control the number of features used in the model \n  - `max_df`, `min_df`: Control document frequency thresholds\n  - `ngram_range`: Defines the range of n-grams to be extracted\n  - `stop_words`: Enables the removal of common words that are typically uninformative in most applications, such as “and”, “the”, etc.\n\n## Recap: Incorporating text features in a machine learning pipeline\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipeline = make_pipeline(\n    CountVectorizer(),\n    SVC()\n)\n```\n\n## (iClicker) Exercise 6.2\n\nSelect all of the following statements which are TRUE.\n\n- (A) `handle_unknown=\"ignore\"` would treat all unknown categories equally.\n- (B) As you increase the value for `max_features` hyperparameter of `CountVectorizer` the training score is likely to go up.\n- (C) Suppose you are encoding text data using `CountVectorizer`. If you encounter a word in the validation or the test split that's not available in the training data, we'll get an error.\n- (D) In the code below, inside `cross_validate`, each fold might have slightly different number of features (columns) in the fold.\n\n```python\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)\n```\n\n## Linear models \n\n:::: {.columns}\n\n:::{.column width=\"45%\"}\n- Linear models make an assumption that the relationship between `X` and `y` is linear. \n- In this case, with only one feature, our model is a straight line.\n- What do we need to represent a line?\n  - Slope ($w_1$): Determines the angle of the line.\n  - Y-intercept ($w_0$): Where the line crosses the y-axis.\n\n:::\n\n\n::: {.column width=\"55%\"}\n\n::: {#82bf4b56 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](old-slides-07-linear-models_files/figure-revealjs/cell-3-output-1.png){}\n:::\n:::\n\n\n- Making predictions\n  - $$ y_{hat} = w_1 \\times \\text{# hours studied} + w_0$$\n\n:::\n\n::::\n\n## `Ridge` vs. `LinearRegression`\n- Ordinary linear regression is sensitive to **multicolinearity** and overfitting\n- Multicolinearity: Overlapping and redundant features. Most of the real-world datasets have colinear features.   \n- Linear regression may produce large and unstable coefficients in such cases. \n- `Ridge` adds a parameter to control the complexity of a model. Finds a line that balances fit and prevents overly large coefficients.\n\n## When to use what?\n- `LinearRegression`\n  - When interpretability is key, and no multicollinearity exists\n- `Ridge` \n  - When you have **multicollinearity** (highly correlated features).\n  - When you want to prevent **overfitting** in linear models.\n- **In this course, we'll use `Ridge`.**\n\n## Logistic regression \n- Suppose your target is binary: pass or fail \n- Logistic regression is used for such binary classification tasks.  \n- Logistic regression predicts a probability that the given example belongs to a particular class.\n- It uses **Sigmoid function** to map any real-valued input into a value between 0 and 1, representing the probability of a specific outcome.\n- A threshold (usually 0.5) is applied to the predicted probability to decide the final class label.  \n\n## Logistic regression: Decision boundary \n\n:::: {.columns}\n\n:::{.column width=\"60%\"}\n\n::: {#5ca87c3e .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](old-slides-07-linear-models_files/figure-revealjs/cell-4-output-1.png){}\n:::\n:::\n\n\n:::\n:::{.column width=\"40%\"}\n- The decision boundary is the point on the x-axis where the corresponding predicted probability on the y-axis is 0.5. \n:::\n\n::::\n\n## Parametric vs. non-Parametric models (high-level)\n- Imagine you are training a logistic regression model. For each of the following scenarios, identify how many parameters (weights and biases) will be learned.\n- Scenario 1: 100 features and 1,000 examples\n- Scenario 2: 100 features and 1 million examples\n\n## Parametric vs. non-Parametric models (high-level)\n:::: {.columns}\n\n:::{.column width=\"50%\"}\n#### Parametric\n- Examples: Logistic regression, linear regression, linear SVM  \n- Models with a fixed number of parameters, regardless of the dataset size\n- Simple, computationally efficient, less prone to overfitting\n- Less flexible, may not capture complex relationships\n:::\n\n:::{.column width=\"50%\"}\n#### Non parametric\n- Examples: KNN, SVM RBF, Decision tree with no specific depth specified \n- Models where the number of parameters grows with the dataset size. They do not assume a fixed form for the functions being learned. \n- Flexible, can adapt to complex patterns\n- Computationally expensive, risk of overfitting with noisy data\n:::\n\n\n::::\n\n## (iClicker) Exercise 7.1\n\nSelect all of the following statements which are TRUE.\n\n- (A) Increasing the hyperparameter `alpha` of `Ridge` is likely to decrease model complexity.\n- (B) `Ridge` can be used with datasets that have multiple features.\n- (C) With `Ridge`, we learn one coefficient per training example.\n- (D) If you train a linear regression model on a 2-dimensional problem (2 features), the model will learn 3 parameters: one for each feature and one for the bias term.\n\n\n## (iClicker) Exercise 7.2\n\nSelect all of the following statements which are TRUE.\n\n- (A) Increasing logistic regression’s `C` hyperparameter increases model complexity.\n- (B) The raw output score can be used to calculate the probability score for a given prediction.\n- (C) For linear classifier trained on $d$ features, the decision boundary is a $d-1$-dimensional hyperparlane.\n- (D) A linear model is likely to be uncertain about the data points close to the decision boundary.\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_07_linear-models.ipynb)\n\n",
    "supporting": [
      "old-slides-07-linear-models_files"
    ],
    "filters": [],
    "includes": {}
  }
}