{
  "hash": "e1caaff1aef0526b0d991aa6c56b2b83",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 5: Preprocessing and sklearn pipelines\"\nauthor: \"Varada Kolhatkar\"\ndescription: \"Preprocessing and sklearn pipelines\"\ndescription-short: \"Preprocessing and sklearn pipelines\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n\neditor:\n  render-on-save: true\n---\n\n\n\n## Announcements \n\n- HW1 grades have been posted.\n- HW1 solutions have been posted on Canvas under Files tab. Please do not share them with anyone or do not post them anywhere.\n- Syllabus quiz due date is September 19th, 11:59 pm. \n- Homework 3 (hw3) has been released (Due: Sept 29th, 11:59 pm)\n  - You can work in pairs for this assignment. \n\n## Recap \n\n- Decision trees: Split data into subsets based on feature values to create decision rules \n- $k$-NNs: Classify based on the majority vote from $k$ nearest neighbors\n- SVM RBFs: Create a boundary using an RBF kernel to separate classes\n\n## Recap\n\n| **Aspect**                     | **Decision Trees**              | **K-Nearest Neighbors (KNN)** | **Support Vector Machines (SVM) with RBF Kernel**      |\n|--------------------------------|---------------------------------|-------------------------------|--------------------------------------------------------|\n| **Main hyperparameters**       | Max depth, min samples split    | Number of neighbors ($k$)     | C (regularization), Gamma (RBF kernel width)           |\n| **Interpretability**           |  |  | \n| **Handling of non-linearity**  |  |  | \n| **Scalability**                |  |  | \n\n## Preprocessing motivation: example \n\nYou‚Äôre trying to find a suitable date based on:\n\n- Age (closer to yours is better).\n- Number of Facebook Friends (closer to your social circle is ideal).\n\n## Preprocessing motivation: example {.smaller}\n\n- You are 30 years old and have 250 Facebook friends.\n\n| Person | Age | #FB Friends | Euclidean Distance Calculation  | Distance    |\n|--------|-----|-------------|---------------------------------|-------------|\n| A      | 25  | 400         | ‚àö(5¬≤ + 150¬≤)                    | 150.08      |\n| B      | 27  | 300         | ‚àö(3¬≤ + 50¬≤)                     | 50.09       |\n| C      | 30  | 500         | ‚àö(0¬≤ + 250¬≤)                    | 250.00      |\n| D      | 60  | 250         | ‚àö(30¬≤ + 0¬≤)                     | 30.00       |\n\nBased on the distances, the two nearest neighbors (2-NN) are:\n\n- **Person D** (Distance: 30.00)\n- **Person B** (Distance: 50.09)\n\nWhat's the problem here? \n\n## What is preprocessing? \n\n- **Preprocessing** is about making the raw dataset ready for machine learning. \n- Let's walk through strategies for handling missing values, categorical variables, text, scaling, and irrelevant features using a class demo.\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_05-06-preprocessing.ipynb)\n\n## (iClicker) Exercise 5.1\n\nTake a guess: In your machine learning project, how much time will you typically spend on data preparation and transformation?\n\n- (A) ~80% of the project time\n- (B) ~20% of the project time\n- (C) ~50% of the project time\n- (D) None. Most of the time will be spent on model building\n\nThe question is adapted from [here](https://developers.google.com/machine-learning/crash-course/numerical-data).\n\n\n## (iClicker) Exercise 5.2\n\nSelect all of the following statements which are TRUE.\n\n- (A) `StandardScaler` ensures a fixed range (i.e., minimum and maximum values) for the features.\n- (B) `StandardScaler` calculates mean and standard deviation for each feature separately.\n- (C) In general, it‚Äôs a good idea to apply scaling on numeric features before training $k$-NN or SVM RBF models.\n- (D) The transformed feature values might be hard to interpret for humans.\n- (E) After applying `SimpleImputer` The transformed data has a different shape than the original data.\n\n\n## (iClicker) Exercise 5.3\n\nSelect all of the following statements which are TRUE.\n\n- (A) You can have scaling of numeric features, one-hot encoding of categorical features, and scikit-learn estimator within a single pipeline.\n- (B) Once you have a `scikit-learn` pipeline object with an estimator as the last step, you can call `fit`, `predict`, and `score` on it.\n- (C) You can carry out data splitting within `scikit-learn` pipeline.\n- (D) We have to be careful of the order we put each transformation and model in a pipeline.\n\n\n# Common transformations\n\n## Imputation: Fill the gaps! (üü© üüß üü¶)\nFill in missing data using a chosen strategy:\n\n- **Mean**: Replace missing values with the average of the available data.\n- **Median**: Use the middle value.\n- **Most Frequent**: Use the most common value (mode).\n- **KNN Imputation**: Fill based on similar neighbors.\n\n### Example:\nImputation is like filling in your average or median or most frequent grade for an assessment you missed. \n\n```python\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\n```\n\n## Scaling: Everything to the same range! (üìâ üìà)\nEnsure all features have a comparable range.\n\n- **StandardScaler**: Mean = 0, Standard Deviation = 1.\n\n### Example:\nScaling is like adjusting the number of everyone‚Äôs Facebook friends so that both the number of friends and their age are on a comparable scale. This way, one feature doesn't dominate the other when making comparisons.\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n\n## One-Hot encoding: üçé  ‚Üí 1Ô∏è‚É£ 0Ô∏è‚É£ 0Ô∏è‚É£\n\nConvert categorical features into binary columns.\n\n- Creates new binary columns for each category.\n- Useful for handling categorical data in machine learning models.\n\n### Example:\nTurn \"Apple, Banana, Orange\" into binary columns:\n\n| Fruit   | üçé | üçå | üçä |\n|---------|-------|--------|--------|\n| Apple üçé  |   1   |   0    |   0    |\n| Banana üçå |   0   |   1    |   0    |\n| Orange üçä |   0   |   0    |   1    |\n\n```python\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)\n```\n\n\n## Ordinal encoding: Ranking matters! (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ‚Üí 3Ô∏è‚É£)\nConvert categories into integer values that have a meaningful order.\n\n- Assign integers based on order or rank.\n- Useful when there is an inherent ranking in the data.\n\n### Example:\nTurn \"Poor, Average, Good\" into 1, 2, 3:\n\n| Rating   | Ordinal |\n|----------|---------|\n| Poor     |    1    |\n| Average  |    2    |\n| Good     |    3    |\n\n```python\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nX_ordinal = encoder.fit_transform(X)\n```\n\n# `sklearn` Transformers vs Estimators\n\n## Transformers\n- Are used to transform or preprocess data.\n- Implement the `fit` and `transform` methods.\n  - `fit(X)`: Learns parameters from the data.\n  - `transform(X)`: Applies the learned transformation to the data.\n  \n- **Examples**:\n  - **Imputation** (`SimpleImputer`): Fills missing values.\n  - **Scaling** (`StandardScaler`): Standardizes features.\n\n## Estimators\n\n- Used to make predictions.\n- Implement `fit` and `predict` methods.\n    - `fit(X, y)`: Learns from labeled data.\n    - `predict(X)`: Makes predictions on new data.\n\n- Examples: `DecisionTreeClassifier`, `SVC`, `KNeighborsClassifier`\n\n\n## The golden rule in feature transformations\n- **Never** transform the entire dataset at once!\n- **Why**? It leads to **data leakage** ‚Äî using information from the test set in your training process, which can artificially inflate model performance.\n- **Fit** transformers like scalers and imputers on the **training set only**.\n- **Apply** the transformations to both the training and test sets **separately**.\n\n### Example:\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\n\n## `sklearn` Pipelines\n\n- Pipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\n- Simplify the code and improves readability.\n- Reduce the risk of data leakage by ensuring proper transformation of the training and test sets.\n- Automatically apply transformations in sequence.\n\n### Example:\nChaining a `StandardScaler` with a `KNeighborsClassifier` model.\n\n```python\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n```\n\n",
    "supporting": [
      "slides-05-preprocessing-pipelines_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}