{
  "hash": "bbe2190a94f12af7a509bd8e896ada78",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering'\ndescription: \"Unsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset,  importance of input data representation in clustering.\"\nformat:\n    revealjs:\n        html-math-method: mathjax\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\neditor:\n  render-on-save: true\n---\n\n\n## Happy Halloween\n\n![](img/eva-halloween.png)\n\n## Announcements \n\n- HW6 is due next week Monday\n  - Computationally intensive \n  - You need to install many packages \n\n## Imports \n\n::: {#e5987101 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-2-output-1.png){}\n:::\n:::\n\n\n## iClicker Exercise 15.1 \n\n**Select all of the following statements which are TRUE.**\n\n- (A) Similar to K-nearest neighbours, K-Means is a non parametric model.\n- (B) The meaning of $K$ in K-nearest neighbours and K-Means clustering is very similar. \n- (C) Scaling of input features is crucial in clustering.  \n- (D) In clustering, it's almost always a good idea to find equal-sized clusters. \n\n# K-means Limitations {.smaller}\n\n## Shape of clusters\n- Good for spherical clusters of more or less equal sizes \n![](img/kmeans_boundaries.png)\n\n## K-Means: failure case 1\n\n- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). \n\n::: {#26e3a065 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-3-output-1.png){}\n:::\n:::\n\n\n## K-Means: failure case 2\n\n- Again, K-Means is unable to capture complex cluster shapes. \n\n::: {#7f4e4041 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-4-output-1.png){}\n:::\n:::\n\n\n## K-Means: failure case 3\n\n- It assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters. \n\n::: {#8767c0cd .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-5-output-1.png){}\n:::\n:::\n\n\n# Can we do better? \n\n## DBSCAN\n\n- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise\n- A density-based clustering algorithm\n\n::: {#0d966220 .cell execution_count=5}\n``` {.python .cell-code}\nX, y = make_moons(n_samples=200, noise=0.08, random_state=42)\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(X)\nplot_original_clustered(X, dbscan, dbscan.labels_)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-6-output-1.png){}\n:::\n:::\n\n\n## How does it work?\n![](img/DBSCAN_search.gif)\n\n## DBSCAN Analogy\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#39009491 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-7-output-1.png){}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\nConsider DBSCAN in a social context: \n\n- Social butterflies (ü¶ã): Core points\n- Friends of social butterflies who are not social butterflies: Border points\n- Lone wolves (üê∫): Noise points  \n:::\n::::\n\n## Two main hyperparameters\n- `eps`: determines what it means for points to be \"close\"\n- `min_samples`: determines the number of **neighboring points** we require to consider in order for a point to be part of a cluster\n\n## DBSCAN: failure cases {.smaller}\n\n- Let's consider this dataset with three clusters of varying densities.  \n- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n\n::: {#65417219 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-8-output-2.png){}\n:::\n:::\n\n\n## Hierarchical clustering \n\n::: {#8faa4777 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-9-output-1.png){}\n:::\n:::\n\n\n## Dendrogram \n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#dd3f5754 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-10-output-1.png){}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n- Dendrogram is a tree-like plot. \n- On the x-axis we have data points. \n- On the y-axis we have distances between clusters. \n:::\n::::\n\n## Flat clusters\n\n- This is good but how can we get cluster labels from a dendrogram? \n- We can bring the clustering to a \"flat\" format use `fcluster`\n\n## Flat clusters\n\n::: {#e12a736d .cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import fcluster\n# flattening the dendrogram based on maximum number of clusters. \nhier_labels1 = fcluster(linkage_array, 3, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels1, title=\"flattened with max_clusts=3\")\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-11-output-1.png){}\n:::\n:::\n\n\n## Linkage criteria {.smaller}\n- When we create a dendrogram, we need to calculate distance between clusters. How do we measure distances between clusters? \n- The **linkage criteria** determines how to find similarity between clusters:\n- Some example linkage criteria are: \n    - Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n    - Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n    - Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n    - Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters\n\n## Activity\n\n- Fill in the table below in this Google doc: https://shorturl.at/3yOdg\n\n| **Clustering Method**  | **KMeans**                                           | **DBSCAN**                                          | **Hierarchical Clustering**                            |\n|------------------------|------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------|\n| **Approach**           | | |\n| **Hyperparameters**    | | |\n| **Shape of clusters**  | | |\n| **Handling noise**     | | | \n| **Examples**           | | |\n\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_15-dbscan-hierarchical.ipynb)\n\n",
    "supporting": [
      "slides-15-DBSCAN-hierarchical_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}