{
  "hash": "1435292344e467189f2a1fea95d007b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering'\ndescription: \"Unsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset,  importance of input data representation in clustering.\"\nformat:\n    revealjs:\n        html-math-method: mathjax\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\neditor:\n  render-on-save: true\n---\n\n\n## Happy Halloween\n\n![](img/eva-halloween.png)\n\n## Announcements \n\n- HW6 is due next week Monday\n  - Computationally intensive \n  - You need to install many packages \n\n## Imports \n\n::: {#bc1ad874 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-2-output-1.png){}\n:::\n:::\n\n\n## iClicker Exercise 15.1 {.smaller}\n\n**Select all of the following statements which are TRUE.**\n\n- (A) With $n$ examples, $k$ clusters, and $d$ features, K-Means learns $k$ cluster centers, each $d$-dimensional. \n- (B) The meaning of $k$ in K-nearest neighbours and K-Means clustering is very similar. \n- (C) Scaling of input features is crucial in clustering.  \n- (D) In clustering, it's almost always a good idea to find equal-sized clusters. \n\n# K-means Limitations {.smaller}\n\n## Shape of clusters\n- Good for spherical clusters of more or less equal sizes \n![](img/kmeans_boundaries.png)\n\n## K-Means: failure case 1\n\n- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). \n\n::: {#2090d40b .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-3-output-1.png){}\n:::\n:::\n\n\n## K-Means: failure case 2\n\n- Again, K-Means is unable to capture complex cluster shapes. \n\n::: {#b47ac38d .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-4-output-1.png){}\n:::\n:::\n\n\n## K-Means: failure case 3\n\n- It assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters. \n\n::: {#7ef61fd8 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-5-output-1.png){}\n:::\n:::\n\n\n# Can we do better? \n\n## DBSCAN {.smaller}\n\n- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise\n- A density-based clustering algorithm\n\n::: {#379057ea .cell execution_count=5}\n``` {.python .cell-code}\nX, y = make_moons(n_samples=200, noise=0.08, random_state=42)\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(X)\nplot_original_clustered(X, dbscan, dbscan.labels_)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-6-output-1.png){}\n:::\n:::\n\n\n## Two main hyperparameters\n\nIn order to identify dense regions, we need two hyperparameters: \n\n- `eps`: determines what it means for points to be \"close\"\n- `min_samples`: determines the number of **neighbouring points** we require to consider in order for a point to be part of a cluster\n\n\n## DBSCAN Analogy {.smaller}\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#6650768f .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-7-output-1.png){}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\nConsider DBSCAN in a social context: \n\n- Social butterflies (ü¶ã): Core points\n- Friends of social butterflies who are not social butterflies: Border points\n- Lone wolves (üê∫): Noise points  \n:::\n::::\n\n\n## **DBSCAN algorithm** {.smaller}\n::: {.columns}\n::: {.column width=\"50%\"}\n\n![](img/DBSCAN_search.gif)\n:::\n::: {.column width=\"50%\"}\n\n- Pick a point $p$ at random.\n- Check whether $p$ is a \"core\" point or not. \n- If $p$ is a core point, give it a colour (label). \n- Spread the colour of $p$ to all of its neighbours.\n- Check if any of the neighbours that received the colour is a core point, if yes, spread the colour to its neighbors as well.\n- Once there are no more core points left to spread the colour, pick a new unlabeled point $p$ and repeat the process.\n:::\n::::\n\n## DBSCAN: failure cases {.smaller}\n\n- Let's consider this dataset with three clusters of varying densities.  \n- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n\n::: {#1a43c631 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-8-output-2.png){}\n:::\n:::\n\n\n## Hierarchical clustering \n\n::: {#4d094275 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-9-output-1.png){}\n:::\n:::\n\n\n## Dendrogram \n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#9b3904f6 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-10-output-1.png){}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n- Dendrogram is a tree-like plot. \n- On the x-axis we have data points. \n- On the y-axis we have distances between clusters. \n:::\n::::\n\n## Flat clusters\n\n- This is good but how can we get cluster labels from a dendrogram? \n- We can bring the clustering to a \"flat\" format use `fcluster`\n\n## Flat clusters\n\n::: {#fa6196e1 .cell execution_count=10}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import fcluster\n# flattening the dendrogram based on maximum number of clusters. \nhier_labels1 = fcluster(linkage_array, 3, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels1, title=\"flattened with max_clusts=3\")\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-11-output-1.png){}\n:::\n:::\n\n\n## Linkage criteria {.smaller}\n- When we create a dendrogram, we need to calculate distance between clusters. How do we measure distances between clusters? \n- The **linkage criteria** determines how to find similarity between clusters:\n- Some example linkage criteria are: \n    - Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n    - Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n    - Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n    - Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters\n\n## Example: Single linkage {.smaller}\n\nSuppose you want to go from 3 clusters to 2 clusters. Which clusters would you merge? \n\n::: {#d6b7091a .cell execution_count=11}\n``` {.python .cell-code}\nX_orig, y = make_blobs(random_state=0, n_samples=11)\nX = StandardScaler().fit_transform(X_orig)\nlinkage_array = single(X)\nhier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 3\", color_threshold=1.0)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-12-output-1.png){}\n:::\n:::\n\n\n##  Example: Single linkage {.smaller}\n\n::: {#d616f0b1 .cell execution_count=12}\n``` {.python .cell-code}\nhier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 2\", color_threshold=1.0)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-15-DBSCAN-hierarchical_files/figure-revealjs/cell-13-output-1.png){}\n:::\n:::\n\n\n## iClicker Exercise 15.2 {.smaller}\n\n**Select all of the following statements which are True**\n\n- (A) In hierarchical clustering we do not have to worry about initialization.\n- (B) Hierarchical clustering can only be applied to smaller datasets because dendrograms are hard to visualize for large datasets.\n- (C) In all the clustering methods we have seen (K-Means, GMMs, DBSCAN, hierarchical clustering), there is a way to decide the number of clusters.\n- (D) To get robust clustering we can naively ensemble cluster labels (e.g., pick the most popular label) produced by different clustering methods.\n- (E) If you have a high Silhouette score and very clean and robust clusters, it means that the algorithm has captured the semantic meaning in the data of our interest.\n\n\n## Activity\n\nDiscuss the following\n\n| **Clustering Method**  | **KMeans**                                           | **DBSCAN**                                          | **Hierarchical Clustering**                            |\n|------------------------|------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------|\n| **Approach**           | | |\n| **Hyperparameters**    | | |\n| **Shape of clusters**  | | |\n| **Handling noise**     | | | \n| **Distance metric**    | | |\n\n## Discussion question \n\nWhich clustering method would you use in each of the scenarios below? Why? How would you represent the data in each case?\n\n- Scenario 1: Customer segmentation in retail\n- Scenario 2: An environmental study aiming to identify clusters of a rare plant species\n- Scenario 3: Clustering furniture items for inventory management and customer recommendations\n\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_15-dbscan-hierarchical.ipynb)\n\n",
    "supporting": [
      "slides-15-DBSCAN-hierarchical_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}