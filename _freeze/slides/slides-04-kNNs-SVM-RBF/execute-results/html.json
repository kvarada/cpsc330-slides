{
  "hash": "1e7d6c6606736e7f6fc9877a104909ee",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Lecture 4: $k$-nearest neighbours and SVM RBFs'\nauthor: \"Varada Kolhatkar\"\ndescription: Supervised Machine Learning Fundamentals\ndescription-short: 'introduction to KNNs, hyperparameter `n_neighbours` or $k$, `C` and `gamma` hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.'\nformat:\n  revealjs:\n    html-math-method: mathjax\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n\n## Announcements \n\n- hw2 is due tonight.\n- Syllabus quiz due date is September 19th, 11:59 pm. \n- Homework 3 (hw3) has been released (Due: Sept 29th, 11:59 pm)\n  - You can work in pairs for this assignment. \n- If you were on the waitlist, you should now know your course enrollment status. \n- The [lecture notes here](https://ubc-cs.github.io/cpsc330-2025W1/lectures/notes/01_intro.html) align with the content presented in the videos. Even though we do not cover all the content from these notebooks during lectures, it's your responsibility to go through them on your own.\n\n\n## Recap: iClicker overfitting 1\n\nWhich of the following scenarios do **NOT necessarily imply overfitting**? \n\n- (A) Training accuracy is very high (0.98) while validation accuracy is much lower (0.60).\n- (B) In a wildlife classifier, the model predicts \"wolf\" whenever there\"s snow in the background, because all wolf photos were taken in snowy regions.\n- (C) The decision boundary of a classifier is wiggly and highly irregular.\n- (D) Training and validation accuracies are both approximately 0.88. \n- (E) A cancer detection model learns that \"a ruler in the corner of the X-ray\" means positive, because doctors tended to measure suspicious cases.\n\n## Recap: iClicker overfitting 2\n\nWhich of the following statements about **overfitting** is true? \n\n- (A) Overfitting makes the model more accurate on both training and unseen data.\n- (B) Overfitting means the model captures noise or irrelevant details from the training data.\n- (C) Overfitting is desirable because it reduces both training and test error.\n- (D) In real-world problems, models are always at risk of overfitting if not properly validated.\n\n## Recap: iClicker underfitting\n\nHow might one address the issue of **underfitting** in a machine learning model. \n\n- (A) Introduce more noise to the training data. \n- (B) Remove features that might be relevant to the prediction. \n- (C) Increase the model's complexity (e.g., more parameters, features, or deeper trees)\n- (D) Use a smaller dataset for training. \n\n## The fundamental tradeoff {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](img/malp_0201.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"40%\"}\n- As you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.  \n- How to pick a model? \n:::\n::::\n\n## iClicker 4.1\n\n**Select all of the following statements which are TRUE.**\n\n- (A) Analogy-based models find examples from the test set that are most similar to the query example we are predicting.\n- (B) Euclidean distance will always have a non-negative value.\n- (C) With $k$-NN, setting the hyperparameter $k$ to larger values typically reduces training error. \n- (D) Similar to decision trees, $k$-NNs finds a small set of good features.\n- (E) In $k$-NN, with $k > 1$, the classification of the closest neighbour to the test example always contributes the most to the prediction.\n\n## iClicker 4.2\n\n**Select all of the following statements which are TRUE.**\n\n- (A) $k$-NN may perform poorly in high-dimensional space (say, *d* > 1000). \n- (B) In sklearnâ€™s SVC classifier, large values of `gamma` tend to result in higher training score but probably lower validation score. \n- (C) If we increase both `gamma` and `C`, we can't be certain if the model becomes more complex or less complex.\n\n\n## Similarity-based algorithms \n- Use similarity or distance metrics to predict targets.\n- Examples: $k$-nearest neighbors, Support Vector Machines (SVMs) with RBF Kernel.\n\n## $k$-nearest neighbours\n- Classifies an object based on the majority label among its $k$ closest neighbors. \n- Main hyperparameter: $k$ or `n_neighbors` in `sklearn`\n- Distance Metrics: Euclidean\n- Strengths: simple and intuitive, can learn complex decision boundaries\n- Challenges: Sensitive to the choice of distance metric and **scaling** (coming up).\n\n## Curse of dimensionality \n- As dimensionality increases, the volume of the space increases exponentially, making the data sparse.\n- Distance metrics lose meaning\n    - Accidental similarity swamps out meaningful similarity\n    - All points become almost equidistant.\n- Overfitting becomes likely: Harder to generalize with high-dimensional data.\n- How to deal with this? \n    - Dimensionality reduction (PCA) (not covered in this course)\n    - Feature selection techniques.\n\n## SVMs with RBF kernel \n- RBF Kernel: Radial Basis Function, a way to transform data into higher dimensions implicitly.\n- Strengths \n    - Effective in high-dimensional and sparse data\n    - Good performance on non-linear problems.\n- Hyperparameters:\n    - $C$: Regularization parameter (trade-off between correct classification of training examples and maximization of the decision margin).\n\t- gamma ($\\gamma$): controls how fast the similarity decays with distance\n\n## Intuition of `C` and `gamma` in SVM RBF\n- `C` (Regularization): Controls the trade-off between perfect training accuracy and having a simpler decision boundary. \n    - High C: Strict, complex boundary (overfitting risk).\n    - Low C: More errors allowed, smoother boundary (generalizes better).\n- `Gamma` (Kernel Width): Controls the influence of individual data points.\n\t- High Gamma: Points have local impact, complex boundary.\n\t- Low Gamma: Points affect broader areas, smoother boundary.\n- Key trade-off: Proper balance between `C` and `gamma` is crucial for avoiding overfitting or underfitting.\n\n\n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_04-kNNs-SVMs.ipynb)\n\n# Models\n## Supervised models we have seen \n\n- Decision trees: Split data into subsets based on feature values to create decision rules \n- $k$-NNs: Classify based on the majority vote from $k$ nearest neighbors\n- SVM RBFs: Create a boundary using an RBF kernel to separate classes\n\n## Comparison of models (activity)\n| **Model**        | Parameters and hyperparameters | **Strengths**  | **Weaknesses**     |\n|------------------|--------------------------------|---------------------------|---------------------------|\n| **Decision Trees**               |  |  |  |\n| **KNNs**              |  |  |  |\n| **SVM RBF**            |  |  |  |\n\n",
    "supporting": [
      "slides-04-kNNs-SVM-RBF_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}