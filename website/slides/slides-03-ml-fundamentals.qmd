---
title: 'CPSC 330 Lecture 3: ML fundamentals'
author: "Varada Kolhatkar"
description: Supervised Machine Learning Fundamentals
description-short: 'generalization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule'
format:
  revealjs:
    html-math-method: mathjax
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/
---

## Announcements 

- Homework 2 (hw2) has been released (Due: Sept 16, 11:59pm)
  - You are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.
  - Group submissions are not allowed for this assignment.
- Advice on keeping up with the material
  - Practice!
  - Start early on homework assignments.
- If you are still on the waitlist, it's your responsibility to keep up with the material and submit assignments.
- Last day to drop without a W standing: Sept 15 


## iClicker 3.1

Clicker cloud join link: https://join.iclicker.com/FZMQ

**Select all of the following statements which are TRUE.**

- (A) A decision tree model with no depth (the default `max_depth` in `sklearn`) is likely to perform very well on the deployment data.
- (B) Data splitting helps us assess how well our model would generalize.
- (C) Deployment data is scored only once.
- (D) Validation data could be used for hyperparameter optimization.
- (E) It’s recommended that data be shuffled before splitting it into train and test sets.


## iClicker 3.2

Clicker cloud join link: https://join.iclicker.com/FZMQ

**Select all of the following statements which are TRUE.**

- (A) $k$-fold cross-validation calls fit $k$ times
- (B) We use cross-validation to get a more robust estimate of model performance.
- (C) If the mean train accuracy is much higher than the mean cross-validation accuracy it's likely to be a case of overfitting.
- (D) The fundamental tradeoff of ML states that as training error goes down, validation error goes up.
- (E) A decision stump on a complicated classification problem is likely to underfit.

## Recap from videos 
- Why do we split the data? What are train/valid/test splits? 
- What are the benefits of cross-validation?
- What is underfitting and overfitting? 
- What’s the fundamental trade-off in supervised machine learning?
- What is the golden rule of machine learning?

## Summary of train, validation, test, and deployment data 

|         | `fit` | `score` | `predict` |
|----------|-------|---------|-----------|
| Train    | ✔️      | ✔️      | ✔️         |
| Validation |      | ✔️      | ✔️         |
| Test    |       |  once   | once         |
| Deployment    |       |       | ✔️         |

## Cross validation

![](img/cross-validation.png){fig-align="center"}

## Cross validation
```{python}
import mglearn
mglearn.plots.plot_cross_validation()
```

## Overfitting and underfitting 

:::: {.columns}

::: {.column width="60%"}
![](img/underfit-overfit-google-developer.png)

[Source](https://developers.google.com/machine-learning/crash-course/overfitting/overfitting)
:::

::: {.column width="40%"}
- An **overfit model** matches the training set so closely that it fails to make correct predictions on new unseen data.  
- An **underfit model** is too simple and does not even make good predictions on the training data 

:::
::::

## The fundamental tradeoff

As you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.  

:::: {.columns}

::: {.column width="50%"}
![](img/malp_0201.png){fig-align="center"}
:::

::: {.column width="50%"}
- Underfitting: Both accuracies rise
- Sweet spot: Validation accuracy peaks
- Overfitting: Training $\uparrow$, Validation $\downarrow$
- Tradeoff: Balance complexity to avoid both
:::

::::

## The golden rule
- Although our primary concern is the model's performance on the test data, this data should not influence the training process in any way.

:::: {.columns}

::: {.column width="50%"}
![](img/golden_rule_analogy.png){fig-align="center"}
Source: Image generated by ChatGPT 5
:::

::: {.column width="50%"}
- **Test data = final exam**  
- You can practice all you want with training/validation data
- But **never peek** at the test set before evaluation
- Otherwise, it's like sneaking answers before the exam $\rightarrow$ **not a real assessment of your learning**.  
:::

::::

# [Class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_03-ml-fundamentals.ipynb)