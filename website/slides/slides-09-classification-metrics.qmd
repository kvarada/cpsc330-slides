---
title: "CPSC 330 Lecture 9: Classification Metrics" 
author: "Varada Kolhatkar"
description: "Metrics for classification"
description-short: "confusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance" 
format:
  revealjs:
    html-math-method: mathjax
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Focus on the breath!

![](img/inukshuk.jpeg){.nostretch fig-align="center" width="600px"}


## Announcements 

- Important information about midterm 1
  - https://piazza.com/class/mekbcze4gyber/post/162
  - **Good news for you: You'll have access to our course notes in the midterm!**
- HW4 was due on Monday, Oct 6th 11:59 pm. 
- HW5 has been released. It's a project-type assignment and you get till Oct 27th to work on it.  


```{python}
import os
import sys
import pandas as pd 
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
%matplotlib inline
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import cross_validate
import mglearn
DATA_DIR = 'data/' 
```

## ML workflow 
![](img/ml-workflow.png)

## Accuracy

- So far, we‚Äôve been measuring model performance using **Accuracy**.  
- **Accuracy** is the proportion of all predictions that were correct ‚Äî whether *positive* or *negative*.  

$$
\text{Accuracy} = \frac{\text{correct classifications}}{\text{total classifications}}
$$

- But is **accuracy** always the right metric to evaluate a model? ü§î  

## A fraud classification example
```{python}
# This dataset will be loaded using a URL instead of a CSV file
DATA_URL = "https://github.com/firasm/bits/raw/refs/heads/master/creditcard.csv"

cc_df = pd.read_csv(DATA_URL, encoding="latin-1")
# Sorting columns so it is easier to read
cc_df = cc_df[['Class', 'Time', 'Amount'] + cc_df.columns[cc_df.columns.str.startswith('V')].to_list()]

train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)
X_train_big, y_train_big = train_df.drop(columns=["Class", "Time"]), train_df["Class"]
X_test, y_test = test_df.drop(columns=["Class", "Time"]), test_df["Class"]
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_big, y_train_big, test_size=0.3, random_state=123
)
print(X_train.shape)
train_df.head()
```

## `DummyClassifier`
Let's try a DummyClassifier, which makes predictions without learning any patterns.

```{python}
#| echo: true
dummy = DummyClassifier()
cross_val_score(dummy, X_train, y_train).mean()
```

- The accuracy looks surprisingly high!
- Should we be happy with this model and deploy it?

## Problem: Class imbalance 

- In many real-world problems, some classes are much rarer than others.

- A model that always predicts "no fraud" could still achieve >99% accuracy!
- This is why accuracy can be misleading in imbalanced datasets.
- We need metrics that differentiate types of errors.

## `DummyClassifier`: Confusion matrix 

Which types of errors would be most critical for the bank to address? Missing a fraud case or flagging a legitimate transaction as fraud?

```{python}
from sklearn.metrics import ConfusionMatrixDisplay  

dummy.fit(X_train, y_train)
cm = ConfusionMatrixDisplay.from_estimator(
    dummy, X_valid, y_valid, values_format="d", display_labels=["Non fraud", "Fraud"]
)
```

## `LogisticRegression`: Confusion matrix

Are we doing better with logistic regression? 

```{python}
from sklearn.metrics import ConfusionMatrixDisplay  
pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(X_train, y_train)
cm = ConfusionMatrixDisplay.from_estimator(
    pipe, X_valid, y_valid, values_format="d", display_labels=["Non fraud", "Fraud"]
)

```

## Understanding the confusion matrix
:::: {.columns}
:::{.column width="80%"}
![](img/tp-fp-tn-fn-fraud.png)
:::

:::{.column width="20%"}
- TN $\rightarrow$ True negatives 
- FP $\rightarrow$ False positives 
- FN $\rightarrow$ False negatives
- TP $\rightarrow$ True positives 
:::
::::

# Practice: confusion matrix terminology

## Confusion matrix questions 

Imagine a spam filter model where emails labeled **1 = spam, 0 = not spam**. 

If a spam email is incorrectly classified as not spam, what kind of error is this?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In an intrusion detection system, **1 = intrusion, 0 = safe**. 

If the system misses an actual intrusion and classifies it as safe, this is a:

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In a medical test for a disease, **1 = diseased, 0 = healthy**. 

If a healthy patient is incorrectly diagnosed as diseased, that's a:

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Metrics other than accuracy 

Now that we understand the different types of errors, we can explore metrics that better capture model performance when **accuracy falls short**, especially for **imbalanced datasets**.

We'll start with three key ones:

- **Precision**
- **Recall**
- **F1-score**

## Precision and recall 

Let's revisit our fraud detection scenario. The circle below represents **all transactions predicted as fraud** by an **imaginary toy model** designed to detect fraudulent activity.

![](img/precision-recall.png){.nostretch fig-align="center" width="600px"}
![](img/fraud-precision-recall.png){.nostretch fig-align="center" width="600px"}

## Intuition behind the two metrics

- **Precision** answers: *Of all the transactions predicted as fraud, how many were actually fraud?*  
  - High precision $\rightarrow$ few false alarms (low false positives).

- **Recall** answers: *Of all the actual fraud cases, how many did the model catch?*  
  - High recall $\rightarrow$ few missed frauds (low false negatives).

## Trade-off between precision and recall

- Increasing **recall** often decreases **precision**, and vice versa.  
- Example:  
  - Predict *‚Äúfraud‚Äù* for every transaction $\rightarrow$ perfect recall, terrible precision.  
  - Predict *‚Äúfraud‚Äù* only when 100% sure $\rightarrow$ high precision, low recall.

**The right balance depends on the application and cost of errors.**

## F1-score

- Sometimes, we want a **single metric** that balances precision and recall.  
- The **F1-score** is the **harmonic mean** of the two:

$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

- High **F1** means both precision and recall are strong.  
- Useful when we care about both false positives **and** false negatives.

## Summary

| Metric | What it measures | High value means |
|:--------|:----------------|:------------------|
| **Accuracy** | Overall correctness | Model gets most predictions right |
| **Precision** | Quality of positive predictions | Few false alarms |
| **Recall** | Quantity of true positives caught | Few missed positives |
| **F1-score** | Balance of precision & recall | Both precision and recall are high |

## iClicker Exercise 9.1

**Select all of the following statements which are TRUE.**

- (A) In medical diagnosis, false positives are more damaging than false negatives (assume "positive" means the person has a disease, "negative" means they don't).
- (B) In spam classification, false positives are more damaging than false negatives (assume "positive" means the email is spam, "negative" means they it's not).
- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.
- (D) If method A gets a higher accuracy than method B, that means its recall is also higher.

## Counter examples

Method A - higher accuracy but lower precision

| Negative | Positive
| -------- |:-------------:|
| 90      | 5|
| 5      | 0|

Method B - lower accuracy but higher precision

| Negative | Positive
| -------- |:-------------:|
| 80      | 15|
| 0      | 5|

## Takeaway

- **Accuracy** summarizes overall correctness but hides class-specific behaviour.  
- You can have **high accuracy but poor precision or recall**,  
  especially in **imbalanced datasets**.  
- Always check **multiple metrics** before deciding which model is better.

# Threshold-based classification

## Predicting with logistic regression 

:::: {.columns}
:::{.column width="40%"}
![](img/lr-threshold.png)
:::
:::{.column width="60%"}
- Most classification models don't directly predict labels. They predict scores or probabilities.
- To get a label (e.g., ‚Äúspam‚Äù or ‚Äúnot spam‚Äù), we choose a threshold (often 0.5).
If the threshold changes, predictions change, and so do the errors. 
- What happens to precision and recall if we change the probability threshold? 
- [Play with classification thresholds](https://developers.google.com/machine-learning/crash-course/classification/thresholding)

::: 
::::

::: {.notes}
- Decreasing the threshold:
  - Increases the number of positive predictions
  - Identify more examples as "fraud" examples
  - More false positives, more true positives. 
  - Recall would either stay the same or go up and precision is likely to go down (precision may increase if all the new examples after decreasing the threshold are TPs).
- Increasing the threshold:
  - Decreases the number of positive predictions
  - Recall would go down or stay the same but precision is likely to go up (precision may go down if TP decrease but FP do not decrease).

- Key question: How do we decide on the best threshold? It depends on the problem and trade-offs between false positives and false negatives.
:::

## PR curve
- Calculate precision and recall (TPR) at every possible threshold and graph them. 
- Top left $\rightarrow$ Threhold of 1 (strict model = high precision)
- Bottom right $\rightarrow$ Threhold of 0 (linient model = high recall)

```{python}
from sklearn.metrics import PrecisionRecallDisplay

PrecisionRecallDisplay.from_estimator(
    pipe,
    X_valid,
    y_valid,
);
```

::: {.notes}

- We can look at all possible thresholds and plot the corresponding precision and recall! 

- As we lower the threshold, the classifier predicts more positives:
  - Recall increases because we capture more true positives.
  - Precision usually decreases because some of the additional positives are false positives.

- As we raise the threshold, the classifier predicts fewer positives:
  - Precision increases because only the most confident predictions are positive.
  - Recall decreases because we miss more true positives.

- The curve shows the trade-offs between precision and recall across all thresholds.
- Ideal performance: top-right corner (high precision and high recall).
- Use cases like fraud detection often require focusing on areas of the curve that favor one metric over the other (e.g., high recall for safety-critical tasks).
:::

## PR curve different thresholds

- Which of the red dots are reasonable trade offs? 

![](img/pr-curve-different-thresholds.png)

## Average Precision (AP) Score 

- AP score summarizes the PR curve by calculating the area under the curve 

```{python}
from sklearn.svm import SVC
num_iter = 1000 # change to 1000 

pipe_svc = make_pipeline(StandardScaler(), SVC(max_iter=num_iter))
pipe_svc.fit(X_train, y_train)
pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=num_iter))
pipe_lr.fit(X_train, y_train)

_, ax = plt.subplots()

PrecisionRecallDisplay.from_estimator(
    pipe_lr,
    X_valid,
    y_valid,
    name="LR",
    ax=ax
);
PrecisionRecallDisplay.from_estimator(
    pipe_svc,
    X_valid,
    y_valid,
    name="SVC",
    ax=ax
);
```
## Exercise: choose the appropriate evaluation metric

- Scenario 1: Balance between precision and recall for a threshold.
- Scenario 2: Assess performance across all thresholds.

**A.** F1 for 1, AP for 2

**B.** AP for 1, F1 Score for 2

**C.** AP for both 

**D.** F1 for both 

::: {.notes}
- F1 score is for a given threshold and measures the quality of `predict`.
- AP score is a summary across thresholds and measures the quality of `predict_proba`.
:::

## iClicker Exercise 9.2

**Select all of the following statements which are TRUE.**

- (A) If we increase the classification threshold, both true and false positives are likely to decrease.
- (B) If we increase the classification threshold, both true and false negatives are likely to decrease.
- (C) Lowering the classification threshold generally increases the model‚Äôs recall.  
- (D) Raising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives.


## ROC curve 
- Calculate the true positive rate (TPR) and false positive rate (FPR) at every possible thresholding and graph TPR over FPR. 
- Good choice when the datasets are roughly balanced. 
![](img/roc-curve-example.png)

## AUC 
- The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.


## ROC AUC questions

Consider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?

:::: {.columns}

:::{.column width="50%"}
![](img/auc_abc)
:::

:::{.column width="50%"}

- (A) If false positives (false alarms) are highly costly
- (B) If false positives are cheap and false negatives (missed true positives) highly costly
- (C) If the costs are roughly equivalent
:::
::::

[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)
