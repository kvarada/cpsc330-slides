---
title: "CPSC 330 Lecture 8: Hyperparameter Optimization"
author: "Varada Kolhatkar"
description: "Linear regression, logistic regression"
description-short: "Linear regression, logistic regression, prediction probabilities, sigmoid, interpretation of coefficients"
format:
  revealjs:
    html-math-method: mathjax
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Announcements 

- Important information about midterm 1
  - https://piazza.com/class/mekbcze4gyber/post/162
- HW3 was due on Monday, Sept 29th 11:59 pm. 
- HW4 has been released 


```{python}
import os
import sys
import pandas as pd 
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from scipy.stats import expon, lognorm, loguniform, randint, uniform, norm, randint
%matplotlib inline
import mglearn
DATA_DIR = 'data/' 
```

## Recap: iClicker LR1

Which of the following are **True**?

- (A) Logistic regression can be used for binary as well as multi-class classification tasks.  
- (B) Logistic regression computes a weighted sum of features and applies the sigmoid function.  
- (C) The sigmoid function ensures outputs between 0 and 1, interpreted as probabilities.  
- (D) The decision boundary in logistic regression is linear, even though the sigmoid is applied. 
- (E) When the weighted sum is 0, $\hat{p}$ = 0.5.  

## Recap: iClicker LR2

Which of the following are **True**?

- (A) Logistic regression coefficients always have to be positive.  
- (B) Larger coefficients (in absolute value) indicate stronger feature influence on the prediction.  
- (C) For $d$ features, the decision boundary is a $d-1$ dimensional hyperplane.  
- (D) In `sklearn`, very small `C` value shrinks coefficients, often leading to underfitting.  
- (E) A larger `C` value allows larger coefficients and a more complex model.  


## Recap: Logistic regression
- A **linear model used for binary classification** tasks. 
  - (Optional) There is am extension of logistic regression called multinomial logistic regression for multiclass classification.
- Parameters: 
  - **Coefficients (Weights)**: The model learns a coefficient or a weight associated with each feature that represents its importance.
  - **Bias (Intercept)**: A constant term added to the linear combination of features and their coefficients.

## Recap: Logistic regression 
- The model computes a weighted sum of the input features’ values, adjusted by their respective coefficients and the bias term.
- This weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the "positive" class.

$$ \hat{p} = \sigma\left(\sum_{i=1}^d w_i x_i + b\right) $$


- $P_{hat}$ is the predicted probability of the example belonging to the positive class. 
- $w_i$ is the learned weight associated with feature $i$
- $x_i$ is the value of the input feature $i$
- $b$ is the bias term 

## Recap: Logistic regression

- For a dataset with $d$ features, the decision boundary that 
separates the classes is a $d-1$ dimensional hyperplane.  
- Complexity hyperparameter: `C` in `sklearn`. 
  - Higher `C` $\rightarrow$ more complex model meaning larger coefficients
  - Lower `C` $\rightarrow$ less complex model meaning smaller coefficients

## Data

```{python}
#| echo: true
sms_df = pd.read_csv(DATA_DIR + "spam.csv", encoding="latin-1")
sms_df = sms_df.drop(columns = ["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"])
sms_df = sms_df.rename(columns={"v1": "target", "v2": "sms"})
train_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)
X_train, y_train = train_df["sms"], train_df["target"]
X_test, y_test = test_df["sms"], test_df["target"]
train_df.head(4)
```

## Model building 

- Let's define a pipeline 

```{python}
#| echo: true

pipe_svm = make_pipeline(CountVectorizer(), SVC())
```

- Suppose we want to try out different hyperparameter values. 
```{python}
#| echo: true

parameters = {
    "max_features": [100, 200, 400],
    "gamma": [0.01, 0.1, 1.0],
    "C": [0.01, 0.1, 1.0],
}
```

## Hyperparameter optimization with loops

- Define a parameter space.
- Iterate through possible combinations.
- Evaluate model performance.
- What are some limitations of this approach? 


## `sklearn` methods 

- `sklearn` provides two main methods for hyperparameter optimization
  - Grid Search
  - Random Search

## Grid Search 

- Covers all possible combinations from the provided grid. 
- Can be parallelized easily.
- Integrates cross-validation.


## Grid search example 
```{python}
#| echo: true

from sklearn.model_selection import GridSearchCV

pipe_svm = make_pipeline(CountVectorizer(), SVC())

param_grid = {
    "countvectorizer__max_features": [100, 200, 400],
    "svc__gamma": [0.01, 0.1, 1.0],
    "svc__C": [0.01, 0.1, 1.0],
}
grid_search = GridSearchCV(pipe_svm, 
                  param_grid = param_grid, 
                  n_jobs=-1, 
                  return_train_score=True
                 )
grid_search.fit(X_train, y_train)
grid_search.best_score_
```

## Random Search 
- More efficient than grid search when dealing with large hyperparameter spaces.
- Samples a given number of parameter settings from distributions.

![](img/randomsearch_bergstra.png)

## Random search example 

```{python}
#| echo: true

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

pipe_svc = make_pipeline(CountVectorizer(), SVC())

param_dist = {
    "countvectorizer__max_features": randint(100, 2000), 
    "svc__C": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),
    "svc__gamma": loguniform(1e-5, 1e3),
}
random_search = RandomizedSearchCV(pipe_svm,                                    
                  param_distributions = param_dist, 
                  n_iter=10, 
                  n_jobs=-1, 
                  return_train_score=True)

# Carry out the search
random_search.fit(X_train, y_train)
random_search.best_score_
```

# Optimization bias 

## Pizza baking competition example

Imagine that you participate in pizza baking competition. 

:::: {columns}
::: {.column width="50%"}

![](img/margherita-pizza.jpeg)
:::

::: {.column width="50%"}
- **Training phase**: Collecting recipes and practicing at home  
- **Validation phase**: Inviting a group of friends for tasting and feedback  
- **Test phase (competition day)**: Serving the judges  
:::
::::

## Overfitting on the validation set
:::: {columns}
::: {.column width="50%"}
- Your friends loved your pineapple pizza. 
- You fine-tune your recipe for the same group of friends, perfecting it for their tastes.

<img src="img/pineapple-pizza.png" width="60%" alt="Pineapple Pizza">
:::

::: {.column width="50%"}
- On the competition day, you confidently present your perfected pineapple pizza.  
- Judges are not impressed: *“This doesn’t appeal to a broad audience.”*  
![](img/eva-sad.png)

:::
::::

This is similar to reusing the same validation set again and again to perfect the model for it! 

## Lesson: Overfitting on the validation set {.smaller}

- You tailored your recipe too closely to your friends’ tastes.  
- They were **not representative** of the broader audience (the judges).  
- The pizza, while perfect for your validation group, failed to **generalize**.  
- Over many iterations, the validation set no longer gives an **unbiased estimate** of performance.  
- That’s why we need a **separate test set**—like a group of tasters who never influenced your pizza.  

## Optimization bias
- Why do we need separate validation and test datasets? 
![](img/optimization-bias.png)

## Mitigating optimization bias.
  - Cross-validation
  - Ensembles 
  - Regularization and choosing a simpler model  

## (iClicker) Exercise 8.1

Select all of the following statements which are TRUE.

- (A) If you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.
- (B) Grid search is guaranteed to find the best hyperparameter values.
- (C) It is possible to get different hyperparameters in different runs of RandomizedSearchCV.

## Questions for you

- You have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?
  - Probably
  - Probably not


## Questions for class discussion

- Suppose you have 10 hyperparameters, each with 4 possible values. If you run `GridSearchCV` with this parameter grid, how many cross-validation experiments will be carried out? 
- Suppose you have 10 hyperparameters and each takes 4 values. If you run `RandomizedSearchCV` with this parameter grid with `n_iter=20`, how many cross-validation experiments will be carried out? 

# [Class Demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_08_hyperparameter-optimization.ipynb)

