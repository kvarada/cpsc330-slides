{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"CPSC 330 Lecture 10: Regression Metrics\" \n",
        "author: \"Varada Kolhatkar\"\n",
        "description: \"Metrics for Regression\"\n",
        "description-short: \"Ridge, \" \n",
        "format:\n",
        "  revealjs:\n",
        "    html-math-method: mathjax\n",
        "    embed-resources: true\n",
        "    slide-number: true\n",
        "    smaller: true\n",
        "    center: true\n",
        "    logo: img/UBC-CS-logo.png\n",
        "    resources:\n",
        "      - data/\n",
        "      - img/  \n",
        "---"
      ],
      "id": "e299cbe0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import ConfusionMatrixDisplay \n",
        "import mglearn\n",
        "DATA_DIR = 'data/' "
      ],
      "id": "644eb6b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Announcements \n",
        "\n",
        "- Important information about midterm 1\n",
        "  - https://piazza.com/class/mekbcze4gyber/post/162\n",
        "  - **Good news for you: You'll have access to our course notes in the midterm!**\n",
        "- HW4 was due on Monday, Oct 6th 11:59 pm. \n",
        "- HW5 has been released. It's a project-type assignment and you get till Oct 27th to work on it.  \n",
        "\n",
        "\n",
        "## Focus on the breath!\n",
        "\n",
        "![](img/inukshuk.jpeg){.nostretch fig-align=\"center\" width=\"500px\"}\n",
        "\n",
        "\n",
        "## Which Metric Fits Best? {.smaller}\n",
        "\n",
        "| Scenario | Data Imbalance | Main Concern | Best Metric(s) / Curve |\n",
        "|-----------|----------------|---------------|------------------------|\n",
        "| **Email Spam Detection** | 10% spam | Avoid false positives| |\n",
        "| **Disease Screening** | 1 in 10,000 | Avoid false negatives | |\n",
        "| **Credit Card Fraud** | 0.1% fraud | Focus on rare positive class | |\n",
        "| **Customer Churn** | 20% churn | Balance FP & FN | |\n",
        "| **Sentiment Analysis** | 50/50 balanced | Overall correctness | |\n",
        "| **Face Recognition** | Balanced pairs | Trade-off FP vs FN | |\n",
        "\n",
        "## Choosing the Right Metric {.smaller}\n",
        "\n",
        "| Metric / Plot | When to Use | Why |\n",
        "|----------------|-------------|-----|\n",
        "| **Precision, Recall, F1** | When you care about *specific error types* (FP vs FN) or a *fixed threshold*. | Focus on particular tradeoffs. |\n",
        "| **PR Curve & AP Score** | When the dataset is **highly imbalanced** (rare positives). | Ignores TNs; focuses on positives. |\n",
        "| **ROC Curve & AUC** | When classes are **moderately imbalanced**. | Measures ranking ability across thresholds. |\n",
        "\n",
        "### ROC \n",
        "\n",
        "How often does the model assign higher scores to positives than to negatives?\n",
        "\n",
        "\t•\tPerfect model: AUC = 1.0\n",
        "\t•\tRandom model: AUC = 0.5\n",
        "\n",
        "## Questions for you \n",
        "\n",
        "- What's the difference between the average precision (AP) score and F1-score? \n",
        "- Which model would you pick? \n",
        "\n",
        "![](img/pr-curve-which-model.png)\n",
        "\n",
        "## Questions for you {.smaller}\n",
        ":::: {.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "![](img/roc-baseline.png) \n",
        ":::\n",
        ":::{.column width=\"40%\"}\n",
        "- What's the AUC of a baseline model? \n",
        ":::\n",
        "::::\n",
        "\n",
        "[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
        "\n",
        "## Questions for you {.smaller}\n",
        "\n",
        ":::: {.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "![](img/roc-curve-which-model.png)\n",
        ":::\n",
        ":::{.column width=\"40%\"}\n",
        "- Which model would you pick? \n",
        ":::\n",
        "::::\n",
        "\n",
        "## Dealing with class imbalance\n",
        "- Under sampling \n",
        "- Oversampling \n",
        "- `class weight=\"balanced\"` (preferred method for this course)\n",
        "- SMOTE\n",
        "\n",
        "## Handling imbalance by chaning class weights\n",
        "\n",
        "- We can specify `class_weight=\"balanced\"` (weight each class inversely proportional to their frequency.)\n"
      ],
      "id": "fbd405b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This dataset will be loaded using a URL instead of a CSV file\n",
        "DATA_URL = \"https://github.com/firasm/bits/raw/refs/heads/master/creditcard.csv\"\n",
        "\n",
        "cc_df = pd.read_csv(DATA_URL, encoding=\"latin-1\")\n",
        "# Sorting columns so it is easier to read\n",
        "cc_df = cc_df[['Class', 'Time', 'Amount'] + cc_df.columns[cc_df.columns.str.startswith('V')].to_list()]\n",
        "\n",
        "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
        "X_train_big, y_train_big = train_df.drop(columns=[\"Class\", \"Time\"]), train_df[\"Class\"]\n",
        "X_test, y_test = test_df.drop(columns=[\"Class\", \"Time\"]), test_df[\"Class\"]\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_big, y_train_big, test_size=0.3, random_state=123\n",
        ")\n",
        "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=num_iter))\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "\n",
        "pipe_lr_balanced = make_pipeline(\n",
        "    StandardScaler(), LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        ")\n",
        "pipe_lr_balanced.fit(X_train, y_train)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    pipe_lr,\n",
        "    X_valid,\n",
        "    y_valid,\n",
        "    values_format=\"d\",\n",
        "    ax=ax[0],\n",
        "    colorbar=False\n",
        ")\n",
        "ax[0].set_title(\"LR\")\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    pipe_lr_balanced,\n",
        "    X_valid,\n",
        "    y_valid,\n",
        "    values_format=\"d\",\n",
        "    ax=ax[1], \n",
        "    colorbar=False\n",
        ")\n",
        "ax[1].set_title(\"balanced\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "29ea6889",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "- This sets the weights so that the classes are \"equal\".\n",
        "- We have reduced false negatives but we have many more false positives now ...\n",
        "  - Decreases false negatives, which improves recall.\n",
        "  - Increases false positives, which might lower precision.\n",
        ":::\n",
        "\n",
        "# [Regression metrics class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_10-regression-metrics.ipynb)\n",
        "\n",
        "## Ridge and RidgeCV\n",
        "- **Ridge Regression**: `alpha` hyperparameter controls model complexity.\n",
        "- **RidgeCV**: Ridge regression with built-in cross-validation to find the optimal `alpha`.\n",
        "\n",
        "## `alpha` hyperparameter\n",
        "- **Role of `alpha`**:\n",
        "  - Controls model complexity\n",
        "  - Higher `alpha`: Simpler model, smaller coefficients.\n",
        "  - Lower `alpha`: Complex model, larger coefficients.\n",
        "\n",
        "## Regression metrics: MSE, RMSE, MAPE\n",
        "\n",
        "- **Mean Squared Error (MSE)**: Average of the squares of the errors.\n",
        "- **Root Mean Squared Error (RMSE)**: Square root of MSE, same units as the target variable.\n",
        "- **Mean Absolute Percentage Error (MAPE)**: Average of the absolute percentage errors.\n",
        "\n",
        "## Applying log transformation to the targets\n",
        "\n",
        "- Suitable when the target has a wide range and spans several orders of magnitude \n",
        "  - Example: counts data such as social media likes or price data\n",
        "- Helps manage skewed data, making patterns more apparent and regression models more effective.\n",
        "- `TransformedTargetRegressor`\n",
        "  - Wraps a regression model and applies a transformation to the target values.\n",
        "\n",
        "\n",
        "## iClicker Exercise 10.1\n",
        "\n",
        "**Select all of the following statements which are TRUE.**\n",
        "\n",
        "- (A) Price per square foot would be a good feature to add in our `X`. \n",
        "- (B) The `alpha` hyperparameter of `Ridge` has similar interpretation of `C` hyperparameter of `LogisticRegression`; higher `alpha` means more complex model. \n",
        "- (C) In `Ridge`, smaller alpha means bigger coefficients whereas bigger alpha means smaller coefficients.  \n",
        "\n",
        "\n",
        "## iClicker Exercise 10.2\n",
        "\n",
        "**Select all of the following statements which are TRUE.**\n",
        "\n",
        "- (A) We can use still use precision and recall for regression problems but now we have other metrics we can use as well.\n",
        "- (B) In `sklearn` for regression problems, using `r2_score()` and `.score()` (with default values) will produce the same results.\n",
        "- (C) RMSE is always going to be non-negative.\n",
        "- (D) MSE does not directly provide the information about whether the model is underpredicting or overpredicting.\n",
        "- (E) We can pass multiple scoring metrics to `GridSearchCV` or `RandomizedSearchCV` for regression as well as classification problems. "
      ],
      "id": "4b28961b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/kvarada/miniforge3/envs/cpsc330/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}