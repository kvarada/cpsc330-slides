---
title: "CPSC 330 Lecture 18: Introduction to deep learning and computer vision"
author: "Varada Kolhatkar"
format: 
    revealjs:
      html-math-method: mathjax
      slide-number: true
      center: true
      logo: img/UBC-CS-logo.png
      resources:
        - data/
        - img/        
---

```{python}
import mglearn
import json
import numpy as np
import pandas as pd
import sys, os
sys.path.append(os.path.join(os.path.abspath("."), "code"))
from deep_learning_code import *
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from torchvision import datasets, models, transforms, utils
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.image as mpimg
%matplotlib inline
```

## Announcements

- HW7 was due yesterday 
- HW8 has been released (due next week Monday)
  - Almost there! Hang in there üòä
- Midterm 2 grading is in progress. 

# iClicker 18.0

**Select all of the following statements which are TRUE for you.**

- (A) I found the multiple-choice questions challenging.
- (B) The coding questions took a lot of time.
- (C) I didn't like the format of the midterm.
- (D) I appreciated the mix of coding, conceptual, and multiple-choice questions.
- (E) I felt the midterm was a good reflection of what we cover in the lectures and homework assignments.

## iClicker 18.1 {.smaller}

**Select all of the following statements which are TRUE.**

- (A) It's possible to use word2vec embedding representations for text classification instead of bag-of-words representation. 
- (B) The topic model approach we used in the last lecture, Latent Dirichlet Allocation (LDA), is an unsupervised approach. 
- (C) In an LDA topic model, the same word can be associated with two different topics with high probability.
- (D) In an LDA topic model, a document is a mixture of multiple topics. 
- (E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output. 

# Multiclass classification 

## What is multiclass classification? 

- So far, we've focused on binary classification (two classes).
- But many real-world problems have more than two classes:
  - Classifying types of emotions in text
  - Identifying species of flowers
  - Recognizing objects in an image

**Goal: assign each example to exactly one of $K$ possible classes.**

## How to handle multiple classes? {.smaller}

Consider this dataset with multiple classes. 

```{python} 
hdb_df = pd.read_csv("data/cleaned_hm_10K_subset.csv")
hdb_df.head()
```

## 

```{python}
#| echo: true
hdb_df["target"].value_counts(normalize=True)
```

- ‚ùì Can we use Decision Trees or KNNs for multiclass classification without any significant modifications?

## Extending logistic regression {.smaller}

- What about logistic regression? The binary version works only for two classes.
- For multiclass problems, we use the multinomial (softmax) logistic regression model.
- It learns a separate weight vector and bias for each class:
  - Parameters per class: $d$ weights + 1 bias
	- Total parameters: $(d + 1) \times K$

**Example:**

If we have 10,000 features (e.g., vocabulary size) and 7 moment classes, the model learns $(10{,}000 + 1) \times 7$ parameters.

## Softmax function

- In binary logistic regression, we use the sigmoid function to map scores to probabilities.
- In multiclass, we need:
	- Probabilities that are positive, and
	- Sum to 1 across all classes.

The softmax function does exactly that.

## (Optional) Softmax function 

Given an input, the probability that it belongs to class $j \in \{1, 2, \dots, K\}$ is calculated using the **softmax function**:

$P(y = j \mid x_i) = \frac{e^{w_j^\top x_i + b_j}}{\sum_{k=1}^{K} e^{w_k^\top x_i + b_k}}$

- $x_i$ is the $i^{th}$ example 
- $w_j$ is the weight vector for class $j$.
- $b_j$ is the bias term for class $j$.
- $K$ is the total number of classes.


## Making predictions

1. **Compute Probabilities**:  
   For each class $j$, compute the probability $P(y = j \mid x_i)$ using the softmax function.

2. **Select the Class with the Highest Probability**:  
   The predicted class $\hat{y}$ is:  
   $\hat{y} = \arg \max_{j \in \{1, \dots, K\}} P(y = j \mid x_i)$

## Example: Softmax output {.smaller}

**Query: ‚ÄúI love my students!‚Äù**

| Class              | Raw Score (Logit) | Exponentiated | Normalized Probability |
|:-------------------|------------------:|---------------:|-----------------------:|
| ‚ù§Ô∏è affection        | 2.1  | 8.166 | 0.4684 |
| üèÜ achievement      | 1.8  | 6.050 | 0.3472 |
| ü§ù bonding          | 1.0  | 2.718 | 0.1559 |
| üåÖ enjoy_the_moment | 0.3  | 1.350 | 0.0775 |
| üéÆ leisure          | -0.2 | 0.819 | 0.0470 |
| üåø nature           | -1.0 | 0.368 | 0.0211 |
| üèÉ exercise         | -1.5 | 0.223 | 0.0128 |
| **Total**           | ‚Äî    | ‚Äî     | **1.000** |



## Binary vs multinomial logistic regression {.smaller}

|   **Aspect**                       | **Binary Logistic Regression**              | **Multinomial Logistic Regression**  |
|--------------------------|---------------------------------------------|--------------------------------------|
| **Target variable**      | 2 classes (binary)                          | More than 2 classes (multi-class)    |
| **Getting probabilities**  | Sigmoid                                   | Softmax                              |
| parameters               | $d$ weights, one per feature and the bias term | $d$ weights and a bias term per class |  
| **Output**               | Single probability                          | Probability distribution over classes |
| **Use case**             | Binary classification (e.g., spam detection) | Multi-class classification (e.g., flower species) |


# Deep Learning 

## Remember this picture we saw earlier? 

- **Deep Learning (DL)** is a subset of machine learning 

![](img/ai-ml-dl.png){.nostretch fig-align="center" width="700px"}

## Image classification
\

Have you used search in Google Photos? You can search for "my photos of cat" and it will retrieve photos from your libraries containing cats.
This can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.

## Image classification
\

Image classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.

![](img/cat_variation.png)
<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->

## Neural networks {.smaller}
\

- Neural networks are perfect for these types of problems where local structures are important. 
- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. 
  - [ImageNet Classification with Deep Convolutional
Neural Networks](https://dl.acm.org/doi/10.1145/3065386)
  - Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. 
- Let's go over the basics of a neural network.

## A graphical view of a linear model
:::: {.columns}

:::{.column width="20%"}
```{python}
import mglearn

mglearn.plots.plot_logistic_regression_graph()
```
:::

:::{.column width="80%"}
- Remember this graphical view of linear models? 
- We have 4 features: x[0], x[1], x[2], x[3]
- The output is calculated as $y = x[0]w[0] + x[1]w[1] + x[2]w[2] + x[3]w[3]$
- For simplicity, we are ignoring the bias term. 
:::
::::


## Introduction to neural networks {.smaller}
\

- Neural networks can be viewed a generalization of linear models where we apply a series of transformations.
- Below we are adding one "layer" of transformations in between features and the target. 
- We are repeating the the process of computing the weighted sum multiple times.  
- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. 

```{python}
mglearn.plots.plot_single_hidden_layer_graph()
```

## One more layer of transformations 
\

- Now we are adding one more layer of transformations. 

```{python}
mglearn.plots.plot_two_hidden_layer_graph()
```

## Neural networks {.smaller}
\

- With a neural net, you specify the number of features after each transformation.
  - In the above, it goes from 4 to 3 to 3 to 1.

- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. 
- Neural network = neural net
- Deep learning ~ using neural networks

## Neural networks example and terminology

![](img/nn-10.png)

## (Optional) How does training work in neural networks? (High-Level) {.smaller}

Training a neural network has **two main steps:**

- **Forward pass:**  
  Feed the input through the network to calculate the predicted output using the current parameter values (weights).

- **Backward pass:**  
  - Measure how far the predicted output is from the actual target (this is the **loss**).
  - Calculate how to adjust each parameter to improve future predictions (**gradients**).
  - Update the parameters using these gradients ‚Äî this helps the model improve.

- Summary
  - **Input $\rightarrow$ Forward Pass $\rightarrow$ Loss $\rightarrow$ Backward Pass $\rightarrow$ Parameter Update $\rightarrow$ Repeat**
  - The model repeats this process many times, gradually improving its predictions.

## Why neural networks? {.smaller}

- They can learn very complex functions.
  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.
  - More layers / bigger layers --> more complex model.
  - You can generally get a model that will not underfit. 

- They work really well for structured data:
  - 1D sequence, e.g. timeseries, language
  - 2D image
  - 3D image or video
- They've had some incredible successes in the last 12 years.
- Transfer learning (coming later today) is really useful.  

## Why not neural networks? {.smaller}

- Often they require a lot of data.
- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).
- They have huge numbers of hyperparameters
  - Think of each layer having hyperparameters, plus some overall hyperparameters.
  - Being slow compounds this problem.
- They are not interpretable.
- I don't recommend training them on your own without further training
- Good news
    - You don't have to train your models from scratch in order to use them.
    - I'll show you some ways to use neural networks without training them yourselves. 

## Deep learning software

The current big players are:

1. [PyTorch](http://pytorch.org)
2. [TensorFlow](https://www.tensorflow.org)

Both are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).

<br><br>

## Introduction to computer vision
\

- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. 
- In the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.

## Introduction to computer vision {.smaller}
\

- image classification: is this a cat or a dog?
- object localization: where is the cat in this image?
- object detection: What are the various objects in the image? 
- instance segmentation: What are the shapes of these various objects in the image? 
- and much more...

![](img/vision-apps.jpeg)

<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> 

## Convolutional Neural Networks (CNNs) {.smaller}
- Neural networks come in different shapes depending on the type of data and the task.
- CNNs are commonly used in **image classification, object detection, and medical imaging.**
- What's the problem if we use the above feedforward architecture to learn patterns from images? 

![](img/cnn-1.png){.nostretch fig-align="center" width="400px"}

## Filters 

- Uses filters that "slide" over the input to detect local patterns.

![](img/cnn-4.png)

- Play around with filters: https://setosa.io/ev/image-kernels/

## CNNs 

![](img/cnn-5.gif)

## CNNs big picture

![](img/cnn_big_picture.png)

---

## Combining CNN and NN {.smaller}

- Sometimes you'll want to combine different types of data in a single network
- The most common case is combining tabular data with image data, for example, using both real estate data and images of a house to predict its sale price:

![](img/multi-input.png)

Source: "[House](https://www.flickr.com/photos/68089229@N06/17458373552)" by [oatsy40](https://www.flickr.com/photos/68089229@N06), "[House in Vancouver](https://www.flickr.com/photos/17573364@N00/433449690)" by [pnwra](https://www.flickr.com/photos/17573364@N00), "[House](https://www.flickr.com/photos/21098413@N04/5405425139)" by [noona11](https://www.flickr.com/photos/21098413@N04) all licensed under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/?ref=ccsearch&atype=rich).


## Pre-trained models {.smaller}
\

- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.
- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.
- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.
- It refers to using a model already trained on one task as a starting point for learning to perform another task.

## Pre-trained models out-of-the-box 
\

![](img/cnn-ex.png)

<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->

- Let's first apply one of these pre-trained models to our own problem right out of the box. 


## Pre-trained models out-of-the-box {.smaller}
\

- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)
- We used a pre-trained model vgg16 which is trained on the ImageNet data. 
- We preprocess the given image. 
- We get prediction from this pre-trained model on a given image along with prediction probabilities.  
- For a given image, this model will spit out one of the 1000 classes from ImageNet. 

## Pre-trained models out-of-the-box {.scrollable}

- Let's predict labels with associated probabilities for unseen images

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
import glob
import matplotlib.pyplot as plt
images = glob.glob("data/test_images/*.*")
plt.figure(figsize=(5, 5));
for image in images:
    img = Image.open(image)
    img.load()
    
    plt.imshow(img)
    plt.show()
    df = classify_image(img)
    print(df.to_string(index=False))
    print("--------------------------------------------------------------")
```
:::


## Pre-trained models out-of-the-box {.smaller}
\

- We got these predictions without "doing the ML ourselves".
- We are using **pre-trained** `vgg16` model which is available in `torchvision`.
  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.
- Many of these models have been pre-trained on famous datasets like **ImageNet**. 
- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. 

## Pre-trained models out-of-the-box {.smaller}
\

- Let's try some images which are unlikely to be there in ImageNet. 
- It's not doing very well here because ImageNet doesn't have proper classes for these images.

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
# Predict labels with associated probabilities for unseen images
images = glob.glob("data/random_img/*.*")
for image in images:
    img = Image.open(image)
    img.load()
    plt.imshow(img)
    plt.show()
    df = classify_image(img)    
    print(df.to_string(index=False))
    print("--------------------------------------------------------------")
```
:::

## Pre-trained models out-of-the-box
\

- Here we used pre-trained models out-of-the-box. 
- Can we use pre-trained models for our own classification problem with our classes? 
- Yes!! We have two options here:
    1. Add some extra layers to the pre-trained network to suit our particular task
    2. Pass training data through the network and save the output to use as features for training some other model


## Pre-trained models to extract features {.smaller}
\

- Let's use pre-trained models to extract features.
- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. 
- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. 
- You can think of each layer a transformer applying some transformations on the input received to that later. 

![](img/cnn-ex.png){.nostretch fig-align="center" width="600px"}


## Pre-trained models to extract features 
\

- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. 
- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  
- Let's try this out. 
- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. 

## Pre-trained models to extract features 
\

Let's look at some sample images in the dataset. 

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
    data_dir = 'data/food/'
    image_datasets, dataloaders = read_data(data_dir)
    dataset_sizes = {x: len(image_datasets[x]) for x in ["train", "valid"]}
    class_names = image_datasets["train"].classes
    inputs, classes = next(iter(dataloaders["valid"]))
    plt.figure(figsize=(10, 8)); plt.axis("off"); plt.title("Sample valid Images")
    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));
```
:::

## Dataset statistics
\

Here is the stat of our toy dataset. 

```{python}
    print(f"Classes: {image_datasets['train'].classes}")
    print(f"Class count: {image_datasets['train'].targets.count(0)}, {image_datasets['train'].targets.count(1)}, {image_datasets['train'].targets.count(2)}")
    print(f"Samples:", len(image_datasets["train"]))
    print(f"First sample: {image_datasets['train'].samples[0]}")
```


## Pre-trained models to extract features 
\

- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  

```{python}
densenet = models.densenet121(weights="DenseNet121_Weights.IMAGENET1K_V1")
densenet.classifier = nn.Identity()  # remove that last "classification" layer
Z_train, y_train, Z_valid, y_valid = get_features(
    densenet, dataloaders["train"], dataloaders["valid"]
)
```

## Shape of the feature vector {.smaller}
\

- Now we have extracted feature vectors for all examples. What's the shape of these features?

```{python}
Z_train.shape
```

- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  

![](img/densenet-architecture.png){.nostretch fig-align="center" width="700px"}

[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)

## A feature vector given by densenet 
\ 

- Let's examine the feature vectors. 

```{python}
pd.DataFrame(Z_train).head()
```
- The features are hard to interpret but they have some important information about the images which can be useful for classification.  

## Logistic regression with the extracted features 
\

- Let's try out logistic regression on these extracted features. 

```{python}
pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))
pipe.fit(Z_train, y_train)
print("Training score: ", pipe.score(Z_train, y_train))
```

```{python}
pipe.score(Z_valid, y_valid)
print("Validation score: ", pipe.score(Z_valid, y_valid))
```

- This is great accuracy for so little data and little effort!!!


## Sample predictions
\

Let's examine some sample predictions on the validation set.  

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
# Show predictions for 25 images in the validation set (5 rows of 5 images)
show_predictions(pipe, Z_valid, y_valid, dataloaders['valid'], class_names, num_images=40)
```
:::


## Object detection {.smaller}
\

- Another useful task and tool to know is object detection using YOLO model. 
- Let's identify objects in a sample image using a pretrained model called YOLO8. 
- List the objects present in this image.

![](data/yolo_test/3356700488_183566145b.jpg){.nostretch fig-align="center" width="400px"}

## Object detection using [YOLO](https://docs.ultralytics.com/)
\

Let's try this out using a pre-trained model. 


```{python}
#| echo: true
from ultralytics import YOLO
model = YOLO("yolov8n.pt")  # pretrained YOLOv8n model

yolo_input = "data/yolo_test/3356700488_183566145b.jpg"
yolo_result = "data/yolo_result.jpg"
# Run batched inference on a list of images
result = model(yolo_input)  # return a list of Results objects
result[0].save(filename=yolo_result)
```

## Object detection output 
\

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
```{python}
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Load the images
input_img = mpimg.imread(yolo_input)
result_img = mpimg.imread(yolo_result)

# Create a figure to display the images side by side
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Display the first image
axes[0].imshow(input_img)
axes[0].axis('off')  # Hide the axes
axes[0].set_title('Original Image')

# Display the second image
axes[1].imshow(result_img)
axes[1].axis('off')  # Hide the axes
axes[1].set_title('Result Image')

# Show the images
plt.tight_layout()
plt.show()
```
:::

## Summary {.smaller}
\

- Neural networks are a flexible class of models.
  - They are particular powerful for structured input like images, videos, audio, etc.
  - They can be challenging to train and often require significant computational resources.
- The good news is we can use pre-trained neural networks.
  - This saves us a huge amount of time/cost/effort/resources.
  - We can use these pre-trained networks directly or use them as feature transformers. 
