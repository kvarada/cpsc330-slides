---
title: "CPSC 330 Lecture 9: Classification Metrics" 
author: "Varada Kolhatkar"
description: "Metrics for classification"
description-short: "confusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance" 
format:
  revealjs:
    html-math-method: mathjax
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Focus on the breath!

![](img/inukshuk.jpeg){.nostretch fig-align="center" width="600px"}


## Announcements 

- Important information about midterm 1
  - https://piazza.com/class/mekbcze4gyber/post/162
  - **Good news for you: You'll have access to our course notes in the midterm!**
- HW4 was due on Monday, Oct 6th 11:59 pm. 
- HW5 has been released. It's a project-type assignment and you get till Oct 27th to work on it.  


```{python}
import os
import sys
import pandas as pd 
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import cross_validate
import mglearn
DATA_DIR = 'data/' 
```

## ML workflow 
![](img/ml-workflow.png)

## Accuracy

- So far, we‚Äôve been measuring model performance using **Accuracy**.  
- **Accuracy** is the proportion of all predictions that were correct ‚Äî whether *positive* or *negative*.  

$$
\text{Accuracy} = \frac{\text{correct classifications}}{\text{total classifications}}
$$

- But is **accuracy** always the right metric to evaluate a model? ü§î  

## A fraud classification example
```{python}
# This dataset will be loaded using a URL instead of a CSV file
DATA_URL = "https://github.com/firasm/bits/raw/refs/heads/master/creditcard.csv"

cc_df = pd.read_csv(DATA_URL, encoding="latin-1")
# Sorting columns so it is easier to read
cc_df = cc_df[['Class', 'Time', 'Amount'] + cc_df.columns[cc_df.columns.str.startswith('V')].to_list()]

train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)
X_train = train_df.drop(columns=["Class"])
y_train = train_df["Class"]
print(X_train.shape)
train_df.head()
```

## `DummyClassifier`
Let's try a DummyClassifier, which makes predictions without learning any patterns.

```{python}
#| echo: true
dummy = DummyClassifier()
pd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True)).mean()

```

- The accuracy looks surprisingly high!
- Should we be happy with this model and deploy it?

## Problem: Class imbalance 

- In many real-world problems, some classes are much rarer than others.

- A model that always predicts "no fraud" could still achieve >99% accuracy!
- This is why accuracy can be misleading in imbalanced datasets.
- We need metrics that differentiate types of errors.

## Fraud Confusion matrix

Which types of errors would be most critical for the bank to address?

- Missing a fraud case?

- Or flagging a legitimate transaction as fraud?

![](img/fraud-confusion-matrix.png)


## Understanding the confusion matrix
:::: {.columns}
:::{.column width="80%"}
![](img/tp-fp-tn-fn-fraud.png)
:::

:::{.column width="20%"}
- TN $\rightarrow$ True negatives 
- FP $\rightarrow$ False positives 
- FN $\rightarrow$ False negatives
- TP $\rightarrow$ True positives 
:::
::::

# Practice: confusion matrix terminology

## Confusion matrix questions 

Imagine a spam filter model where emails labeled **1 = spam, 0 = not spam**. 

If a spam email is incorrectly classified as not spam, what kind of error is this?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In an intrusion detection system, **1 = intrusion, 0 = safe**. 

If the system misses an actual intrusion and classifies it as safe, this is a:

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In a medical test for a disease, **1 = diseased, 0 = healthy**. 

If a healthy patient is incorrectly diagnosed as diseased, that's a:

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Metrics other than accuracy 

Now that we understand the different types of errors, we can explore metrics that better capture model performance when **accuracy falls short**, especially for **imbalanced datasets**.

We'll start with three key ones:

- **Precision**
- **Recall**
- **F1-score**

- Precision
- Recall
- F1-score 

## Precision and recall 

Let's revisit our fraud detection scenario. The circle below represents **all transactions predicted as fraud** by an **imaginary toy model** designed to detect fraudulent activity.

![](img/precision-recall.png){.nostretch fig-align="center" width="600px"}
![](img/fraud-precision-recall.png){.nostretch fig-align="center" width="600px"}

## Intuition behind the two metrics

- **Precision** answers:  
  > *Of all the transactions predicted as fraud, how many were actually fraud?*  
  High precision $\rightarrow$ few false alarms (low false positives).

- **Recall** answers:  
  > *Of all the actual fraud cases, how many did the model catch?*  
  High recall $\rightarrow$ few missed frauds (low false negatives).

## Trade-off between precision and recall

- Increasing **recall** often decreases **precision**, and vice versa.  
- Example:  
  - Predict *‚Äúfraud‚Äù* for every transaction $rightarrow$ perfect recall, terrible precision.  
  - Predict *‚Äúfraud‚Äù* only when 100% sure $rightarrow$ high precision, low recall.

**The right balance depends on the application and cost of errors.**

## F1-score

- Sometimes, we want a **single metric** that balances precision and recall.  
- The **F1-score** is the **harmonic mean** of the two:

$$
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

- High **F1** means both precision and recall are strong.  
- Useful when we care about both false positives **and** false negatives.

## Summary

| Metric | What it measures | High value means |
|:--------|:----------------|:------------------|
| **Accuracy** | Overall correctness | Model gets most predictions right |
| **Precision** | Quality of positive predictions | Few false alarms |
| **Recall** | Quantity of true positives caught | Few missed positives |
| **F1-score** | Balance of precision & recall | Both precision and recall are high |

## iClicker Exercise 9.1

**Select all of the following statements which are TRUE.**

- (A) In medical diagnosis, false positives are more damaging than false negatives (assume "positive" means the person has a disease, "negative" means they don't).
- (B) In spam classification, false positives are more damaging than false negatives (assume "positive" means the email is spam, "negative" means they it's not).
- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.
- (D) If method A gets a higher accuracy than method B, that means its recall is also higher.

## Counter examples

Method A - higher accuracy but lower precision

| Negative | Positive
| -------- |:-------------:|
| 90      | 5|
| 5      | 0|

Method B - lower accuracy but higher precision

| Negative | Positive
| -------- |:-------------:|
| 80      | 15|
| 0      | 5|

## Thresholding 

- The above metrics assume a fixed threshold. 
- We use thresholding to get the binary prediction. 
- A typical threshold is 0.5.
    - A prediction of 0.90 $\rightarrow$ a high likelihood that the transaction is fraudulent and we predict **fraud**
    - A prediction of 0.20 $\rightarrow$ a low likelihood that the transaction is non-fraudulent and we predict **Non fraud**
- **What happens if the predicted score is equal to the chosen threshold?**

- [Play with classification thresholds](https://developers.google.com/machine-learning/crash-course/classification/thresholding)


## iClicker Exercise 9.2

**Select all of the following statements which are TRUE.**

- (A) If we increase the classification threshold, both true and false positives are likely to decrease.
- (B) If we increase the classification threshold, both true and false negatives are likely to decrease.
- (C) Lowering the classification threshold generally increases the model‚Äôs recall.  
- (D) Raising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives.


## PR curve
- Calculate precision and recall (TPR) at every possible threshold and graph them. 
- Better choice for highly imbalanced datasets 

![](img/pr-curve-example.png)


## ROC curve 
- Calculate the true positive rate (TPR) and false positive rate (FPR) at every possible thresholding and graph TPR over FPR. 
- Good choice when the datasets are roughly balanced. 
![](img/roc-curve-example.png)

## AUC 
- The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.


## ROC AUC questions

Consider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?

:::: {.columns}

:::{.column width="50%"}
![](img/auc_abc)
:::

:::{.column width="50%"}

- (A) If false positives (false alarms) are highly costly
- (B) If false positives are cheap and false negatives (missed true positives) highly costly
- (C) If the costs are roughly equivalent
:::
::::

[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)
