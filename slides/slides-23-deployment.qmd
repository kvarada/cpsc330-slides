---
title: "CPSC 330 Lecture 23: Deployment and conclusion"
author: "Varada Kolhatkar"
format: 
    revealjs:
      html-math-method: mathjax    
      embed-resources: true
      slide-number: true
      logo: img/UBC-CS-logo.png
      resources:
        - data/
        - img/        
---

```{python}
import os
import sys

sys.path.append(os.path.join(os.path.abspath("."), "code"))
from plotting_functions import *

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

sys.path.append(os.path.join(os.path.abspath("."), "code"))
from utils import *

plt.rcParams["font.size"] = 12

import warnings

warnings.filterwarnings("default")
DATA_DIR = os.path.join(os.path.abspath("."), "data/")
```

## Announcements

- Last lecture today ü•∫! 
- HW9 is due Friday, Dec 5th at 11:59 PM (No late submission allowed.)
- My OH next week has been moved to 11 AM. Do you prefer in-person or Zoom OH? 
- For an in-person OH I'll book a larger room. 

## iClicker 

What percentage of machine learning projects do you think never make it into production?

- (A) Between 20-30%
- (B) Around 50%
- (C) Between 60-70%
- (D) Between 80-90%

[Source](https://venturebeat.com/ai/why-do-87-of-data-science-projects-never-make-it-into-production)

## Model deployment 

**Model deployment** is the process of making machibe learing model available for use in production enviroments. 

- Carefully development machine learning models is only half the story. 
- The real challenge in practice is getting ML to work reliably in the real world!! 

## Model deployment {.smaller}

Model deployment is a complex, multi-step process that requires making several important decisions, such as:

- Choosing the right infrastructure: What hardware or cloud resources are needed (CPU, GPU, memory, storage)?
- Meeting performance requirements: What latency and throughput do users expect?
- Planning for scale: How many users or requests per second must the system support, now and in the future?
- Monitoring in production: How will you track model performance, data drift, and system health over time?
- Managing model updates: When deploying a new version, how do you verify that it actually performs better than the current model (e.g., A/B tests)?

## Goal of this lecture

To demonstrate, through a simple example, how you can take the models you've built in this course (or in your side projects) and deploy them in a lightweight, real-world setting.

## ‚ùì‚ùì Questions for you {.smaller}

Imagine you've created a machine learning model and are eager to share it with others. Consider the following scenarios for sharing your model:

- **To a non-technical Audience:** How would you present your model to friends and family who may not have a technical background?
- **To a technical audience:** How would you share your model with peers or professionals in the field who have a technical understanding of machine learning? 
- **In an academic or research setting:** How would you disseminate your model within academic or research communities?

## Try out this moment predictor

[https://cpsc330-moment-predictor.onrender.com/](https://cpsc330-moment-predictor.onrender.com/)

- In this lecture, I will show you how to set up/develop this. 

## What we need? 

- After we train a model, we want to use it!
- The user likely does not want to install your Python stack, train your model.
- You don't necessarily want to share the dataset.
- So we need to do two things:
  1. Save/store your model for later use.
  2. Make the saved model conveniently accessible.

## We will use the tools below 

Saving the model

- [Joblib](https://joblib.readthedocs.io/): A Python library for efficiently saving and loading trained models

Making the model accessible

- [Flask](https://flask.palletsprojects.com/): A lightweight web framework that lets you create a local API (request $\rightarrow$ prediction $\rightarrow$ response)
- [render](https://render.com/): A cloud service that hosts your Flask app so anyone can access your model online
(public URL $\rightarrow$ no need to manage servers)

# [Class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_24-Deployment.ipynb) 

## Course evaluations (~15 mins)

https://canvas.ubc.ca/courses/170662/external_tools/53187

- They help us improve our teaching!
- UBC & CS uses them to provide rewards to instructors and TAs who are doing well!
- UBC & CS uses them to identify where instructors, TAs and courses need additional supports to improve.
- UBC uses these in evaluating professors for tenure and promotion.
- I'll very much appreciate your constructive and concrete feedback.  

## What did we cover 

- Part 1: Supervised learning on tabular data: ML fundamentals, preprocessing and data encoding, a bunch of models, evaluation metrics, feature importances and model transparency, feature selection, hyperparameter optimization
 
- Part 2: Dealing with other non-tabular data types: Clustering, recommender systems, computer vision with pre-trained deep learning models (high level), language data, text preprocessing, embeddings, topic modeling, time series, right-censored data / survival analysis

- Part 3: Communication, ethics, and deployment

## What we didn't cover 
- How do these models work under the hood 

![](img/modular-ML.png)

## What next?

If you want to further develop your machine learning skills:

- Practice!
- Work on your own projects. Make your work available and reproducible. 

- If you are interested in research in machine learning
  - Take CPSC 340. If you do not have the required prereqs you can try to audit it.
  - Get into the habit of reading papers and replicating results


## ‚ùì‚ùì Questions for you

For each of the scenarios below 

- Identify if ML is a good solution for a problem. If yes
    - Frame the problem to a ML problem.
    - Discuss what kind of features you would need to effectively solve the problem
    - What would be a reasonable baseline? 
    - Which model would be a suitable model for the given scenario? 
    - What would be the appropriate success metrics.

## QueuePredictor app with call-level data {.smaller .scrollable}

:::: {.columns}
:::{.column width="50%"}
```{python}
df = pd.read_csv('data/call-wait-time-v1.csv')
df
```
:::
:::{.column width="50%"}
**Scenario:** A call center wants to inform callers of their expected wait time when they join the queue.

**Available data:** Historical calls with both completed (answered) and abandoned (hung up) calls.

:::
::::

## QueuePredictor app with interval data {.smaller .scrollable}

:::: {.columns}
:::{.column width="50%" .smaller .scrollable}

```{python}
df = pd.read_csv('data/call-wait-time-v2.csv')
df
```

:::
:::{.column width="50%"}
**Scenario:** Same problem, but data is aggregated by 5-minute intervals instead of individual calls.

**Available data:** Each row represents a time window with summary statistics.

:::
::::

## ‚ùì‚ùì More scenarios for practice {.smaller}


|     App           | Goal | 
|-------------------|--------------------------|
| To-doList App    | Keep track of the tasks that a user inputs and organize them by date | 
| SegmentSphere App | To segment customers to tailor marketing strategies based on purchasing behavior |
| Video app	        | Recommend useful videos|
| Dining app        | Identify cuisine by a restaurant's menu|
| Weather app       | Calculate precipitation in six hour increments for a geographic region  |
| EvoCarShare app   | Calculate number of car rentals in four increaments at a particular Evo parking spot |
| Pharma app| Understand the effect of a new drug on patient survival time |

## Conclusion & farewell {.smaller}

That's all! We made it! I hope you learned something useful from the course. You all are wonderful students and I had fun teaching this course ‚ô•Ô∏è!

![](img/eva-thank-you.png)

If you didn‚Äôt fill out course evaluations during class , it‚Äôll be great if you can fill them in when you get a chance.

## Time permitting 

- Class picture üòä

![](img/eva-photo.png)