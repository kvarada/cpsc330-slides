---
title: 'CPSC 330 Lecture 14: K-Means'
description: "Unsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset,  importance of input data representation in clustering."
format:
    revealjs:
        html-math-method: mathjax
        slide-number: true
        slide-level: 2
        theme:
          - slides.scss
        center: true
        logo: img/UBC-CS-logo.png
        resources:
          - data/
          - img/

editor:
  render-on-save: true
---

## Focus on the breath!

![](img/inukshuk.jpeg){.nostretch fig-align="center" width="500px"}

## Announcements 

- Midterm 1 grades were released last week
- HW5 was due yesterday 
- HW6 has been released. It's due next week Monday. 
  - Computationally intensive 
  - You need to install many packages 

## (iClicker) Midterm poll {.smaller}

**Select all of the following statements which are TRUE.**

- (A) I'm happy with my progress and learning in this course.
- (B) I find the course content interesting, but the pace is a bit overwhelming. Balancing this course with other responsibilities is challenging
- (C) I'm doing okay, but I feel stressed and worried about upcoming assessments.
- (D) I'm confused about some concepts and would appreciate more clarification or review sessions.
- (E) I'm struggling to keep up with the material. I am not happy with my learning in this course and my morale is low :(. 


## Supervised learning {.smaller}

- Training data comprises a set of observations ($X$) and their corresponding targets ($y$). 
- We wish to find a model function $f$ that relates $X$ to $y$.
- Then use that model function to predict the targets of new examples.
- We have been working with this set up so far. 

![](img/sup-learning.png)

## Unsupervised learning {.smaller}

- Training data consists of observations ($X$) without any corresponding targets.
- Unsupervised learning could be used to group similar things together in $X$ or to find underlying structure in the data. 

![](img/unsup-learning.png)


## Clustering activity {.smaller}

![](img/food-clustering-activity.png)

- Categorize the food items in the image and write your categories. Do you think there is one correct way to cluster these images? Why or why not?
- If you want to build a machine learning model to cluster such images how would you represent such images?
- Write your answers here: https://docs.google.com/document/d/12GXA9Efi_19WiRnCi8FiOmdpxM-I_wq2nz97LjvXILg/edit?usp=sharing

## The "perfect" spaghetti sauce

Suppose you are a hypothetical spaghetti sauce company and you're asked to create the "perfect" spaghetti sauce which makes all your customers happy. The truth is humans are diverse and there is no "perfect" spaghetti sauce. There are "perfect" spaghetti sauces that cater to different tastes! 

## The "perfect" spaghetti sauce {.smaller}

Howard Moskowitz found out that Americans fall into one of the following three categories: 

- people who like their spaghetti sauce **plain**
- people who like their spaghetti sauce **spicy**
- people who like their spaghetti sauce **extra chunky**

![](img/prego-pasta-sauces.png){.nostretch fig-align="center" width="700px"}

Reference: [Malcolm Gladwell’s Ted talk](https://www.ted.com/talks/malcolm_gladwell_choice_happiness_and_spaghetti_sauce?language=en)

## The "perfect" spaghetti sauce

- If one “perfect” authentic sauce satisfies 60%, of the people on average, creating several tailored sauce clusters could increase average happiness to between 75% to 78%.
- Can we apply this concept of clustering and tailoring solutions to specific groups in machine learning?


# K-Means 

## K-Means Clustering
- **Algorithm Steps**:
  1. Select K initial centroids.
  2. Assign each data point to the nearest centroid.
  3. Recalculate centroids based on assigned points.
  4. Repeat until centroids stabilize or reach a maximum number of iterations.

## K-Means example 
![](img/k-means.png)

## K-Means pros and cons
- **Advantages**:
  - Simple and efficient for large datasets.
  - Works well with spherical clusters.

- **Limitations**:
  - Needs pre-defined K.
  - Sensitive to outliers and initial centroid placement.

## iClicker Exercise 14.1 

Select all of the following statements which are **True**

- (A) K-Means algorithm always converges to the same solution.
- (B) $K$ in K-Means should always be $\leq$ # of features.
- (C) In K-Means, it makes sense to have $K$ $\leq$ # of examples. 
- (D) In K-Means, in some iterations some points may be left unassigned. 

## iClicker Exercise 14.2 

Select all of the following statements which are **True**

- (A) K-Means is sensitive to initialization and the solution may change depending upon the initialization. 
- (B) K-means terminates when the number of clusters does not increase between iterations.
- (C) K-means terminates when the centroid locations do not change between iterations.
- (D) K-Means is guaranteed to find the optimal solution. 


# Choosing k

## The Elbow method

- **Purpose**: Identify the optimal number of clusters (K).
- **How it Works**:
  - Plot intra-cluster distances for different values of K.
  - Look for the "elbow" point where the intra-cluster reduction slows.

- **Interpretation**:
  - The point of diminishing returns suggests a good K.

## The Elbow method example
![](img/elbow-plot)

## The Silhouette method {.smaller}
- **Silhouette Score**: Measures how well data points fit within their cluster.
  - $s(i) = \frac{b(i) - a(i)}{\max (a(i), b(i))}$
    - $a(i)$: Mean distance to other points in the same cluster.
    - $b(i)$: Mean distance to points in the nearest neighboring cluster.

![](img/distances-for-silhoutte.png)

## The Silhouette method
- **Range**: -1 to 1
  - **1**: Perfect clustering.
  - **0**: Overlapping clusters.
  - **Negative**: Poor clustering.
- Higher average silhouette score indicates "better" clustering.


## Silhouette plot example

![](img/silhoutte-example.png)

## iClicker Exercise 14.3 {.smaller}

Select all of the following statements which are **True** 

- (A) If you train K-Means with `n_clusters`= the number of examples, the inertia value will be 0. 
- (B) The elbow plot shows the tradeoff between within cluster distance and the number of clusters.
- (C) Unlike the Elbow method, the Silhouette method is not dependent on the notion of cluster centers.
- (D) The elbow plot is not a reliable method to obtain the optimal number of clusters in all cases. 
- (E) The Silhouette scores ranges between -1 and 1 where higher scores indicates better cluster assignments.  

# [Class demo](https://github.com/UBC-CS/cpsc330-2025W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_14-k-means.ipynb)