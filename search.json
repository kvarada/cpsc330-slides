[
  {
    "objectID": "slides/slides-01-intro.html#learning-outcomes",
    "href": "slides/slides-01-intro.html#learning-outcomes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "üéØ Learning Outcomes",
    "text": "üéØ Learning Outcomes\nBy the end of this lesson, you will be able to:\n\nExplain the difference between AI, ML, and DL\nDescribe what machine learning is and when it is appropriate to use ML-based solutions.\nBriefly describe supervised learning.\nDifferentiate between traditional programming and machine learning.\nEvaluate whether a machine learning solution is suitable for your problem or whether a rule-based or human-expert solution is more appropriate.\nNavigate the course materials and get familiar with the course syllabus and policies."
  },
  {
    "objectID": "slides/slides-01-intro.html#cpsc-330-website",
    "href": "slides/slides-01-intro.html#cpsc-330-website",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 website",
    "text": "CPSC 330 website\n\n\n\nCourse Jupyter book: https://ubc-cs.github.io/cpsc330-2025W1\nCourse GitHub repository: https://github.com/UBC-CS/cpsc330-2025W1"
  },
  {
    "objectID": "slides/slides-01-intro.html#meet-your-instructor",
    "href": "slides/slides-01-intro.html#meet-your-instructor",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet your instructor",
    "text": "Meet your instructor\n\n\n\n\n\nVarada Kolhatkar [ ã…ô…æ…ôda k…îÀêl…¶…ô àk…ôr]\nYou can call me Varada, V, or Ada.\nAssociate Professor of Teaching in the Department of Computer Science.\nPh.D.¬†in Computational Linguistics at the University of Toronto.\nI primarily teach machine learning courses in the Master of Data Science (MDS) program.\nContact information\n\nEmail: kvarada@cs.ubc.ca\nOffice: ICCS 237"
  },
  {
    "objectID": "slides/slides-01-intro.html#meet-eva-a-fictitious-persona",
    "href": "slides/slides-01-intro.html#meet-eva-a-fictitious-persona",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet Eva (a fictitious persona)!",
    "text": "Meet Eva (a fictitious persona)!\n\n\n\n\nEva is among one of you. She has some experience in Python programming. She knows machine learning as a buzz word. During her recent internship, she has developed some interest and curiosity in the field. She wants to learn what is it and how to use it. She is a curious person and usually has a lot of questions!"
  },
  {
    "objectID": "slides/slides-01-intro.html#you-all",
    "href": "slides/slides-01-intro.html#you-all",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "You all",
    "text": "You all\n\nIntroduce yourself to your neighbour.\nSince we‚Äôre going to spend the semester with each other, I would like to know you a bit better.\nPlease fill out Getting to know you survey when you get a chance."
  },
  {
    "objectID": "slides/slides-01-intro.html#asking-questions-during-class",
    "href": "slides/slides-01-intro.html#asking-questions-during-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Asking questions during class",
    "text": "Asking questions during class\n\nYou are encouraged to ask questions by raising your hand.\nNo question is a stupid question.\nRecommended reading as you begin your learning journey: The Fear of Publicly Not Knowing\n\n\nWhat I quickly came to realize was that publicly not knowing wasn‚Äôt a indicator of stupidity, it was an indicator of understanding. And from what I‚Äôve seen, it is one of the clearest indicators of success in people ‚Äî more than school prestige, more than GPA."
  },
  {
    "objectID": "slides/slides-01-intro.html#activity-1",
    "href": "slides/slides-01-intro.html#activity-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 1",
    "text": "Activity 1\n\nDiscuss with you neighbour\n\nWhat do you know about machine learning?\nWhat would you like to get out this course?\nAre there any particular topics or aspects of this course that you are especially excited or anxious about? Why?"
  },
  {
    "objectID": "slides/slides-01-intro.html#which-cat-do-you-think-is-ai-generated",
    "href": "slides/slides-01-intro.html#which-cat-do-you-think-is-ai-generated",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Which cat do you think is AI-generated?",
    "text": "Which cat do you think is AI-generated?\n\n\n\nSource\n\n\nA\nB\nBoth\nNone\n\n\n\nWhat clues did you use to decide?"
  },
  {
    "objectID": "slides/slides-01-intro.html#what-are-ai-ml-dl",
    "href": "slides/slides-01-intro.html#what-are-ai-ml-dl",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What are AI, ML, DL?",
    "text": "What are AI, ML, DL?\n\nArtificial Intelligence (AI): Making computers act smart\n\nExamples: Deep Blue, early spell checkers\n\nMachine Learning (ML): Learning patterns from data\n\nExample: Spam filtering in Gmail\n\nDeep Learning (DL): Using neural networks to learn complex patterns\n\nExamples: Face recognition in your phone, voice assistants"
  },
  {
    "objectID": "slides/slides-01-intro.html#lets-walk-through-an-example",
    "href": "slides/slides-01-intro.html#lets-walk-through-an-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs walk through an example",
    "text": "Let‚Äôs walk through an example\n\nHave you used search in Google Photos? You can search for ‚Äúcat‚Äù and it will retrieve photos from your libraries containing cats.\nThis can be done using image classification."
  },
  {
    "objectID": "slides/slides-01-intro.html#image-classification",
    "href": "slides/slides-01-intro.html#image-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Image classification",
    "text": "Image classification\n\nImagine we want a system that can tell cats and foxes apart.\nHow might we do this with traditional programming? With ML?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage ID\nWhiskers Present\nEar Size\nFace Shape\nFur Color\nEye Shape\nLabel\n\n\n\n\n1\nYes\nLarge\nRound\nMixed\nRound\nCat\n\n\n2\nYes\nMedium\nRound\nBrown\nAlmond\nCat\n\n\n3\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n4\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n5\nYes\nSmall\nRound\nMixed\nRound\nCat\n\n\n6\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n7\nYes\nSmall\nRound\nGrey\nRound\nCat\n\n\n8\nYes\nSmall\nRound\nBlack\nRound\nCat\n\n\n9\nYes\nLarge\nPointed\nRed\nNarrow\nFox"
  },
  {
    "objectID": "slides/slides-01-intro.html#traditional-programming-example",
    "href": "slides/slides-01-intro.html#traditional-programming-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Traditional programming: example",
    "text": "Traditional programming: example\n\n\n\n\n\nYou hard-code rules. If all of the following satisfy, it‚Äôs a fox.\n\npointed face ‚úÖ\nred fur ‚úÖ\nnarrow eyes ‚úÖ\n\n\nThis works for normal cases, but what if there are exceptions"
  },
  {
    "objectID": "slides/slides-01-intro.html#ml-approach-example",
    "href": "slides/slides-01-intro.html#ml-approach-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ML approach: example",
    "text": "ML approach: example\n\n\n\n\n\nWe don‚Äôt tell the model the exact rule. Instead, we give it many labeled images, and it learns probabilistic patterns across multiple features, not rigid rules.\n\nIf fur is red ‚Üí 90% chance of Fox."
  },
  {
    "objectID": "slides/slides-01-intro.html#dl-approach-example",
    "href": "slides/slides-01-intro.html#dl-approach-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "DL approach: example",
    "text": "DL approach: example\n\n\n\n\n\nA neural network automatically learns which features to look at (edges ‚Üí textures ‚Üí objects).\nNo need to even specify face shape or fur colour. It learns relevant features on its own."
  },
  {
    "objectID": "slides/slides-01-intro.html#what-is-ml",
    "href": "slides/slides-01-intro.html#what-is-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is ML?",
    "text": "What is ML?\n\nML uses algorithms to learn patterns from data and build models.\n\nThese models can:\n\nMake predictions on new data\n\nSupport complex decisions\n\nGenerate new content\n\n\nML systems can improve when trained on more data.\n\nThere is no one-size-fits-all model. The right choice depends on the problem."
  },
  {
    "objectID": "slides/slides-01-intro.html#when-to-use-ml",
    "href": "slides/slides-01-intro.html#when-to-use-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "When to use ML?",
    "text": "When to use ML?\n\nWhen the problem can‚Äôt be solved with a fixed set of rules\nWhen you have lots of data and complex relationships\nWhen human decision-making is too slow or inconsistent\n\n\n\n\n\n\n\n\nApproach\nBest for\n\n\n\n\nTraditional Programming\nRules are known, data is clean/predictable\n\n\nMachine Learning\nRules are complex/unknown, data is noisy"
  },
  {
    "objectID": "slides/slides-01-intro.html#example-supervised-classification",
    "href": "slides/slides-01-intro.html#example-supervised-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Supervised classification",
    "text": "Example: Supervised classification\n\nWe want to predict liver disease from tabular features:\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nTarget\n\n\n\n\n40\n14.5\n6.4\n358\n50\n75\n5.7\n2.1\n0.50\nDisease\n\n\n33\n0.7\n0.2\n256\n21\n30\n8.5\n3.9\n0.80\nDisease\n\n\n24\n0.7\n0.2\n188\n11\n10\n5.5\n2.3\n0.71\nNo Disease\n\n\n60\n0.7\n0.2\n171\n31\n26\n7.0\n3.5\n1.00\nNo Disease\n\n\n18\n0.8\n0.2\n199\n34\n31\n6.5\n3.5\n1.16\nNo Disease"
  },
  {
    "objectID": "slides/slides-01-intro.html#model-training",
    "href": "slides/slides-01-intro.html#model-training",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Model training",
    "text": "Model training\n\nfrom lightgbm.sklearn import LGBMClassifier\nmodel = LGBMClassifier(random_state=123, verbose=-1)\nmodel.fit(X_train, y_train)\n\nLGBMClassifier(random_state=123, verbose=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nboosting_type¬†\n'gbdt'\n\n\n\nnum_leaves¬†\n31\n\n\n\nmax_depth¬†\n-1\n\n\n\nlearning_rate¬†\n0.1\n\n\n\nn_estimators¬†\n100\n\n\n\nsubsample_for_bin¬†\n200000\n\n\n\nobjective¬†\nNone\n\n\n\nclass_weight¬†\nNone\n\n\n\nmin_split_gain¬†\n0.0\n\n\n\nmin_child_weight¬†\n0.001\n\n\n\nmin_child_samples¬†\n20\n\n\n\nsubsample¬†\n1.0\n\n\n\nsubsample_freq¬†\n0\n\n\n\ncolsample_bytree¬†\n1.0\n\n\n\nreg_alpha¬†\n0.0\n\n\n\nreg_lambda¬†\n0.0\n\n\n\nrandom_state¬†\n123\n\n\n\nn_jobs¬†\nNone\n\n\n\nimportance_type¬†\n'split'\n\n\n\nverbose¬†\n-1"
  },
  {
    "objectID": "slides/slides-01-intro.html#new-examples",
    "href": "slides/slides-01-intro.html#new-examples",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "New examples",
    "text": "New examples\n\nGiven features of new patients below we‚Äôll use this model to predict whether these patients have the liver disease or not.\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\n19\n1.4\n0.8\n178\n13\n26\n8.0\n4.6\n1.30\n\n\n12\n1.0\n0.2\n719\n157\n108\n7.2\n3.7\n1.00\n\n\n60\n5.7\n2.8\n214\n412\n850\n7.3\n3.2\n0.78\n\n\n42\n0.5\n0.1\n162\n155\n108\n8.1\n4.0\n0.90"
  },
  {
    "objectID": "slides/slides-01-intro.html#model-predictions-on-new-examples",
    "href": "slides/slides-01-intro.html#model-predictions-on-new-examples",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Model predictions on new examples",
    "text": "Model predictions on new examples\n\nLet‚Äôs examine predictions\n\n\npred_df = pd.DataFrame({\"Predicted_target\": model.predict(X_test).tolist()})\ndf_concat = pd.concat([pred_df, X_test.reset_index(drop=True)], axis=1)\nHTML(df_concat.to_html(index=False))\n\n\n\n\nPredicted_target\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\nNo Disease\n19\n1.4\n0.8\n178\n13\n26\n8.0\n4.6\n1.30\n\n\nDisease\n12\n1.0\n0.2\n719\n157\n108\n7.2\n3.7\n1.00\n\n\nDisease\n60\n5.7\n2.8\n214\n412\n850\n7.3\n3.2\n0.78\n\n\nDisease\n42\n0.5\n0.1\n162\n155\n108\n8.1\n4.0\n0.90"
  },
  {
    "objectID": "slides/slides-01-intro.html#example-supervised-regression",
    "href": "slides/slides-01-intro.html#example-supervised-regression",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Supervised regression",
    "text": "Example: Supervised regression\nSuppose we want to predict housing prices given a number of attributes associated with houses. The target here is continuous and not discrete.\n\n\n\n\n\ntarget\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n509000.0\n2\n1.50\n1930\n3521\n2.0\n0\n0\n3\n8\n1930\n0\n1989\n0\n98007\n47.6092\n-122.146\n1840\n3576\n\n\n675000.0\n5\n2.75\n2570\n12906\n2.0\n0\n0\n3\n8\n2570\n0\n1987\n0\n98075\n47.5814\n-122.050\n2580\n12927\n\n\n420000.0\n3\n1.00\n1150\n5120\n1.0\n0\n0\n4\n6\n800\n350\n1946\n0\n98116\n47.5588\n-122.392\n1220\n5120\n\n\n680000.0\n8\n2.75\n2530\n4800\n2.0\n0\n0\n4\n7\n1390\n1140\n1901\n0\n98112\n47.6241\n-122.305\n1540\n4800\n\n\n357823.0\n3\n1.50\n1240\n9196\n1.0\n0\n0\n3\n8\n1240\n0\n1968\n0\n98072\n47.7562\n-122.094\n1690\n10800"
  },
  {
    "objectID": "slides/slides-01-intro.html#building-a-regression-model",
    "href": "slides/slides-01-intro.html#building-a-regression-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Building a regression model",
    "text": "Building a regression model\n\nfrom lightgbm.sklearn import LGBMRegressor\n\nX_train, y_train = train_df.drop(columns= [\"target\"]), train_df[\"target\"]\nX_test, y_test = test_df.drop(columns= [\"target\"]), train_df[\"target\"]\n\nmodel = LGBMRegressor()\nmodel.fit(X_train, y_train);"
  },
  {
    "objectID": "slides/slides-01-intro.html#predicting-prices-of-unseen-houses",
    "href": "slides/slides-01-intro.html#predicting-prices-of-unseen-houses",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting prices of unseen houses",
    "text": "Predicting prices of unseen houses\n\npred_df = pd.DataFrame(\n    {\"Predicted_target\": model.predict(X_test[0:4]).tolist()}\n)\ndf_concat = pd.concat([pred_df, X_test[0:4].reset_index(drop=True)], axis=1)\nHTML(df_concat.to_html(index=False))\n\n\n\n\nPredicted_target\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n345831.740542\n4\n2.25\n2130\n8078\n1.0\n0\n0\n4\n7\n1380\n750\n1977\n0\n98055\n47.4482\n-122.209\n2300\n8112\n\n\n601042.018745\n3\n2.50\n2210\n7620\n2.0\n0\n0\n3\n8\n2210\n0\n1994\n0\n98052\n47.6938\n-122.130\n1920\n7440\n\n\n311310.186024\n4\n1.50\n1800\n9576\n1.0\n0\n0\n4\n7\n1800\n0\n1977\n0\n98045\n47.4664\n-121.747\n1370\n9576\n\n\n597555.592401\n3\n2.50\n1580\n1321\n2.0\n0\n2\n3\n8\n1080\n500\n2014\n0\n98107\n47.6688\n-122.402\n1530\n1357\n\n\n\n\n\nWe are predicting continuous values here as apposed to discrete values in disease vs.¬†no disease example."
  },
  {
    "objectID": "slides/slides-01-intro.html#example-text-classification",
    "href": "slides/slides-01-intro.html#example-text-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Text classification",
    "text": "Example: Text classification\n\nSuppose you are given some data with labeled spam and non-spam messages and you want to predict whether a new message is spam or not spam.\n\n\nCodeOutput\n\n\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\n\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\nspam\nLookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\n\n\nham\nAight, I'll hit you up when I get some cash\n\n\nham\nDon no da:)whats you plan?\n\n\nham\nGoing to take your babe out ?\n\n\nham\nNo need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room."
  },
  {
    "objectID": "slides/slides-01-intro.html#lets-train-a-model",
    "href": "slides/slides-01-intro.html#lets-train-a-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs train a model",
    "text": "Let‚Äôs train a model\n\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\nclf = make_pipeline(CountVectorizer(max_features=5000), LogisticRegression(max_iter=5000))\nclf.fit(X_train, y_train) # Training the model\n\nPipeline(steps=[('countvectorizer', CountVectorizer(max_features=5000)),\n                ('logisticregression', LogisticRegression(max_iter=5000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('countvectorizer', ...), ('logisticregression', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    CountVectorizer?Documentation for CountVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput¬†\n'content'\n\n\n\nencoding¬†\n'utf-8'\n\n\n\ndecode_error¬†\n'strict'\n\n\n\nstrip_accents¬†\nNone\n\n\n\nlowercase¬†\nTrue\n\n\n\npreprocessor¬†\nNone\n\n\n\ntokenizer¬†\nNone\n\n\n\nstop_words¬†\nNone\n\n\n\ntoken_pattern¬†\n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range¬†\n(1, ...)\n\n\n\nanalyzer¬†\n'word'\n\n\n\nmax_df¬†\n1.0\n\n\n\nmin_df¬†\n1\n\n\n\nmax_features¬†\n5000\n\n\n\nvocabulary¬†\nNone\n\n\n\nbinary¬†\nFalse\n\n\n\ndtype¬†\n&lt;class 'numpy.int64'&gt;\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n5000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "slides/slides-01-intro.html#unseen-messages",
    "href": "slides/slides-01-intro.html#unseen-messages",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Unseen messages",
    "text": "Unseen messages\n\nNow use the trained model to predict targets of unseen messages:\n\n\n\n\n\n\n\n\n\n\nsms\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one m...\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok."
  },
  {
    "objectID": "slides/slides-01-intro.html#predicting-on-unseen-data",
    "href": "slides/slides-01-intro.html#predicting-on-unseen-data",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting on unseen data",
    "text": "Predicting on unseen data\nThe model is accurately predicting labels for the unseen text messages above!\n\n\n\n\n\n\n\n¬†\nsms\nspam_predictions\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\nham\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\nham\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\nspam\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\nham"
  },
  {
    "objectID": "slides/slides-01-intro.html#examplel-text-classification-with-llms",
    "href": "slides/slides-01-intro.html#examplel-text-classification-with-llms",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Examplel: Text classification with LLMs",
    "text": "Examplel: Text classification with LLMs\n¬†\n\nLLMs = Large Language Models\n\n\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n# Sentiment analysis pipeline\nanalyzer = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\nanalyzer([\"I asked my model to predict my future, and it said '404: Life not found.'\",\n          '''Machine learning is just like cooking‚Äîsometimes you follow the recipe, \n            and other times you just hope for the best!.'''])\n\n[{'label': 'NEGATIVE', 'score': 0.995707631111145},\n {'label': 'POSITIVE', 'score': 0.9994770884513855}]"
  },
  {
    "objectID": "slides/slides-01-intro.html#zero-shot-learning",
    "href": "slides/slides-01-intro.html#zero-shot-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning",
    "text": "Zero-shot learning\n\n\n\nNow suppose you want to identify the emotion expressed in the text rather than just positive or negative.\n\n\n\n\n['im feeling rather rotten so im not very ambitious right now',\n 'im updating my blog because i feel shitty',\n 'i never make her separate from me because i don t ever want her to feel like i m ashamed with her',\n 'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n 'i was feeling a little vain when i did this one',\n 'i cant walk into a shop anywhere where i do not feel uncomfortable',\n 'i felt anger when at the end of a telephone call',\n 'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n 'i like to have the same breathless feeling as a reader eager to see what will happen next',\n 'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer']"
  },
  {
    "objectID": "slides/slides-01-intro.html#zero-shot-learning-for-emotion-detection",
    "href": "slides/slides-01-intro.html#zero-shot-learning-for-emotion-detection",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning for emotion detection",
    "text": "Zero-shot learning for emotion detection\n\n\n\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline \nimport torch\n\n#Load the pretrained model\nmodel_name = \"facebook/bart-large-mnli\"\nclassifier = pipeline('zero-shot-classification', model=model_name)\nexs = dataset[\"test\"][\"text\"][10:20]\ncandidate_labels = [\"sadness\", \"joy\", \"love\",\"anger\", \"fear\", \"surprise\"]\noutputs = classifier(exs, candidate_labels)"
  },
  {
    "objectID": "slides/slides-01-intro.html#zero-shot-learning-for-emotion-detection-1",
    "href": "slides/slides-01-intro.html#zero-shot-learning-for-emotion-detection-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning for emotion detection",
    "text": "Zero-shot learning for emotion detection\n\n\n\n\n\n\n\n\n\n\n\n\nsequence\nlabels\nscores\n\n\n\n\n0\ni don t feel particularly agitated\n[surprise, anger, joy, sadness, fear, love]\n[0.3600878417491913, 0.3019028306007385, 0.11901311576366425, 0.11381475627422333, 0.060391537845134735, 0.04478989914059639]\n\n\n1\ni feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey\n[joy, love, surprise, fear, sadness, anger]\n[0.36994311213493347, 0.28871551156044006, 0.25607970356941223, 0.04292311519384384, 0.03344893828034401, 0.008889686316251755]\n\n\n2\ni pay attention it deepens into a feeling of being invaded and helpless\n[fear, surprise, sadness, anger, joy, love]\n[0.3414697051048279, 0.3088077902793884, 0.25616782903671265, 0.07989819347858429, 0.007844815962016582, 0.005811681505292654]\n\n\n3\ni just feel extremely comfortable with the group of people that i dont even need to hide myself\n[joy, surprise, love, sadness, anger, fear]\n[0.33052313327789307, 0.2947230041027069, 0.15343120694160461, 0.0769142434000969, 0.07596743106842041, 0.06844092905521393]\n\n\n4\ni find myself in the odd position of feeling supportive of\n[surprise, joy, fear, love, sadness, anger]\n[0.8287989497184753, 0.0431794710457325, 0.03977375477552414, 0.03141302987933159, 0.03141230717301369, 0.025422487407922745]\n\n\n5\ni was feeling as heartbroken as im sure katniss was\n[sadness, surprise, fear, love, anger, joy]\n[0.7667977213859558, 0.1818464994430542, 0.025871247053146362, 0.01175675168633461, 0.008171566762030125, 0.005556134041398764]\n\n\n6\ni feel a little mellow today\n[surprise, joy, love, fear, sadness, anger]\n[0.4937359094619751, 0.2632196545600891, 0.11367890238761902, 0.06402157247066498, 0.05095512419939041, 0.014388807117938995]\n\n\n7\ni feel like my only role now would be to tear your sails with my pessimism and discontent\n[sadness, anger, surprise, fear, joy, love]\n[0.6992810368537903, 0.20048701763153076, 0.06185832992196083, 0.032874055206775665, 0.0036468510515987873, 0.0018528478685766459]\n\n\n8\ni feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight\n[anger, surprise, sadness, fear, joy, love]\n[0.6029909253120422, 0.19827117025852203, 0.10198791325092316, 0.08116964250802994, 0.010117068886756897, 0.005463309586048126]\n\n\n9\ni feel like reds and purples are just so rich and kind of perfect\n[joy, surprise, love, anger, fear, sadness]\n[0.36441490054130554, 0.3051193952560425, 0.1946256458759308, 0.05556650087237358, 0.05413544923067093, 0.026138072833418846]"
  },
  {
    "objectID": "slides/slides-01-intro.html#example-predicting-labels-of-a-given-image",
    "href": "slides/slides-01-intro.html#example-predicting-labels-of-a-given-image",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Predicting labels of a given image",
    "text": "Example: Predicting labels of a given image\n\nSuppose you have a bunch of animal images. You do not have any labels associated with them and you want to predict labels of these images.\nWe can use machine learning to predict labels of these images using a technique called transfer learning.\n\n\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\n                     tiger cat              0.636\n              tabby, tabby cat              0.174\nPembroke, Pembroke Welsh corgi              0.081\n               lynx, catamount              0.011\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.994\n                  leopard, Panthera pardus              0.005\njaguar, panther, Panthera onca, Felis onca              0.001\n       snow leopard, ounce, Panthera uncia              0.000\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                   Class  Probability score\n                                 macaque              0.885\npatas, hussar monkey, Erythrocebus patas              0.062\n      proboscis monkey, Nasalis larvatus              0.015\n                       titi, titi monkey              0.010\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                        Class  Probability score\nWalker hound, Walker foxhound              0.582\n             English foxhound              0.144\n                       beagle              0.068\n                  EntleBucher              0.059\n--------------------------------------------------------------\n\n\n\n:::"
  },
  {
    "objectID": "slides/slides-01-intro.html#finding-groups-in-food-images",
    "href": "slides/slides-01-intro.html#finding-groups-in-food-images",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Finding groups in food images",
    "text": "Finding groups in food images"
  },
  {
    "objectID": "slides/slides-01-intro.html#k-means-on-food-dataset",
    "href": "slides/slides-01-intro.html#k-means-on-food-dataset",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "K-Means on food dataset",
    "text": "K-Means on food dataset\n\n\n\ndensenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\ndensenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer\n\n\nZ_food = get_features_unsup(densenet, food_inputs)\nk = 5\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_food)\n\nKMeans(n_clusters=5, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters¬†\n5\n\n\n\ninit¬†\n'k-means++'\n\n\n\nn_init¬†\n'auto'\n\n\n\nmax_iter¬†\n300\n\n\n\ntol¬†\n0.0001\n\n\n\nverbose¬†\n0\n\n\n\nrandom_state¬†\n123\n\n\n\ncopy_x¬†\nTrue\n\n\n\nalgorithm¬†\n'lloyd'"
  },
  {
    "objectID": "slides/slides-01-intro.html#examining-food-clusters",
    "href": "slides/slides-01-intro.html#examining-food-clusters",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Examining food clusters",
    "text": "Examining food clusters\n\n\n\n\nfor cluster in range(k):\n    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)\n\n39\nImage indices:  [ 39 197  12  14 138 181]\n\n\n\n\n\n\n\n\n\n228\nImage indices:  [228  65 128  54 175 260]\n\n\n\n\n\n\n\n\n\n138\nImage indices:  [138  54 185 278  39  89]\n\n\n\n\n\n\n\n\n\n193\nImage indices:  [193  39 145 212 169 108]\n\n\n\n\n\n\n\n\n\n120\nImage indices:  [120 268 244  94  72  87]"
  },
  {
    "objectID": "slides/slides-01-intro.html#questions-for-you",
    "href": "slides/slides-01-intro.html#questions-for-you",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "‚ùì‚ùì Questions for you",
    "text": "‚ùì‚ùì Questions for you\niClicker cloud join link: https://join.iclicker.com/FZMQ\nSelect all that apply: Which problems are suitable for ML?\n\n\nChecking if a UBC email address ends with @student.ubc.ca before allowing login\n\n\nDeciding which students should be awarded a scholarship based on their personal essays\n\n\nPredicting which songs you‚Äôll like based on your Spotify listening history\n\n\nDetecting plagiarism by checking if two essays are exactly identical\n\n\nAutomatically tagging photos of your friends on Instagram"
  },
  {
    "objectID": "slides/slides-01-intro.html#summary-when-is-ml-suitable",
    "href": "slides/slides-01-intro.html#summary-when-is-ml-suitable",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Summary: When is ML suitable?",
    "text": "Summary: When is ML suitable?\n\n\n\n\n\n\n\nApproach\nBest Used When‚Ä¶\n\n\n\n\nMachine Learning\nThe dataset is large and complex, and the decision rules are unknown, fuzzy, or too complex to define explicitly\n\n\nRule-based System\nThe logic is clear and deterministic, and the rules or thresholds are known and stable\n\n\nHuman Expert\nThe problem involves ethics, creativity, emotion, or ambiguity that can‚Äôt be formalized easily"
  },
  {
    "objectID": "slides/slides-01-intro.html#activity-2",
    "href": "slides/slides-01-intro.html#activity-2",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 2",
    "text": "Activity 2\nThink of a problem you have come across in the past which could be solved using machine learning.\n\nWhat would be the input and output?\nHow do humans solve this now? Are there heuristics or rules?\nWhat kind of data do you have or could you collect?"
  },
  {
    "objectID": "slides/slides-01-intro.html#types-of-machine-learning",
    "href": "slides/slides-01-intro.html#types-of-machine-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nHere are some typical learning problems.\n\nSupervised learning (Gmail spam filtering)\nUnsupervised learning (Google News)\nReinforcement learning (AlphaGo)\nGenerative AI (ChatGPT)\nRecommendation systems (Amazon item recommendation system)"
  },
  {
    "objectID": "slides/slides-01-intro.html#what-is-supervised-learning",
    "href": "slides/slides-01-intro.html#what-is-supervised-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is supervised learning?",
    "text": "What is supervised learning?\n\nTraining data comprises a set of observations (X) and their corresponding targets (y).\nWe wish to find a model function f that relates X to y.\nWe use the model function to predict targets of new examples."
  },
  {
    "objectID": "slides/slides-01-intro.html#evas-questions",
    "href": "slides/slides-01-intro.html#evas-questions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ü§î Eva‚Äôs questions",
    "text": "ü§î Eva‚Äôs questions\n\n\nAt this point, Eva is wondering about many questions.\n\nHow are we exactly ‚Äúlearning‚Äù whether a message is spam and ham?\nAre we expected to get correct predictions for all possible messages? How does it predict the label for a message it has not seen before?\n\nWhat if the model mis-labels an unseen example? For instance, what if the model incorrectly predicts a non-spam as a spam? What would be the consequences?\nHow do we measure the success or failure of spam identification?\nIf you want to use this model in the wild, how do you know how reliable it is?\n\nWould it be useful to know how confident the model is about the predictions rather than just a yes or a no?\n\nIt‚Äôs great to think about these questions right now. But Eva has to be patient. By the end of this course you‚Äôll know answers to many of these questions!"
  },
  {
    "objectID": "slides/slides-01-intro.html#break",
    "href": "slides/slides-01-intro.html#break",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Break",
    "text": "Break"
  },
  {
    "objectID": "slides/slides-01-intro.html#course-website",
    "href": "slides/slides-01-intro.html#course-website",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course website",
    "text": "Course website\n\n\n\n\n\n\nImportant\n\n\nCourse website: https://github.com/UBC-CS/cpsc330-2025W1 is the most important link. Please read everything on this GitHub page!\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMake sure you go through the syllabus thoroughly and complete the syllabus quiz before Sept 19th at 11:59pm."
  },
  {
    "objectID": "slides/slides-01-intro.html#what-do-we-cover",
    "href": "slides/slides-01-intro.html#what-do-we-cover",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What do we cover",
    "text": "What do we cover\n\nDesigned for a diverse group of students (CS, Statistics, and beyond)\n\nGentle introduction to machine learning, but also valuable for those with prior experience\n\nCovers foundational concepts in ML and data science:\n\nData preprocessing, supervised learning, clustering\n\nRecommendation systems, text processing\n\nIntro to neural networks, time series, survival analysis\n\n\nEmphasis on hands-on skills:\n\nModel development, evaluation, interpretation\n\nEthical considerations and clear communication"
  },
  {
    "objectID": "slides/slides-01-intro.html#course-structure",
    "href": "slides/slides-01-intro.html#course-structure",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course structure",
    "text": "Course structure\n\nIntroduction\n\nWeek 1\n\nPart I: ML fundamentals, preprocessing, midterm 1\n\nWeeks 2, 3, 4, 5, 6, 7, 8\n\nPart II: Unsupervised learning, transfer learning, common special cases, midterm 1\n\nWeeks 8, 9, 10, 11, 12\n\nPart III: Communication and ethics\n\nML skills are not beneficial if you can‚Äôt use them responsibly and communicate your results. In this module we‚Äôll talk about these aspects.\nWeeks 13, 14"
  },
  {
    "objectID": "slides/slides-01-intro.html#cpsc-330-vs.-340",
    "href": "slides/slides-01-intro.html#cpsc-330-vs.-340",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 vs.¬†340",
    "text": "CPSC 330 vs.¬†340\nRead 330_vs_340 which explains the difference between two courses.\nTLDR:\n\n340: how do ML models work?\n330: how do I use ML models?\nCPSC 340 has many prerequisites.\nCPSC 340 goes deeper but has a more narrow scope.\nI think CPSC 330 will be more useful if you just plan to apply basic ML."
  },
  {
    "objectID": "slides/slides-01-intro.html#course-calendar",
    "href": "slides/slides-01-intro.html#course-calendar",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course calendar",
    "text": "Course calendar\nHere is our course Calendar. Make sure you check it on a regular basis:\nhttps://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330-2025W1/blob/main/docs/calendar.html"
  },
  {
    "objectID": "slides/slides-01-intro.html#lecture-format",
    "href": "slides/slides-01-intro.html#lecture-format",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture format",
    "text": "Lecture format\n\nIn person lectures T/Th.\nSometimes there will be videos to watch before lecture. You will find the list of pre-watch videos in the schedule on the course webpage.\nWe will also try to work on some questions and exercises together during the class.\nAll materials will be posted in this GitHub repository."
  },
  {
    "objectID": "slides/slides-01-intro.html#tutorials",
    "href": "slides/slides-01-intro.html#tutorials",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Tutorials",
    "text": "Tutorials\n\nWeekly tutorials will be run by the TAs.\nThere is a small bonus grade associated with attending tutorials.\nMake use of this helpful resource."
  },
  {
    "objectID": "slides/slides-01-intro.html#lecture-notes",
    "href": "slides/slides-01-intro.html#lecture-notes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture notes",
    "text": "Lecture notes\n\nA draft version of lecture notes is available in this Jupyter book\nA ‚Äúfinalized‚Äù version will be pushed to GitHub and the Jupyter book right before each class.\nEach instructor will have slightly adapted versions of notes to present slides during lectures.\n\nYou will find the link to these slides in our repository: https://ubc-cs.github.io/cpsc330-2025W1/lectures/102-Varada-lectures/README.html\nAll the lectures from last year are available here.\nWe cannot promise anything will stay the same from last year to this year, so read them in advance at your own risk."
  },
  {
    "objectID": "slides/slides-01-intro.html#registration-waitlist-and-prerequisites",
    "href": "slides/slides-01-intro.html#registration-waitlist-and-prerequisites",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Registration, waitlist and prerequisites",
    "text": "Registration, waitlist and prerequisites\n\n\n\n\n\n\nImportant\n\n\nPlease go through this document carefully before contacting your instructors about these issues. Even then, we are very unlikely to be able to help with registration, waitlist or prerequisite issues.\n\n\n\n\nThere are still seats available in Section 103.\nIf you are on the waitlist and would like to try your chances, you should already have access to Piazza and Gradescope.\nPlease note that it is your responsibility to complete and submit all assessments while you are on the waitlist. No concessions will be made for students who are waitlisted.\nIf you are unable to secure a seat this term, the course will be offered again with two sections next semester, and once more in the summer."
  },
  {
    "objectID": "slides/slides-01-intro.html#course-conda-environment",
    "href": "slides/slides-01-intro.html#course-conda-environment",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course conda environment",
    "text": "Course conda environment\n\nFollow the setup instructions here to create a course conda environment on your computer.\nIf you do not have your computer with you, you can partner up with someone and set up your own computer later."
  },
  {
    "objectID": "slides/slides-01-intro.html#python-requirementsresources",
    "href": "slides/slides-01-intro.html#python-requirementsresources",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Python requirements/resources",
    "text": "Python requirements/resources\nWe will primarily use Python in this course.\nHere is the basic Python knowledge you‚Äôll need for the course:\n\nBasic Python programming\nNumpy\nPandas\nBasic matplotlib\n\nHomework 1 is all about Python.\n\n\n\n\n\n\nNote\n\n\nWe do not have time to teach all the Python we need but you can find some useful Python resources here."
  },
  {
    "objectID": "slides/slides-01-intro.html#workload",
    "href": "slides/slides-01-intro.html#workload",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Workload",
    "text": "Workload\nWhat does a typical week look like?\n\nBefore class: Watch pre-lecture videos or preview notes\n\nIn class: Two 80-minute lectures with iClicker questions, activities, and live demos\n\nSupport: Weekly tutorials and office hours\nPractice: Weekly assignments (except exam weeks)"
  },
  {
    "objectID": "slides/slides-01-intro.html#tips-for-success",
    "href": "slides/slides-01-intro.html#tips-for-success",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Tips for success:",
    "text": "Tips for success:\n\nAttend lectures regularly and ask questions\n\nStart homework early. Hands-on practice is essential\nUse Generative AI tools responsibly. No blind copy-pasting\nAlways question your data, methods, and results ‚Äî justify your choices"
  },
  {
    "objectID": "slides/slides-01-intro.html#attendance",
    "href": "slides/slides-01-intro.html#attendance",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Attendance",
    "text": "Attendance\nRaise your hand if you‚Äôve ever\n\nLearned something important just from overhearing a classmate‚Äôs question\nSuddenly ‚Äúgot it‚Äù because of something the instructor said or showed that wasn‚Äôt written in the slides üí°\nMade a good friend or professional connection just by showing up in a classroom ü§ù\nFelt more motivated when the room around you was engaged\n\nYour presence and engagement matters!! Together we create energy that makes lectures valuable!"
  },
  {
    "objectID": "slides/slides-01-intro.html#grading-scheme",
    "href": "slides/slides-01-intro.html#grading-scheme",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Grading scheme",
    "text": "Grading scheme\n\nThe grading breakdown is here.\nThe policy on challenging grades is here."
  },
  {
    "objectID": "slides/slides-01-intro.html#exams",
    "href": "slides/slides-01-intro.html#exams",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Exams",
    "text": "Exams\nTwo midterms and a final exam all on PrairieLearn in Computer-based Testing Facility (CBTF).\n\nMidterm 1: Conceptual. Multiple choice, multi-select, and reasoning questions.\nMidterm 2: Includes some coding.\nFinal exam: A mix of question types covering the full course."
  },
  {
    "objectID": "slides/slides-01-intro.html#homework-assignmnts",
    "href": "slides/slides-01-intro.html#homework-assignmnts",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Homework assignmnts",
    "text": "Homework assignmnts\n\nOur notes are created in a Jupyter notebook, with file extension .ipynb.\nAlso, you will complete your homework assignments using Jupyter notebooks.\nConfusingly, ‚ÄúJupyter notebook‚Äù is also the original application that opens .ipynb files - but has since been replaced by Jupyter Lab.\n\nI am using Jupyter Lab, some things might not work with the Jupyter notebook application.\nYou can also open these files in Visual Studio Code."
  },
  {
    "objectID": "slides/slides-01-intro.html#jupyter-notebooks",
    "href": "slides/slides-01-intro.html#jupyter-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\n\nNotebooks contain a mix of code, code output, markdown-formatted text (including LaTeX equations), and more.\nWhen you open a Jupyter notebook in one of these apps, the document is ‚Äúlive‚Äù, meaning you can run the code.\n\nFor example:\n\n1 + 1\n\n2\n\n\n\nx = [1, 2, 3]\nx[0] = 9999\nx\n\n[9999, 2, 3]"
  },
  {
    "objectID": "slides/slides-01-intro.html#jupyter",
    "href": "slides/slides-01-intro.html#jupyter",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter",
    "text": "Jupyter\n\nBy default, Jupyter prints out the result of the last line of code, so you don‚Äôt need as many print statements.\nIn addition to the ‚Äúlive‚Äù notebooks, Jupyter notebooks can be statically rendered in the web browser, e.g.¬†this.\n\nThis can be convenient for quick read-only access, without needing to launch the Jupyter notebook/lab application.\nBut you need to launch the app properly to interact with the notebooks."
  },
  {
    "objectID": "slides/slides-01-intro.html#important-note",
    "href": "slides/slides-01-intro.html#important-note",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Important note",
    "text": "Important note\n\nNote that your first homework assignment is due this coming Tuesday, September 9, 11:59 PM. This is a relatively straightforward assignment on Python.\nIf you struggle with this assignment then that could be a sign that you will struggle later on in the course.\n\nYou must do the first two homework assignments on your own."
  },
  {
    "objectID": "slides/slides-01-intro.html#plagiarism",
    "href": "slides/slides-01-intro.html#plagiarism",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Plagiarism",
    "text": "Plagiarism\nRaise your hand if you‚Äôve ever copied code from StackOverflow.\n\n\n\n\n\nCopying isn‚Äôt always wrong but not acknowledging it is.\nPlagiarism may lead to serious consequences"
  },
  {
    "objectID": "slides/slides-01-intro.html#using-generative-ai-in-this-course",
    "href": "slides/slides-01-intro.html#using-generative-ai-in-this-course",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Using generative AI in this course",
    "text": "Using generative AI in this course\nPlease read our full Generative AI usage policy.\nTL;DR: use AI to support, not substitute, your work. If you use a tool:\n\nCite it (tool name/version).\nAnnotate your use (what you asked it to do and how you incorporated/edited the output).\nBe able to explain and reproduce your work without the tool.\nDo not share sensitive content (e.g., assessments, private data).\nFollow group-work rules and be extra careful when collaborating.\nYou are responsible for any errors (‚Äúhallucinations‚Äù) the tool produces."
  },
  {
    "objectID": "slides/slides-01-intro.html#optional-slides-on-llm-usage",
    "href": "slides/slides-01-intro.html#optional-slides-on-llm-usage",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "(Optional) Slides on LLM usage",
    "text": "(Optional) Slides on LLM usage\nCheck out these slides I‚Äôve put together for GenAI usage during your learning journey."
  },
  {
    "objectID": "slides/slides-01-intro.html#who-to-contact-grading-concerns",
    "href": "slides/slides-01-intro.html#who-to-contact-grading-concerns",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Who to contact: Grading concerns?",
    "text": "Who to contact: Grading concerns?\n\nStart by opening a regrade request.\n\nIf not resolved in two weeks, reach out to your section instructor"
  },
  {
    "objectID": "slides/slides-01-intro.html#who-to-contact-admin-stuffconcessions",
    "href": "slides/slides-01-intro.html#who-to-contact-admin-stuffconcessions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Who to contact: admin stuff/concessions?",
    "text": "Who to contact: admin stuff/concessions?\n\nRead Frequently Asked Questions before contacting.\nContact our course co-ordinator"
  },
  {
    "objectID": "slides/slides-01-intro.html#who-to-contact-questions-on-the-content",
    "href": "slides/slides-01-intro.html#who-to-contact-questions-on-the-content",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Who to contact: Questions on the content?",
    "text": "Who to contact: Questions on the content?\n\nMake sure to read our guide on asking for help before reaching out.\n\nPost your question on piazza.\nMake use of instructor and TA office hours and tutorials\nI am open to answering questions after class."
  },
  {
    "objectID": "slides/slides-01-intro.html#code-of-conduct",
    "href": "slides/slides-01-intro.html#code-of-conduct",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Code of conduct",
    "text": "Code of conduct\n\nOur main forum for getting help will be Piazza.\n\n\n\n\n\n\n\nImportant\n\n\nPlease read this entire document about asking for help. TLDR: Be respectful."
  },
  {
    "objectID": "slides/slides-01-intro.html#checklist-for-you-before-the-next-class",
    "href": "slides/slides-01-intro.html#checklist-for-you-before-the-next-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Checklist for you before the next class",
    "text": "Checklist for you before the next class\n\nHave you read the syllabus and course policies carefully?\nAre you able to access course Canvas shell?\nAre you able to access course Piazza?\nAre you able to access Gradescope? (If not, refer to the Gradescope Student Guide.)\nAre you able to access iClicker Cloud for Section 102 for this course? Make sure to access it via Canvas.\nDid you follow the setup instructions here to create a course conda environment on your computer?\nDid you complete the ‚ÄúGetting to know you‚Äù survey on Canvas?\nDid you complete the anonymous restaurant survey on Qualtrics?\nAre you almost finished or at least started with homework 1?"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#happy-halloween",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#happy-halloween",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Happy Halloween",
    "text": "Happy Halloween"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#announcements",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#announcements",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Announcements",
    "text": "Announcements\n\nHW6 is due next week Monday\n\nComputationally intensive\nYou need to install many packages"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#imports",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#imports",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#iclicker-exercise-15.1",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#iclicker-exercise-15.1",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "iClicker Exercise 15.1",
    "text": "iClicker Exercise 15.1\nSelect all of the following statements which are TRUE.\n\n\nWith \\(n\\) examples, \\(k\\) clusters, and \\(d\\) features, K-Means learns \\(k\\) cluster centers, each \\(d\\)-dimensional.\n\n\nThe meaning of \\(k\\) in K-nearest neighbours and K-Means clustering is very similar.\n\n\nScaling of input features is crucial in clustering.\n\n\n\nIn clustering, it‚Äôs almost always a good idea to find equal-sized clusters."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#shape-of-clusters",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#shape-of-clusters",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Shape of clusters",
    "text": "Shape of clusters\n\nGood for spherical clusters of more or less equal sizes"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-1",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-1",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "K-Means: failure case 1",
    "text": "K-Means: failure case 1\n\nK-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below)."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-2",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-2",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "K-Means: failure case 2",
    "text": "K-Means: failure case 2\n\nAgain, K-Means is unable to capture complex cluster shapes."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-3",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#k-means-failure-case-3",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "K-Means: failure case 3",
    "text": "K-Means: failure case 3\n\nIt assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#dbscan",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#dbscan",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nDensity-Based Spatial Clustering of Applications with Noise\nA density-based clustering algorithm\n\n\nX, y = make_moons(n_samples=200, noise=0.08, random_state=42)\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(X)\nplot_original_clustered(X, dbscan, dbscan.labels_)"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#two-main-hyperparameters",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#two-main-hyperparameters",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Two main hyperparameters",
    "text": "Two main hyperparameters\nIn order to identify dense regions, we need two hyperparameters:\n\neps: determines what it means for points to be ‚Äúclose‚Äù\nmin_samples: determines the number of neighbouring points we require to consider in order for a point to be part of a cluster"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-analogy",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-analogy",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "DBSCAN Analogy",
    "text": "DBSCAN Analogy\n\n\n\n\n\n\n\n\n\n\n\n\nConsider DBSCAN in a social context:\n\nSocial butterflies (ü¶ã): Core points\nFriends of social butterflies who are not social butterflies: Border points\nLone wolves (üê∫): Noise points"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-algorithm",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-algorithm",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "DBSCAN algorithm",
    "text": "DBSCAN algorithm\n\n\n\n\n\nPick a point \\(p\\) at random.\nCheck whether \\(p\\) is a ‚Äúcore‚Äù point or not.\nIf \\(p\\) is a core point, give it a colour (label).\nSpread the colour of \\(p\\) to all of its neighbours.\nCheck if any of the neighbours that received the colour is a core point, if yes, spread the colour to its neighbors as well.\nOnce there are no more core points left to spread the colour, pick a new unlabeled point \\(p\\) and repeat the process."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-failure-cases",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#dbscan-failure-cases",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "DBSCAN: failure cases",
    "text": "DBSCAN: failure cases\n\nLet‚Äôs consider this dataset with three clusters of varying densities.\n\nK-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of \\(K\\) in advance.\n\n\n\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#hierarchical-clustering",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#hierarchical-clustering",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#dendrogram",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#dendrogram",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Dendrogram",
    "text": "Dendrogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nDendrogram is a tree-like plot.\nOn the x-axis we have data points.\nOn the y-axis we have distances between clusters."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#flat-clusters",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#flat-clusters",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Flat clusters",
    "text": "Flat clusters\n\nThis is good but how can we get cluster labels from a dendrogram?\nWe can bring the clustering to a ‚Äúflat‚Äù format use fcluster"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#flat-clusters-1",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#flat-clusters-1",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Flat clusters",
    "text": "Flat clusters\n\nfrom scipy.cluster.hierarchy import fcluster\n# flattening the dendrogram based on maximum number of clusters. \nhier_labels1 = fcluster(linkage_array, 3, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels1, title=\"flattened with max_clusts=3\")"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#linkage-criteria",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#linkage-criteria",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Linkage criteria",
    "text": "Linkage criteria\n\nWhen we create a dendrogram, we need to calculate distance between clusters. How do we measure distances between clusters?\nThe linkage criteria determines how to find similarity between clusters:\nSome example linkage criteria are:\n\nSingle linkage \\(\\rightarrow\\) smallest minimal distance, leads to loose clusters\nComplete linkage \\(\\rightarrow\\) smallest maximum distance, leads to tight clusters\nAverage linkage \\(\\rightarrow\\) smallest average distance between all pairs of points in the clusters\nWard linkage \\(\\rightarrow\\) smallest increase in within-cluster variance, leads to equally sized clusters"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#example-single-linkage",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#example-single-linkage",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Example: Single linkage",
    "text": "Example: Single linkage\nSuppose you want to go from 3 clusters to 2 clusters. Which clusters would you merge?\n\nX_orig, y = make_blobs(random_state=0, n_samples=11)\nX = StandardScaler().fit_transform(X_orig)\nlinkage_array = single(X)\nhier_labels = fcluster(linkage_array, 3, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 3\", color_threshold=1.0)"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#example-single-linkage-1",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#example-single-linkage-1",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Example: Single linkage",
    "text": "Example: Single linkage\n\nhier_labels = fcluster(linkage_array, 2, criterion=\"maxclust\") \nplot_dendrogram_clusters(X, linkage_array, hier_labels, title=\"maxclust 2\", color_threshold=1.0)"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#iclicker-exercise-2.3",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#iclicker-exercise-2.3",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "iClicker Exercise 2.3",
    "text": "iClicker Exercise 2.3\nSelect all of the following statements which are True\n\n\nIn hierarchical clustering we do not have to worry about initialization.\n\n\nHierarchical clustering can only be applied to smaller datasets because dendrograms are hard to visualize for large datasets.\n\n\nIn all the clustering methods we have seen (K-Means, GMMs, DBSCAN, hierarchical clustering), there is a way to decide the number of clusters.\n\n\nTo get robust clustering we can naively ensemble cluster labels (e.g., pick the most popular label) produced by different clustering methods.\n\n\nIf you have a high Silhouette score and very clean and robust clusters, it means that the algorithm has captured the semantic meaning in the data of our interest."
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#activity",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#activity",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Activity",
    "text": "Activity\nDiscuss the following\n\n\n\n\n\n\n\n\n\nClustering Method\nKMeans\nDBSCAN\nHierarchical Clustering\n\n\n\n\nApproach\n\n\n\n\n\nHyperparameters\n\n\n\n\n\nShape of clusters\n\n\n\n\n\nHandling noise\n\n\n\n\n\nDistance metric"
  },
  {
    "objectID": "slides/slides-15-DBSCAN-hierarchical.html#discussion-question",
    "href": "slides/slides-15-DBSCAN-hierarchical.html#discussion-question",
    "title": "CPSC 330 Lecture 15: DBSCAN, Hierarchical Clustering",
    "section": "Discussion question",
    "text": "Discussion question\nWhich clustering method would you use in each of the scenarios below? Why? How would you represent the data in each case?\n\nScenario 1: Customer segmentation in retail\nScenario 2: An environmental study aiming to identify clusters of a rare plant species\nScenario 3: Clustering furniture items for inventory management and customer recommendations"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#section",
    "href": "slides/slides-08-hyperparameter-optimization.html#section",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#check-in",
    "href": "slides/slides-08-hyperparameter-optimization.html#check-in",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Check-in",
    "text": "Check-in\nHow are you feeling today?\n\n\nThings are more or less under control!\n\n\nExcited about hyperparameter optimization!\n\n\nLost üòû\n\n\nTired / Sleepy üò¥\n\n\nSecretly thinking of lunch ü•ó"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#announcements",
    "href": "slides/slides-08-hyperparameter-optimization.html#announcements",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/mekbcze4gyber/post/162\n\nHW3 was due on Monday, Sept 29th 11:59 pm.\nHW4 has been released"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#recap-iclicker-logistic-regression-1",
    "href": "slides/slides-08-hyperparameter-optimization.html#recap-iclicker-logistic-regression-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: iClicker Logistic Regression 1",
    "text": "Recap: iClicker Logistic Regression 1\nWhich of the following are True?\n\n\nLogistic regression can be used for binary as well as multi-class classification tasks.\n\n\n\nLogistic regression computes a weighted sum of features and applies the sigmoid function.\n\n\n\nThe sigmoid function ensures outputs between 0 and 1, interpreted as probabilities.\n\n\n\nThe decision boundary in logistic regression is linear, even though the sigmoid is applied.\n\n\nWhen the weighted sum is 0, \\(\\hat{p}\\) = 0.5."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#recap-iclicker-logistic-regression-1-1",
    "href": "slides/slides-08-hyperparameter-optimization.html#recap-iclicker-logistic-regression-1-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: iClicker Logistic Regression 1",
    "text": "Recap: iClicker Logistic Regression 1\nWhich of the following are True?\n\n\nLogistic regression coefficients always have to be positive.\n\n\n\nLarger coefficients (in absolute value) indicate stronger feature influence on the prediction.\n\n\n\nFor \\(d\\) features, the decision boundary is a \\(d-1\\) dimensional hyperplane.\n\n\n\nIn sklearn, very small C value shrinks coefficients, often leading to underfitting.\n\n\n\nA larger C value allows larger coefficients and a more complex model."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression",
    "href": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nA linear model used for binary classification tasks.\n\n(Optional) There is am extension of logistic regression called multinomial logistic regression for multiclass classification.\n\nParameters:\n\nCoefficients (Weights): The model learns a coefficient or a weight associated with each feature that represents its importance.\nBias (Intercept): A constant term added to the linear combination of features and their coefficients."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression-1",
    "href": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nThe model computes a weighted sum of the input features‚Äô values, adjusted by their respective coefficients and the bias term.\nThis weighted sum is passed through a sigmoid function to transform it into a probability score, indicating the likelihood of the input belonging to the ‚Äúpositive‚Äù class.\n\n\\[ \\hat{p} = \\sigma\\left(\\sum_{i=1}^d w_i x_i + b\\right) \\]\n\n\\(P_{hat}\\) is the predicted probability of the example belonging to the positive class.\n\\(w_i\\) is the learned weight associated with feature \\(i\\)\n\\(x_i\\) is the value of the input feature \\(i\\)\n\\(b\\) is the bias term"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression-2",
    "href": "slides/slides-08-hyperparameter-optimization.html#recap-logistic-regression-2",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Recap: Logistic regression",
    "text": "Recap: Logistic regression\n\nFor a dataset with \\(d\\) features, the decision boundary that separates the classes is a \\(d-1\\) dimensional hyperplane.\n\nComplexity hyperparameter: C in sklearn.\n\nHigher C \\(\\rightarrow\\) more complex model meaning larger coefficients\nLower C \\(\\rightarrow\\) less complex model meaning smaller coefficients"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#data",
    "href": "slides/slides-08-hyperparameter-optimization.html#data",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Data",
    "text": "Data\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\n3130\nspam\nLookAtMe!: Thanks for your purchase of a video...\n\n\n106\nham\nAight, I'll hit you up when I get some cash\n\n\n4697\nham\nDon no da:)whats you plan?\n\n\n856\nham\nGoing to take your babe out ?"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#model-building",
    "href": "slides/slides-08-hyperparameter-optimization.html#model-building",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Model building",
    "text": "Model building\n\nLet‚Äôs define a pipeline\n\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\n\nSuppose we want to try out different hyperparameter values.\n\n\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#hyperparameter-optimization-with-loops",
    "href": "slides/slides-08-hyperparameter-optimization.html#hyperparameter-optimization-with-loops",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Hyperparameter optimization with loops",
    "text": "Hyperparameter optimization with loops\n\nDefine a parameter space.\nIterate through possible combinations.\nEvaluate model performance.\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#sklearn-methods",
    "href": "slides/slides-08-hyperparameter-optimization.html#sklearn-methods",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "sklearn methods",
    "text": "sklearn methods\n\nsklearn provides two main methods for hyperparameter optimization\n\nGrid Search\nRandom Search"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#grid-search",
    "href": "slides/slides-08-hyperparameter-optimization.html#grid-search",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Grid Search",
    "text": "Grid Search\n\nCovers all possible combinations from the provided grid.\nCan be parallelized easily.\nIntegrates cross-validation."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#grid-search-example",
    "href": "slides/slides-08-hyperparameter-optimization.html#grid-search-example",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Grid search example",
    "text": "Grid search example\n\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(pipe_svm, \n                  param_grid = param_grid, \n                  n_jobs=-1, \n                  return_train_score=True\n                 )\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n\nnp.float64(0.9782606272997375)"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#random-search",
    "href": "slides/slides-08-hyperparameter-optimization.html#random-search",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Random Search",
    "text": "Random Search\n\nMore efficient than grid search when dealing with large hyperparameter spaces.\nSamples a given number of parameter settings from distributions."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#random-search-example",
    "href": "slides/slides-08-hyperparameter-optimization.html#random-search-example",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Random search example",
    "text": "Random search example\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(pipe_svm,                                    \n                  param_distributions = param_dist, \n                  n_iter=10, \n                  n_jobs=-1, \n                  return_train_score=True)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n\nnp.float64(0.9818506556179762)"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#pizza-baking-competition-example",
    "href": "slides/slides-08-hyperparameter-optimization.html#pizza-baking-competition-example",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Pizza baking competition example",
    "text": "Pizza baking competition example\nImagine that you participate in pizza baking competition.\n\n\n\n\n\nTraining phase: Collecting recipes and practicing at home\n\nValidation phase: Inviting a group of friends for tasting and feedback\n\nTest phase (competition day): Serving the judges"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#overfitting-on-the-validation-set",
    "href": "slides/slides-08-hyperparameter-optimization.html#overfitting-on-the-validation-set",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Overfitting on the validation set",
    "text": "Overfitting on the validation set\n\n\n\nYour friends loved your pineapple pizza.\nYou fine-tune your recipe for the same group of friends, perfecting it for their tastes.\n\n\n\n\nOn the competition day, you confidently present your perfected pineapple pizza.\n\nJudges are not impressed: ‚ÄúThis doesn‚Äôt appeal to a broad audience.‚Äù\n\n\n\nThis is similar to reusing the same validation set again and again to perfect the model for it!"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#lesson-overfitting-on-the-validation-set",
    "href": "slides/slides-08-hyperparameter-optimization.html#lesson-overfitting-on-the-validation-set",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Lesson: Overfitting on the validation set",
    "text": "Lesson: Overfitting on the validation set\n\nYou tailored your recipe too closely to your friends‚Äô tastes.\n\nThey were not representative of the broader audience (the judges).\n\nThe pizza, while perfect for your validation group, failed to generalize.\n\nOver many iterations, the validation set no longer gives an unbiased estimate of performance.\n\nThat‚Äôs why we need a separate test set (like a group of tasters who never influenced your pizza)."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#optimization-bias-1",
    "href": "slides/slides-08-hyperparameter-optimization.html#optimization-bias-1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Optimization bias",
    "text": "Optimization bias\n\nWhy do we need separate validation and test datasets?"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#mitigating-optimization-bias.",
    "href": "slides/slides-08-hyperparameter-optimization.html#mitigating-optimization-bias.",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Mitigating optimization bias.",
    "text": "Mitigating optimization bias.\n\nCross-validation\nEnsembles\nRegularization and choosing a simpler model"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#iclicker-exercise-8.1",
    "href": "slides/slides-08-hyperparameter-optimization.html#iclicker-exercise-8.1",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "(iClicker) Exercise 8.1",
    "text": "(iClicker) Exercise 8.1\nSelect all of the following statements which are TRUE.\n\n\nIf you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n\n\nGrid search is guaranteed to find the best hyperparameter values.\n\n\nIt is possible to get different hyperparameters in different runs of RandomizedSearchCV."
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#questions-for-you",
    "href": "slides/slides-08-hyperparameter-optimization.html#questions-for-you",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Questions for you",
    "text": "Questions for you\n\nYou have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n\nProbably\nProbably not"
  },
  {
    "objectID": "slides/slides-08-hyperparameter-optimization.html#questions-for-class-discussion",
    "href": "slides/slides-08-hyperparameter-optimization.html#questions-for-class-discussion",
    "title": "CPSC 330 Lecture 8: Hyperparameter Optimization",
    "section": "Questions for class discussion",
    "text": "Questions for class discussion\n\nSuppose you have 10 hyperparameters, each with 4 possible values. If you run GridSearchCV with this parameter grid, how many cross-validation experiments will be carried out?\nSuppose you have 10 hyperparameters and each takes 4 values. If you run RandomizedSearchCV with this parameter grid with n_iter=20, how many cross-validation experiments will be carried out?"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#focus-on-the-breath",
    "href": "slides/slides-09-classification-metrics.html#focus-on-the-breath",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Focus on the breath!",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#announcements",
    "href": "slides/slides-09-classification-metrics.html#announcements",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/mekbcze4gyber/post/162\nGood news for you: You‚Äôll have access to our course notes in the midterm!\n\nHW4 was due on Monday, Oct 6th 11:59 pm.\nHW5 has been released. It‚Äôs a project-type assignment and you get till Oct 27th to work on it."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#ml-workflow",
    "href": "slides/slides-09-classification-metrics.html#ml-workflow",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ML workflow",
    "text": "ML workflow"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#accuracy",
    "href": "slides/slides-09-classification-metrics.html#accuracy",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Accuracy",
    "text": "Accuracy\n\nSo far, we‚Äôve been measuring model performance using Accuracy.\n\nAccuracy is the proportion of all predictions that were correct ‚Äî whether positive or negative.\n\n\\[\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{total classifications}}\n\\]\n\nBut is accuracy always the right metric to evaluate a model? ü§î"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#a-fraud-classification-example",
    "href": "slides/slides-09-classification-metrics.html#a-fraud-classification-example",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "A fraud classification example",
    "text": "A fraud classification example\n\n\n(139554, 29)\n\n\n\n\n\n\n\n\n\nClass\nTime\nAmount\nV1\nV2\nV3\nV4\nV5\nV6\nV7\n...\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\n\n\n\n\n64454\n0\n51150.0\n1.00\n-3.538816\n3.481893\n-1.827130\n-0.573050\n2.644106\n-0.340988\n2.102135\n...\n-1.509991\n1.345904\n0.530978\n-0.860677\n-0.201810\n-1.719747\n0.729143\n-0.547993\n-0.023636\n-0.454966\n\n\n37906\n0\n39163.0\n18.49\n-0.363913\n0.853399\n1.648195\n1.118934\n0.100882\n0.423852\n0.472790\n...\n0.810267\n-0.192932\n0.687055\n-0.094586\n0.121531\n0.146830\n-0.944092\n-0.558564\n-0.186814\n-0.257103\n\n\n79378\n0\n57994.0\n23.74\n1.193021\n-0.136714\n0.622612\n0.780864\n-0.823511\n-0.706444\n-0.206073\n...\n0.258815\n-0.178761\n-0.310405\n-0.842028\n0.085477\n0.366005\n0.254443\n0.290002\n-0.036764\n0.015039\n\n\n245686\n0\n152859.0\n156.52\n1.604032\n-0.808208\n-1.594982\n0.200475\n0.502985\n0.832370\n-0.034071\n...\n-1.009429\n-0.040448\n0.519029\n1.429217\n-0.139322\n-1.293663\n0.037785\n0.061206\n0.005387\n-0.057296\n\n\n60943\n0\n49575.0\n57.50\n-2.669614\n-2.734385\n0.662450\n-0.059077\n3.346850\n-2.549682\n-1.430571\n...\n0.157993\n-0.430295\n-0.228329\n-0.370643\n-0.211544\n-0.300837\n-1.174590\n0.573818\n0.388023\n0.161782\n\n\n\n\n5 rows √ó 31 columns"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#dummyclassifier",
    "href": "slides/slides-09-classification-metrics.html#dummyclassifier",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "DummyClassifier",
    "text": "DummyClassifier\nLet‚Äôs try a DummyClassifier, which makes predictions without learning any patterns.\n\ndummy = DummyClassifier()\ncross_val_score(dummy, X_train, y_train).mean()\n\nnp.float64(0.9983017327649726)\n\n\n\nThe accuracy looks surprisingly high!\nShould we be happy with this model and deploy it?"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#problem-class-imbalance",
    "href": "slides/slides-09-classification-metrics.html#problem-class-imbalance",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Problem: Class imbalance",
    "text": "Problem: Class imbalance\n\ny_train.value_counts()\n\nClass\n0    139317\n1       237\nName: count, dtype: int64\n\n\n\nIn many real-world problems, some classes are much rarer than others.\nA model that always predicts ‚Äúno fraud‚Äù could still achieve &gt;99% accuracy!\nThis is why accuracy can be misleading in imbalanced datasets.\nWe need metrics that differentiate types of errors."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#dummyclassifier-confusion-matrix",
    "href": "slides/slides-09-classification-metrics.html#dummyclassifier-confusion-matrix",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "DummyClassifier: Confusion matrix",
    "text": "DummyClassifier: Confusion matrix\nWhich types of errors would be most critical for the bank to address? Missing a fraud case or flagging a legitimate transaction as fraud?"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#logisticregression-confusion-matrix",
    "href": "slides/slides-09-classification-metrics.html#logisticregression-confusion-matrix",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "LogisticRegression: Confusion matrix",
    "text": "LogisticRegression: Confusion matrix\nAre we doing better with logistic regression?"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#understanding-the-confusion-matrix",
    "href": "slides/slides-09-classification-metrics.html#understanding-the-confusion-matrix",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Understanding the confusion matrix",
    "text": "Understanding the confusion matrix\n\n\n\n\n\nTN \\(\\rightarrow\\) True negatives\nFP \\(\\rightarrow\\) False positives\nFN \\(\\rightarrow\\) False negatives\nTP \\(\\rightarrow\\) True positives"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#confusion-matrix-questions",
    "href": "slides/slides-09-classification-metrics.html#confusion-matrix-questions",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nImagine a spam filter model where emails labeled 1 = spam, 0 = not spam.\nIf a spam email is incorrectly classified as not spam, what kind of error is this?\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#confusion-matrix-questions-1",
    "href": "slides/slides-09-classification-metrics.html#confusion-matrix-questions-1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nIn an intrusion detection system, 1 = intrusion, 0 = safe.\nIf the system misses an actual intrusion and classifies it as safe, this is a:\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#confusion-matrix-questions-2",
    "href": "slides/slides-09-classification-metrics.html#confusion-matrix-questions-2",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Confusion matrix questions",
    "text": "Confusion matrix questions\nIn a medical test for a disease, 1 = diseased, 0 = healthy.\nIf a healthy patient is incorrectly diagnosed as diseased, that‚Äôs a:\n\n\nA false positive\n\n\nA true positive\n\n\nA false negative\n\n\nA true negative"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#metrics-other-than-accuracy",
    "href": "slides/slides-09-classification-metrics.html#metrics-other-than-accuracy",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Metrics other than accuracy",
    "text": "Metrics other than accuracy\nNow that we understand the different types of errors, we can explore metrics that better capture model performance when accuracy falls short, especially for imbalanced datasets.\nWe‚Äôll start with three key ones:\n\nPrecision\nRecall\nF1-score"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#precision-and-recall",
    "href": "slides/slides-09-classification-metrics.html#precision-and-recall",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Precision and recall",
    "text": "Precision and recall\nLet‚Äôs revisit our fraud detection scenario. The circle below represents all transactions predicted as fraud by an imaginary toy model designed to detect fraudulent activity."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#intuition-behind-the-two-metrics",
    "href": "slides/slides-09-classification-metrics.html#intuition-behind-the-two-metrics",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Intuition behind the two metrics",
    "text": "Intuition behind the two metrics\n\nPrecision: Of all the transactions predicted as fraud, how many were actually fraud?\n\nHigh precision \\(\\rightarrow\\) few false alarms (low false positives).\n\nRecall: Of all the actual fraud cases, how many did the model catch?\n\nHigh recall \\(\\rightarrow\\) few missed frauds (low false negatives)."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#trade-off-between-precision-and-recall",
    "href": "slides/slides-09-classification-metrics.html#trade-off-between-precision-and-recall",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Trade-off between precision and recall",
    "text": "Trade-off between precision and recall\n\nIncreasing recall often decreases precision, and vice versa.\n\nExample:\n\nPredict ‚Äúfraud‚Äù for every transaction \\(\\rightarrow\\) perfect recall, terrible precision.\n\nPredict ‚Äúfraud‚Äù only when 100% sure \\(\\rightarrow\\) high precision, low recall.\n\n\nThe right balance depends on the application and cost of errors."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#f1-score",
    "href": "slides/slides-09-classification-metrics.html#f1-score",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "F1-score",
    "text": "F1-score\n\nSometimes, we want a single metric that balances precision and recall.\n\nThe F1-score is the harmonic mean of the two:\n\n\\[\nF1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\nHigh F1 means both precision and recall are strong.\n\nUseful when we care about both false positives and false negatives."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#summary",
    "href": "slides/slides-09-classification-metrics.html#summary",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nMetric\nWhat it measures\nHigh value means\n\n\n\n\nAccuracy\nOverall correctness\nModel gets most predictions right\n\n\nPrecision\nQuality of positive predictions\nFew false alarms\n\n\nRecall\nQuantity of true positives caught\nFew missed positives\n\n\nF1-score\nBalance of precision & recall\nBoth precision and recall are high"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#iclicker-exercise-9.1",
    "href": "slides/slides-09-classification-metrics.html#iclicker-exercise-9.1",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "iClicker Exercise 9.1",
    "text": "iClicker Exercise 9.1\nSelect all of the following statements which are TRUE.\n\n\nIn medical diagnosis, false positives are more damaging than false negatives (assume ‚Äúpositive‚Äù means the person has a disease, ‚Äúnegative‚Äù means they don‚Äôt).\n\n\nIn spam classification, false positives are more damaging than false negatives (assume ‚Äúpositive‚Äù means the email is spam, ‚Äúnegative‚Äù means they it‚Äôs not).\n\n\nIf method A gets a higher accuracy than method B, that means its precision is also higher.\n\n\nIf method A gets a higher accuracy than method B, that means its recall is also higher."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#counter-examples",
    "href": "slides/slides-09-classification-metrics.html#counter-examples",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Counter examples",
    "text": "Counter examples\nMethod A - higher accuracy but lower precision\n\n\n\nNegative\nPositive\n\n\n\n\n90\n5\n\n\n5\n0\n\n\n\nMethod B - lower accuracy but higher precision\n\n\n\nNegative\nPositive\n\n\n\n\n80\n15\n\n\n0\n5"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#takeaway",
    "href": "slides/slides-09-classification-metrics.html#takeaway",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Takeaway",
    "text": "Takeaway\n\nAccuracy summarizes overall correctness but hides class-specific behaviour.\n\nYou can have high accuracy but poor precision or recall,\nespecially in imbalanced datasets.\n\nAlways check multiple metrics before deciding which model is better."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#predicting-with-logistic-regression",
    "href": "slides/slides-09-classification-metrics.html#predicting-with-logistic-regression",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Predicting with logistic regression",
    "text": "Predicting with logistic regression\n\n\n\n\n\nMost classification models don‚Äôt directly predict labels. They predict scores or probabilities.\nTo get a label (e.g., ‚Äúfraud‚Äù or ‚Äúnon fraud‚Äù), we choose a threshold (often 0.5). If the threshold changes, predictions change, and so do the errors.\nWhat happens to precision and recall if we change the probability threshold?\nPlay with classification thresholds\n\n\n\n\nDecreasing the threshold:\n\nIncreases the number of positive predictions\nIdentify more examples as ‚Äúfraud‚Äù examples\nMore false positives, more true positives.\nRecall would either stay the same or go up and precision is likely to go down (precision may increase if all the new examples after decreasing the threshold are TPs).\n\nIncreasing the threshold:\n\nDecreases the number of positive predictions\nRecall would go down or stay the same but precision is likely to go up (precision may go down if TP decrease but FP do not decrease).\n\nKey question: How do we decide on the best threshold? It depends on the problem and trade-offs between false positives and false negatives."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#pr-curve",
    "href": "slides/slides-09-classification-metrics.html#pr-curve",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "PR curve",
    "text": "PR curve\n\nCalculate precision and recall (TPR) at every possible threshold and graph them.\nTop left \\(\\rightarrow\\) Very high threshold (strict model = high precision)\nBottom right \\(\\rightarrow\\) Very low threshold (linient model = high recall)\n\n\n\n\nWe can look at all possible thresholds and plot the corresponding precision and recall!\nAs we lower the threshold, the classifier predicts more positives:\n\nRecall increases because we capture more true positives.\nPrecision usually decreases because some of the additional positives are false positives.\n\nAs we raise the threshold, the classifier predicts fewer positives:\n\nPrecision increases because only the most confident predictions are positive.\nRecall decreases because we miss more true positives.\n\nThe curve shows the trade-offs between precision and recall across all thresholds.\nIdeal performance: top-right corner (high precision and high recall).\nUse cases like fraud detection often require focusing on areas of the curve that favor one metric over the other (e.g., high recall for safety-critical tasks)."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#pr-curve-different-thresholds",
    "href": "slides/slides-09-classification-metrics.html#pr-curve-different-thresholds",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "PR curve different thresholds",
    "text": "PR curve different thresholds\n\nWhich of the red dots are reasonable trade offs?"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#average-precision-ap-score",
    "href": "slides/slides-09-classification-metrics.html#average-precision-ap-score",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "Average Precision (AP) Score",
    "text": "Average Precision (AP) Score\n\n\n\nAP score summarizes the PR curve by calculating the area under the curve\nIt measures the ranking ability of a model; how well it assigns higher probabilities to positive examples than to negative ones, regardless of the specific threshold."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#iclicker-exercise",
    "href": "slides/slides-09-classification-metrics.html#iclicker-exercise",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "iClicker Exercise",
    "text": "iClicker Exercise\nChoose the appropriate evaluation metric for the following scenarios:\nScenario 1: Balance between precision and recall for a threshold.\nScenario 2: Assess performance across all thresholds.\n\n\nF1 for 1, AP for 2\n\n\nAP for 1, F1 Score for 2\n\n\nAP for both\n\n\nF1 for both\n\n\n\n\nF1 score is for a given threshold and measures the quality of predict.\nAP score is a summary across thresholds and measures the quality of predict_proba."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#iclicker-exercise-9.2",
    "href": "slides/slides-09-classification-metrics.html#iclicker-exercise-9.2",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "iClicker Exercise 9.2",
    "text": "iClicker Exercise 9.2\nSelect all of the following statements which are TRUE.\n\n\nIf we increase the classification threshold, both true and false positives are likely to decrease.\n\n\nIf we increase the classification threshold, both true and false negatives are likely to decrease.\n\n\nLowering the classification threshold generally increases the model‚Äôs recall.\n\n\n\nRaising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#roc-curve",
    "href": "slides/slides-09-classification-metrics.html#roc-curve",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nCompute the True Positive Rate (TPR) and False Positive Rate (FPR) at every possible threshold, and plot TPR vs FPR.\n\nHow well does the model separate positive and negative classes in terms of predicted probability?\nA good choice when the dataset is reasonably balanced or not extremely imbalanced (e.g., fraud detection, disease diagnosis)."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#roc-curve-example",
    "href": "slides/slides-09-classification-metrics.html#roc-curve-example",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ROC Curve example",
    "text": "ROC Curve example\n\nBottom-left \\(\\rightarrow\\) very high threshold (almost everything predicted negative: low recall, low FPR).\n\nTop-right \\(\\rightarrow\\) very low threshold (almost everything predicted positive: high recall, high FPR)."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#auc",
    "href": "slides/slides-09-classification-metrics.html#auc",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "AUC",
    "text": "AUC\n\nThe area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative."
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#roc-auc-questions",
    "href": "slides/slides-09-classification-metrics.html#roc-auc-questions",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "ROC AUC questions",
    "text": "ROC AUC questions\nConsider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?\n\n\n\n\n\n\nIf false positives (false alarms) are highly costly\n\n\nIf false positives are cheap and false negatives (missed true positives) highly costly\n\n\nIf the costs are roughly equivalent\n\n\n\nSource"
  },
  {
    "objectID": "slides/slides-09-classification-metrics.html#what-did-we-learn",
    "href": "slides/slides-09-classification-metrics.html#what-did-we-learn",
    "title": "CPSC 330 Lecture 9: Classification Metrics",
    "section": "What did we learn?",
    "text": "What did we learn?\n\nWhy accuracy is not always a good metric?\nConfusion matrix\nPrecision, recall, & f1-score\nPrecision-recall curves & average precision\nReceiver Operator Characteristic (ROC) curves & AUC"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#focus-on-the-breath",
    "href": "slides/slides-10-regression-metrics.html#focus-on-the-breath",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Focus on the breath!",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#announcements",
    "href": "slides/slides-10-regression-metrics.html#announcements",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/mekbcze4gyber/post/162\nGood news for you: You‚Äôll have access to our course notes in the midterm!\n\nHW4 was due on Monday, Oct 6th 11:59 pm.\nHW5 has been released. It‚Äôs a project-type assignment and you get till Oct 27th to work on it."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#iclicker-oh",
    "href": "slides/slides-10-regression-metrics.html#iclicker-oh",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "iClicker OH",
    "text": "iClicker OH\nI‚Äôm planning to hold an in-person midterm review office hour. Which time works best for you?\n\n\nFriday, October 10th 2pm\n\n\nTuesday, October 14th 2pm\n\n\nTuesday, October 14th 4pm"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#which-metric-fits-best",
    "href": "slides/slides-10-regression-metrics.html#which-metric-fits-best",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Which metric fits best?",
    "text": "Which metric fits best?\n\n\n\nScenario\nData Imbalance\nMain Concern\nBest Metric(s) / Curve\n\n\n\n\nEmail Spam Detection\n10% spam\nAvoid false positives\n\n\n\nDisease Screening\n1 in 10,000\nAvoid false negatives\n\n\n\nCredit Card Fraud\n0.1% fraud\nFocus on rare positive class\n\n\n\nCustomer Churn\n20% churn\nBalance FP & FN\n\n\n\nSentiment Analysis\n50/50 balanced\nOverall correctness\n\n\n\nFace Recognition\nBalanced pairs\nTrade-off FP vs FN"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#summary-choosing-the-right-metric",
    "href": "slides/slides-10-regression-metrics.html#summary-choosing-the-right-metric",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Summary: Choosing the right metric",
    "text": "Summary: Choosing the right metric\n\n\n\n\n\n\n\n\nMetric / Plot\nWhen to Use\nWhy\n\n\n\n\nPrecision, Recall, F1\nWhen you care about specific error types (FP vs FN) or a fixed threshold.\nFocus on particular tradeoffs.\n\n\nPR Curve & AP Score\nWhen the dataset is highly imbalanced (rare positives).\nIgnores TNs; focuses on positives.\n\n\nROC Curve & AUC\nWhen classes are moderately imbalanced.\nMeasures ranking ability across thresholds."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#questions-for-you",
    "href": "slides/slides-10-regression-metrics.html#questions-for-you",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\nWhat‚Äôs the difference between the average precision (AP) score and F1-score?\nWhich model would you pick?"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#roc-of-a-baseline-model",
    "href": "slides/slides-10-regression-metrics.html#roc-of-a-baseline-model",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "ROC of a baseline model",
    "text": "ROC of a baseline model\n\n\n\n\nAUC‚ÄìROC measures the probability that a randomly chosen positive example receives a higher score than a randomly chosen negative example.\n\nPerfect model: (AUC = 1.0). Always ranks positives above negatives.\n\nRandom model (AUC = 0.5): No discriminative ability (equivalent to random guessing).\n\n\n\nSource"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#questions-for-you-1",
    "href": "slides/slides-10-regression-metrics.html#questions-for-you-1",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Questions for you",
    "text": "Questions for you\n\n\n\n\n\nWhich model would you pick?"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#dealing-with-class-imbalance",
    "href": "slides/slides-10-regression-metrics.html#dealing-with-class-imbalance",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Dealing with class imbalance",
    "text": "Dealing with class imbalance\n\nUnder sampling\nOversampling\nclass weight=\"balanced\" (preferred method for this course)\nSMOTE"
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#handling-imbalance-by-chaning-class-weights",
    "href": "slides/slides-10-regression-metrics.html#handling-imbalance-by-chaning-class-weights",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Handling imbalance by chaning class weights",
    "text": "Handling imbalance by chaning class weights\n\nWe can specify class_weight=‚Äúbalanced‚Äù to give more importance to rare examples during training.\n\n\n\n\nThis sets the weights so that the classes are ‚Äúequal‚Äù.\nWe have reduced false negatives but we have many more false positives now ‚Ä¶\n\nDecreases false negatives, which improves recall.\nIncreases false positives, which might lower precision."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#ridge-and-ridgecv",
    "href": "slides/slides-10-regression-metrics.html#ridge-and-ridgecv",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Ridge and RidgeCV",
    "text": "Ridge and RidgeCV\n\nRidge Regression: alpha hyperparameter controls model complexity.\nRidgeCV: Ridge regression with built-in cross-validation to find the optimal alpha."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#alpha-hyperparameter",
    "href": "slides/slides-10-regression-metrics.html#alpha-hyperparameter",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "alpha hyperparameter",
    "text": "alpha hyperparameter\n\nRole of alpha:\n\nControls model complexity\nHigher alpha: Simpler model, smaller coefficients.\nLower alpha: Complex model, larger coefficients."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#regression-metrics-mse-rmse-mape-r2_score",
    "href": "slides/slides-10-regression-metrics.html#regression-metrics-mse-rmse-mape-r2_score",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Regression metrics: MSE, RMSE, MAPE, r2_score",
    "text": "Regression metrics: MSE, RMSE, MAPE, r2_score\n\nMean Squared Error (MSE): Average of the squares of the errors.\nRoot Mean Squared Error (RMSE): Square root of MSE, same units as the target variable.\nr2 measures how much of the variation in the target variable your model can explain.-\nMean Absolute Percentage Error (MAPE): Average of the absolute percentage errors."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#applying-log-transformation-to-the-targets",
    "href": "slides/slides-10-regression-metrics.html#applying-log-transformation-to-the-targets",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Applying log transformation to the targets",
    "text": "Applying log transformation to the targets\n\nSuitable when the target has a wide range and spans several orders of magnitude\n\nExample: counts data such as social media likes or price data\n\nHelps manage skewed data, making patterns more apparent and regression models more effective.\nTransformedTargetRegressor\n\nWraps a regression model and applies a transformation to the target values."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#iclicker-exercise-10.1",
    "href": "slides/slides-10-regression-metrics.html#iclicker-exercise-10.1",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "iClicker Exercise 10.1",
    "text": "iClicker Exercise 10.1\nSelect all of the following statements which are TRUE.\n\n\nPrice per square foot would be a good feature to add in our X.\n\n\nThe alpha hyperparameter of Ridge has similar interpretation of C hyperparameter of LogisticRegression; higher alpha means more complex model.\n\n\nIn Ridge, smaller alpha means bigger coefficients whereas bigger alpha means smaller coefficients."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#iclicker-exercise-10.2",
    "href": "slides/slides-10-regression-metrics.html#iclicker-exercise-10.2",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "iClicker Exercise 10.2",
    "text": "iClicker Exercise 10.2\nSelect all of the following statements which are TRUE.\n\n\nWe can still use precision and recall for regression problems but now we have other metrics we can use as well.\n\n\nIn sklearn for regression problems, using r2_score() and .score() (with default values) will produce the same results.\n\n\nRMSE is always going to be non-negative.\n\n\nMSE does not directly provide the information about whether the model is underpredicting or overpredicting.\n\n\nWe can pass multiple scoring metrics to GridSearchCV or RandomizedSearchCV for regression as well as classification problems."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#which-metric-fits-the-scenario",
    "href": "slides/slides-10-regression-metrics.html#which-metric-fits-the-scenario",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Which metric fits the scenario?",
    "text": "Which metric fits the scenario?\n\n\n\n\n\n\n\n\nScenario\nWhat matters most?\nBest metric(s)?\n\n\n\n\nPredicting house prices ranging from $60K‚Äì$800K.\nA $30K error is huge for a $60K house but small for a $500K house.\n\n\n\nPredicting exam scores (0‚Äì100).\nYou want an interpretable measure of average error in points.\n\n\n\nPredicting energy consumption in a large industrial system.\nLarge errors are very costly and should be penalized heavily.\n\n\n\nPredicting insurance claim amounts.\nYou want to compare how well different models explain the variation in claims."
  },
  {
    "objectID": "slides/slides-10-regression-metrics.html#which-metric-fits-the-scenario-1",
    "href": "slides/slides-10-regression-metrics.html#which-metric-fits-the-scenario-1",
    "title": "CPSC 330 Lecture 10: Regression Metrics",
    "section": "Which metric fits the scenario?",
    "text": "Which metric fits the scenario?\n\nFor interpretability: prefer RMSE or MAPE\nWhen you want to discourage large error: MSE is common\nFor fair comparison: r2 provides a normalized score similar to accuracy in classification.\n\nFor imbalanced scales: MAPE helps when proportional error matters more than absolute error."
  },
  {
    "objectID": "lecture-16.html",
    "href": "lecture-16.html",
    "title": "Lecture 16: Recommender systems",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 16: Recommender systems"
    ]
  },
  {
    "objectID": "lecture-16.html#slides",
    "href": "lecture-16.html#slides",
    "title": "Lecture 16: Recommender systems",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 16: Recommender systems"
    ]
  },
  {
    "objectID": "lecture-16.html#outline",
    "href": "lecture-16.html#outline",
    "title": "Lecture 16: Recommender systems",
    "section": "Outline",
    "text": "Outline\n\nThe problem of recommender systems\nThe utility metrics\nBaselines\nKNN Imputer\nContent-based recommendation systems\nBeyond error rate in recommendation systems",
    "crumbs": [
      "Lectures",
      "Lecture 16: Recommender systems"
    ]
  },
  {
    "objectID": "lecture-14.html",
    "href": "lecture-14.html",
    "title": "Lecture 14: K Means",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 14: K Means"
    ]
  },
  {
    "objectID": "lecture-14.html#slides",
    "href": "lecture-14.html#slides",
    "title": "Lecture 14: K Means",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 14: K Means"
    ]
  },
  {
    "objectID": "lecture-14.html#outline",
    "href": "lecture-14.html#outline",
    "title": "Lecture 14: K Means",
    "section": "Outline",
    "text": "Outline\n\nUnsupervised paradigm.\nMotivation and potential applications of clustering.\nK-Means algorithm\nPros and cons of K-Means\nThe Elbow plot and Silhouette plots for a given dataset.\nImportance of input data representation in clustering.",
    "crumbs": [
      "Lectures",
      "Lecture 14: K Means"
    ]
  },
  {
    "objectID": "lecture-12.html",
    "href": "lecture-12.html",
    "title": "Lecture 12: Feature importances",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 12: Feature importances"
    ]
  },
  {
    "objectID": "lecture-12.html#slides",
    "href": "lecture-12.html#slides",
    "title": "Lecture 12: Feature importances",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 12: Feature importances"
    ]
  },
  {
    "objectID": "lecture-12.html#outline",
    "href": "lecture-12.html#outline",
    "title": "Lecture 12: Feature importances",
    "section": "Outline",
    "text": "Outline\n\ninterpreting coefficients of linear models\nmodel transparency\nfeature_importances\nSHAP",
    "crumbs": [
      "Lectures",
      "Lecture 12: Feature importances"
    ]
  },
  {
    "objectID": "lecture-10.html",
    "href": "lecture-10.html",
    "title": "Lecture 10: Regression metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-10.html#slides",
    "href": "lecture-10.html#slides",
    "title": "Lecture 10: Regression metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-10.html#outline",
    "href": "lecture-10.html#outline",
    "title": "Lecture 10: Regression metrics",
    "section": "Outline",
    "text": "Outline\n\nRecap: Classification metrics\nRidge and RidgeCV.\nalpha hyperparameter of Ridge\nMSE, RMSE, MAPE\nApply log-transform on the target values in a regression problem with TransformedTargetRegressor",
    "crumbs": [
      "Lectures",
      "Lecture 10: Regression metrics"
    ]
  },
  {
    "objectID": "lecture-08.html",
    "href": "lecture-08.html",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-08.html#slides",
    "href": "lecture-08.html#slides",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-08.html#outline",
    "href": "lecture-08.html#outline",
    "title": "Lecture 8: Hyperparameter Optimization",
    "section": "Outline",
    "text": "Outline\n\nWhy hyperparameter optimization?\nHyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV\nOptimization bias",
    "crumbs": [
      "Lectures",
      "Lecture 8: Hyperparameter Optimization"
    ]
  },
  {
    "objectID": "lecture-06.html",
    "href": "lecture-06.html",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-06.html#slides",
    "href": "lecture-06.html#slides",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-06.html#outline",
    "href": "lecture-06.html#outline",
    "title": "Lecture 6: sklearn column transformer and text fearutres",
    "section": "Outline",
    "text": "Outline\n\nRecap\nColumn transformer\nArguments of one-hot encoder\nsklearn CountVectorizer\nincorporating text features in the pipeline",
    "crumbs": [
      "Lectures",
      "Lecture 6: `sklearn` column transformer and text fearutres"
    ]
  },
  {
    "objectID": "lecture-04.html",
    "href": "lecture-04.html",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 4: $k$-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-04.html#slides",
    "href": "lecture-04.html#slides",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 4: $k$-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-04.html#outline",
    "href": "lecture-04.html#outline",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Outline",
    "text": "Outline\n\nRecap\nSimilarity-based algorithms\n\\(k\\)-nearest neighbours intuitions\nCurse of dimensionality\nSVM with RBF kernel intuition\nInterpretation of C and gamma hyperparameters of SVM RBF",
    "crumbs": [
      "Lectures",
      "Lecture 4: $k$-nearest neighbours and SVM RBFs"
    ]
  },
  {
    "objectID": "lecture-02.html",
    "href": "lecture-02.html",
    "title": "Lecture 2: Terminology, baselines, decision Trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision Trees"
    ]
  },
  {
    "objectID": "lecture-02.html#slides",
    "href": "lecture-02.html#slides",
    "title": "Lecture 2: Terminology, baselines, decision Trees",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision Trees"
    ]
  },
  {
    "objectID": "lecture-02.html#outline",
    "href": "lecture-02.html#outline",
    "title": "Lecture 2: Terminology, baselines, decision Trees",
    "section": "Outline",
    "text": "Outline\n\nFeatures, target, examples, training\nParameters and hyperparameters\nDecision boundary\nClassification vs.¬†regression\nInference vs.¬†prediction\nAccuracy vs.¬†error, baselines\nIntuition of decision trees",
    "crumbs": [
      "Lectures",
      "Lecture 2: Terminology, baselines, decision Trees"
    ]
  },
  {
    "objectID": "slides/slides-16-recommender-systems.html#announcements",
    "href": "slides/slides-16-recommender-systems.html#announcements",
    "title": "CPSC 330 Lecture 16: Recommendation systems",
    "section": "Announcements",
    "text": "Announcements\n\nNo classes or OH during the midterm break.\nMidterm 2 coming up next week"
  },
  {
    "objectID": "slides/slides-16-recommender-systems.html#iclicker-exercise",
    "href": "slides/slides-16-recommender-systems.html#iclicker-exercise",
    "title": "CPSC 330 Lecture 16: Recommendation systems",
    "section": "iClicker Exercise",
    "text": "iClicker Exercise\nWhat percentage of watch time on YouTube do you think comes from recommendations?\n\n\n50%\n\n\n60%\n\n\n20%\n\n\n90%\n\n\nThis question is based on this source. The statistics might have changed now."
  },
  {
    "objectID": "slides/slides-16-recommender-systems.html#iclicker-exercise-16.1",
    "href": "slides/slides-16-recommender-systems.html#iclicker-exercise-16.1",
    "title": "CPSC 330 Lecture 16: Recommendation systems",
    "section": "iClicker Exercise 16.1",
    "text": "iClicker Exercise 16.1\nSelect all of the following statements which are True\n\n\nIn the context of recommendation systems, the shapes of validation utility matrix and train utility matrix are the same.\n\n\nRMSE perfectly captures what we want to measure in the context of recommendation systems.\n\n\nIt would be reasonable to impute missing values in the utility matrix by taking the average of the ratings given to an item by similar users.\n\n\n\nIn KNN type imputation, if a user has not rated any items yet, a reasonable strategy would be recommending them the most popular item."
  },
  {
    "objectID": "slides/slides-16-recommender-systems.html#iclicker-exercise-16.2",
    "href": "slides/slides-16-recommender-systems.html#iclicker-exercise-16.2",
    "title": "CPSC 330 Lecture 16: Recommendation systems",
    "section": "iClicker Exercise 16.2",
    "text": "iClicker Exercise 16.2\nSelect all of the following statements which are True\n\nIn content-based filtering we leverage available item features in addition to similarity between users.\nIn content-based filtering you represent each user in terms of known features of items.\nIn the set up of content-based filtering we discussed, if you have a new movie, you would have problems predicting ratings for that movie.\nIn content-based filtering if a user has a number of ratings in the training utility matrix but does not have any ratings in the validation utility matrix then we won‚Äôt be able to calculate RMSE for the validation utility matrix."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#announcements",
    "href": "slides/slides-13-feature-engineering-selection.html#announcements",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Announcements",
    "text": "Announcements\n\nHW5 is due next week Monday. Make use of office hours and tutorials this week."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise",
    "href": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "iClicker Exercise",
    "text": "iClicker Exercise\n\n\nIf a feature has a strong correlation with the target variable, the magnitude of its feature importance must be high.\n\n\nFeature importance tells us how the model‚Äôs predictions change when a particular feature changes.\n\n\nIn linear models, a positive coefficient indicates that as the feature increases, the prediction increases.\n\n\nIn tree-based models such as Random Forests or Gradient Boosted Trees, feature importances have signs that tell whether the relationship is positive or negative.\n\n\nIn permutation importance, if shuffling the education feature causes a large drop in the model‚Äôs performance, it indicates that the model heavily relies on that feature for its predictions."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#why-bother-about-model-transparancey",
    "href": "slides/slides-13-feature-engineering-selection.html#why-bother-about-model-transparancey",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Why bother about model transparancey?",
    "text": "Why bother about model transparancey?"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#shap-intuition",
    "href": "slides/slides-13-feature-engineering-selection.html#shap-intuition",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "SHAP intuition",
    "text": "SHAP intuition\n\nThink of the model as a ‚Äúblack box‚Äù that outputs predictions.\nSHAP asks: If we treat each feature as a player contributing to the final prediction, how much credit does each one deserve?\nTo answer this fairly, SHAP looks at all possible combinations of features and averages their marginal contributions.\nA marginal contribution is how much the prediction changes when you add that feature to a subset of other features."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#shap",
    "href": "slides/slides-13-feature-engineering-selection.html#shap",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "SHAP",
    "text": "SHAP"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-shap",
    "href": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-shap",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "iClicker Exercise SHAP",
    "text": "iClicker Exercise SHAP\n\n\nSHAP values are model parameters learned during training.\n\n\nCoefficients in a linear model and SHAP values both quantify how much each feature contributes to a prediction, but coefficients are global while SHAP values are local.\n\n\nSHAP values can only be computed for tree-based models.\n\n\nA waterfall plot shows how each feature‚Äôs SHAP value cumulatively contributes to a single prediction.\n\n\nSHAP provides the same explanation for all examples in the dataset."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-13.0",
    "href": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-13.0",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "iClicker Exercise 13.0",
    "text": "iClicker Exercise 13.0\nSuppose you are working on a machine learning project. If you have to prioritize one of the following in your project which of the following would it be?\n\n\nThe quality and size of the data\n\n\nMost recent deep neural network model\n\n\nMost recent optimization algorithm"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#discussion-question",
    "href": "slides/slides-13-feature-engineering-selection.html#discussion-question",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Discussion question",
    "text": "Discussion question\n\nSuppose we want to predict whether a flight will arrive on time or be delayed. We have a dataset with the following information about flights:\n\nDeparture Time\nExpected Duration of Flight (in minutes)\n\n\nUpon analyzing the data, you notice a pattern: flights tend to be delayed more often during the evening rush hours. What feature could be valuable to add for this prediction task?"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#garbage-in-garbage-out.",
    "href": "slides/slides-13-feature-engineering-selection.html#garbage-in-garbage-out.",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Garbage in, garbage out.",
    "text": "Garbage in, garbage out.\n\nModel building is interesting. But in your machine learning projects, you‚Äôll be spending more than half of your time on data preparation, feature engineering, and transformations.\nThe quality of the data is important. Your model is only as good as your data."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#activity-measuring-quality-of-the-data",
    "href": "slides/slides-13-feature-engineering-selection.html#activity-measuring-quality-of-the-data",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Activity: Measuring quality of the data",
    "text": "Activity: Measuring quality of the data\n\nDiscuss some attributes of good- and bad-quality data"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#what-is-feature-engineering",
    "href": "slides/slides-13-feature-engineering-selection.html#what-is-feature-engineering",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\n\nBetter features: more flexibility, higher score, we can get by with simple and more interpretable models.\nIf your features, i.e., representation is bad, whatever fancier model you build is not going to help.\n\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. - Jason Brownlee"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#some-quotes-on-feature-engineering",
    "href": "slides/slides-13-feature-engineering-selection.html#some-quotes-on-feature-engineering",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Some quotes on feature engineering",
    "text": "Some quotes on feature engineering\nA quote by Pedro Domingos A Few Useful Things to Know About Machine Learning\n\n‚Ä¶ At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#some-quotes-on-feature-engineering-1",
    "href": "slides/slides-13-feature-engineering-selection.html#some-quotes-on-feature-engineering-1",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Some quotes on feature engineering",
    "text": "Some quotes on feature engineering\nA quote by Andrew Ng, Machine Learning and AI via Brain simulations\n\nComing up with features is difficult, time-consuming, requires expert knowledge. ‚ÄúApplied machine learning‚Äù is basically feature engineering."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#better-features-usually-help-more-than-a-better-model",
    "href": "slides/slides-13-feature-engineering-selection.html#better-features-usually-help-more-than-a-better-model",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Better features usually help more than a better model",
    "text": "Better features usually help more than a better model\n\nGood features would ideally:\n\ncapture most important aspects of the problem\nallow learning with few examples\ngeneralize to new scenarios.\n\nThere is a trade-off between simple and expressive features:\n\nWith simple features overfitting risk is low, but scores might be low.\nWith complicated features scores can be high, but so is overfitting risk."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#the-best-features-may-be-dependent-on-the-model-you-use",
    "href": "slides/slides-13-feature-engineering-selection.html#the-best-features-may-be-dependent-on-the-model-you-use",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "The best features may be dependent on the model you use",
    "text": "The best features may be dependent on the model you use\n\nExamples:\n\nFor counting-based methods like decision trees separate relevant groups of variable values\n\nDiscretization makes sense\n\nFor distance-based methods like KNN, we want different class labels to be ‚Äúfar‚Äù.\n\nStandardization\n\nFor regression-based methods like linear regression, we want targets to have a linear dependency on features."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#domain-specific-transformations",
    "href": "slides/slides-13-feature-engineering-selection.html#domain-specific-transformations",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Domain-specific transformations",
    "text": "Domain-specific transformations\nIn some domains there are natural transformations to do:\n\nSpectrograms (sound data)\nConvolutions (image data)\n\n\nSource"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#feature-selection-pipeline",
    "href": "slides/slides-13-feature-engineering-selection.html#feature-selection-pipeline",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Feature selection pipeline",
    "text": "Feature selection pipeline"
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#two-useful-methods",
    "href": "slides/slides-13-feature-engineering-selection.html#two-useful-methods",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "Two useful methods",
    "text": "Two useful methods\n\nModel-based selection\n\nUse a supervised machine learning model to judge the importance of each feature.\nKeep only the most important once.\n\nRecursive feature elimination\n\nBuild a series of models\nAt each iteration, discard the least important feature according to the model."
  },
  {
    "objectID": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-13.2",
    "href": "slides/slides-13-feature-engineering-selection.html#iclicker-exercise-13.2",
    "title": "CPSC 330 Lecture 13: Model Transparency, Feature Engineering and Selection",
    "section": "(iClicker) Exercise 13.2",
    "text": "(iClicker) Exercise 13.2\nSelect all of the following statements which are TRUE.\n\n\nYou can carry out feature selection using linear models by pruning the features which have very small weights (i.e., coefficients less than a threshold).\n\n\nThe order of features removed given by rfe.ranking_ is the same as the order of original feature importances given by the model."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#focus-on-the-breath",
    "href": "slides/slides-11-ensembles.html#focus-on-the-breath",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Focus on the breath!",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#announcements",
    "href": "slides/slides-11-ensembles.html#announcements",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Announcements",
    "text": "Announcements\n\nMidterm 1 window starts tomorrow!!\nMidterm OH: Tuesday, Oct 14th 2pm to 3pm ORCH 4018\nDuring midterm:\n\nPiazza will be turned off.\nNo tutorials and office hours."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#iclicker-exercise-11.0",
    "href": "slides/slides-11-ensembles.html#iclicker-exercise-11.0",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "iClicker Exercise 11.0",
    "text": "iClicker Exercise 11.0\nWhich of the following scenarios has worked effectively for you in the past?\n\n\nWorking independently on a project/assignment.\n\n\nWorking with like-minded people.\n\n\nTeaming up with a diverse group offering varied perspectives and skills."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#the-wisdom-of-crowds",
    "href": "slides/slides-11-ensembles.html#the-wisdom-of-crowds",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "The Wisdom of Crowds",
    "text": "The Wisdom of Crowds\nGroups can often make better decisions than individuals, especially when group members are diverse enough."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#ensembles-key-idea",
    "href": "slides/slides-11-ensembles.html#ensembles-key-idea",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Ensembles key idea",
    "text": "Ensembles key idea\n\n\n\nSource\n\n\nCombine predictions from multiple models for a more accurate and stable result\nClassification: either take majority class or average predicted probabilities\nRegression: Average (mean or median) predictions\nAveraging reduces individual model errors, making the ensemble more robust than any single model"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#ensembles",
    "href": "slides/slides-11-ensembles.html#ensembles",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Ensembles",
    "text": "Ensembles\n\nEnsemble methods are widely used in industry and dominate machine learning competitions (e.g., Kaggle).\n\nIn this course, we‚Äôll explore\n\nTree-based ensembles:\n\nRandom Forests\n\nGradient Boosted Trees (at a high level)\n\nWays to combine models:\n\nAveraging: combine model outputs directly\n\nStacking: use one model to learn how to combine others"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#when-do-ensembles-help",
    "href": "slides/slides-11-ensembles.html#when-do-ensembles-help",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "When do ensembles help?",
    "text": "When do ensembles help?\nIn which of these scenarios does an ensemble improve performance?\n\n\n\n\n\nSource"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#when-do-ensembles-help-1",
    "href": "slides/slides-11-ensembles.html#when-do-ensembles-help-1",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "When do ensembles help?",
    "text": "When do ensembles help?\nEnsembles improve performance when the individual models are both competent (have expertise) and make different kinds of errors (are diverse)."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#diversity",
    "href": "slides/slides-11-ensembles.html#diversity",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Diversity",
    "text": "Diversity\n\nEnsembles work best when their models make different types of mistakes.\n\nIf all models make the same errors, averaging won‚Äôt help.\n\nWe can encourage diversity by:\n\nIntroducing randomness (e.g., bootstrapped samples and random feature subsets (Random Forests))\nSequentially focusing on previous errors (e.g., Gradient Boosting)"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#expertise",
    "href": "slides/slides-11-ensembles.html#expertise",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Expertise",
    "text": "Expertise\n\nDiversity alone is not enough. Individual models should still have some predictive skill.\n\nIf each model performs poorly (worse than random), the ensemble can‚Äôt recover.\n\nBest ensembles strike a balance between:\n\nDiversity: models disagree in useful ways\n\nExpertise: each model performs better than chance"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#what-can-we-randomize-between-trees",
    "href": "slides/slides-11-ensembles.html#what-can-we-randomize-between-trees",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "What can we randomize between trees?",
    "text": "What can we randomize between trees?"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#two-sources-of-randomness-in-random-forests",
    "href": "slides/slides-11-ensembles.html#two-sources-of-randomness-in-random-forests",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Two sources of randomness in random forests",
    "text": "Two sources of randomness in random forests\n\nBootstrap sampling of data\nEach tree is trained on a slightly different dataset ‚Üí decorrelates trees.\nRandom subset of features at each split\nEach split considers only a random subset of features ‚Üí further reduces correlation."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#bootstrap-sample",
    "href": "slides/slides-11-ensembles.html#bootstrap-sample",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Bootstrap sample",
    "text": "Bootstrap sample\n\nEnsures trees see different versions of the data\n\nCreates diversity while preserving similar overall structure\n\n\n\n\nSource"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#random-subset-of-features-at-each-split",
    "href": "slides/slides-11-ensembles.html#random-subset-of-features-at-each-split",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Random subset of features at each split",
    "text": "Random subset of features at each split\n\n\n\nSource\n\n\nSelect a different subset of features at every split\nEach tree now explores different parts of the feature space\n\nü§î But why not pick one subset per tree instead?"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#why-use-a-new-subset-at-each-split",
    "href": "slides/slides-11-ensembles.html#why-use-a-new-subset-at-each-split",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Why use a new subset at each split?",
    "text": "Why use a new subset at each split?\n\n\n\n\n\n\n\n\n\nStrategy\nDiversity\nExpertise\nIssue\n\n\n\n\nOne subset per tree\nVery high\nOften low\nMay miss important features\n\n\nNew subset per split\nModerate\nHigh\nStronger, less correlated trees\n\n\n\n\nUsing a new subset per split strikes a balance between diversity and strength\n\nPrevents trees from becoming too weak while still keeping them different\n\n\n‚ÄúRandom selection of features at each node gives substantial additional accuracy gains over bagging alone.‚Äù\n‚Äî Breiman (2001)"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#interim-summary",
    "href": "slides/slides-11-ensembles.html#interim-summary",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Interim summary",
    "text": "Interim summary\n\nRandom Forests reduce overfitting by decorrelating trees\nTwo ingredients make this happen: Random data (bootstrapping) and random features (per split)\n\nTogether, they build a strong yet diverse ensemble"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#iclicker-exercise-11.1",
    "href": "slides/slides-11-ensembles.html#iclicker-exercise-11.1",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "iClicker Exercise 11.1",
    "text": "iClicker Exercise 11.1\nSelect the most accurate option below.\n\n\nEvery tree in a random forest uses a different bootstrap sample of the training set.\n\n\nTo train a tree in a random forest, we first randomly select a subset of features. The tree is then restricted to only using those features.\n\n\nThe n_estimators hyperparameter of random forests should be tuned to get a better performance on the validation or test data.\n\n\n\nIn random forests we build trees in a sequential fashion, where the current tree is dependent upon the previous tree.\n\n\n\nLet classifiers A, B, and C have training errors of 10%, 20%, and 30%, respectively. Then, the best possible training error from averaging A, B and C is 10%."
  },
  {
    "objectID": "slides/slides-11-ensembles.html#from-random-forests-to-boosting",
    "href": "slides/slides-11-ensembles.html#from-random-forests-to-boosting",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "From random forests to boosting",
    "text": "From random forests to boosting\n\nIn random forests, we build trees independently and average their predictions\n\nreduces overfitting by combining diverse models\n\n\nIn boosting, we build trees sequentially\n\neach new tree learns from the mistakes of the previous ones\n\nreduces underfitting by making the model smarter over time"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#why-it-works",
    "href": "slides/slides-11-ensembles.html#why-it-works",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Why it works",
    "text": "Why it works\n\nSequential correction: each tree improves on what‚Äôs left of the error\n\nBias reduction: starts simple and adds complexity gradually\n\nControlled learning: small trees and a learning rate prevent overfitting"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#most-commonly-used-boosting-models",
    "href": "slides/slides-11-ensembles.html#most-commonly-used-boosting-models",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Most commonly used boosting models",
    "text": "Most commonly used boosting models\n\nXGBoost: extremely optimized and widely adopted in industry\n\nLightGBM: faster training with large datasets\n\nCatBoost: handles categorical features efficiently\n\n\n‚ÄúMore recently, gradient boosting machines (GBMs) have become a Swiss army knife in many a Kaggler‚Äôs toolbelt.‚Äù\n‚Äî Sebastian Raschka, Joshua Patterson, & Corey Nolet (2020)"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#averaging-voting",
    "href": "slides/slides-11-ensembles.html#averaging-voting",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Averaging (Voting)",
    "text": "Averaging (Voting)\n\nCombines predictions from multiple base models\nWorks for both classification and regression\nImproves stability and performance by averaging over models\nRandom forest is a type of averaging classifier"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#types-of-voting",
    "href": "slides/slides-11-ensembles.html#types-of-voting",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Types of Voting",
    "text": "Types of Voting\n\n\nHard Voting\n\nEach model votes for a class label\n\nFinal prediction = majority vote\n\n\nSoft Voting\n\nAverages the predicted probabilities from each model\n\nFinal prediction = class with highest average probability\n\nUsually performs better"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#stacking",
    "href": "slides/slides-11-ensembles.html#stacking",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Stacking",
    "text": "Stacking\n\nGoes one step beyond averaging\nInstead of simple averaging, trains a meta-model to learn how to best combine base models‚Äô predictions"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#how-stacking-works",
    "href": "slides/slides-11-ensembles.html#how-stacking-works",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "How stacking works",
    "text": "How stacking works\n\nTrain several base models on the training data\nCollect their predictions on a validation set\nTrain a meta-model on those predictions to learn the optimal combination\nFinal prediction = output of the meta-model"
  },
  {
    "objectID": "slides/slides-11-ensembles.html#stacking-1",
    "href": "slides/slides-11-ensembles.html#stacking-1",
    "title": "CPSC 330 Lecture 11: Ensembles",
    "section": "Stacking",
    "text": "Stacking\n\n\n\nSource"
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#announcements",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#announcements",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Announcements",
    "text": "Announcements\n\nhw2 is due tonight.\nSyllabus quiz due date is September 19th, 11:59 pm.\nHomework 3 (hw3) has been released (Due: Sept 29th, 11:59 pm)\n\nYou can work in pairs for this assignment.\n\nIf you were on the waitlist, you should now know your course enrollment status.\nThe lecture notes here align with the content presented in the videos. Even though we do not cover all the content from these notebooks during lectures, it‚Äôs your responsibility to go through them on your own."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-overfitting-1",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-overfitting-1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: iClicker overfitting 1",
    "text": "Recap: iClicker overfitting 1\nWhich of the following scenarios do NOT necessarily imply overfitting?\n\n\nTraining accuracy is very high (0.98) while validation accuracy is much lower (0.60).\n\n\nIn a wildlife classifier, the model predicts ‚Äúwolf‚Äù whenever there‚Äùs snow in the background, because all wolf photos were taken in snowy regions.\n\n\nThe decision boundary of a classifier is wiggly and highly irregular.\n\n\nTraining and validation accuracies are both approximately 0.88.\n\n\nA cancer detection model learns that ‚Äúa ruler in the corner of the X-ray‚Äù means positive, because doctors tended to measure suspicious cases."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-overfitting-2",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-overfitting-2",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: iClicker overfitting 2",
    "text": "Recap: iClicker overfitting 2\nWhich of the following statements about overfitting is true?\n\n\nOverfitting makes the model more accurate on both training and unseen data.\n\n\nOverfitting means the model captures noise or irrelevant details from the training data.\n\n\nOverfitting is desirable because it reduces both training and test error.\n\n\nIn real-world problems, models are always at risk of overfitting if not properly validated."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-underfitting",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#recap-iclicker-underfitting",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: iClicker underfitting",
    "text": "Recap: iClicker underfitting\nHow might one address the issue of underfitting in a machine learning model.\n\n\nIntroduce more noise to the training data.\n\n\nRemove features that might be relevant to the prediction.\n\n\nIncrease the model‚Äôs complexity (e.g., more parameters, features, or deeper trees)\n\n\nUse a smaller dataset for training."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#the-fundamental-tradeoff",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#the-fundamental-tradeoff",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "The fundamental tradeoff",
    "text": "The fundamental tradeoff\n\n\n\n\n\n\n\n\n\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.\n\nHow to pick a model?"
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#iclicker-4.1",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#iclicker-4.1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "iClicker 4.1",
    "text": "iClicker 4.1\nSelect all of the following statements which are TRUE.\n\n\nAnalogy-based models find examples from the test set that are most similar to the query example we are predicting.\n\n\nEuclidean distance will always have a non-negative value.\n\n\nWith \\(k\\)-NN, setting the hyperparameter \\(k\\) to larger values typically reduces training error.\n\n\nSimilar to decision trees, \\(k\\)-NNs finds a small set of good features.\n\n\nIn \\(k\\)-NN, with \\(k &gt; 1\\), the classification of the closest neighbour to the test example always contributes the most to the prediction."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#iclicker-4.2",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#iclicker-4.2",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "iClicker 4.2",
    "text": "iClicker 4.2\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-NN may perform poorly in high-dimensional space (say, d &gt; 1000).\n\n\nIn sklearn‚Äôs SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score.\n\n\nIf we increase both gamma and C, we can‚Äôt be certain if the model becomes more complex or less complex."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#similarity-based-algorithms",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#similarity-based-algorithms",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Similarity-based algorithms",
    "text": "Similarity-based algorithms\n\nUse similarity or distance metrics to predict targets.\nExamples: \\(k\\)-nearest neighbors, Support Vector Machines (SVMs) with RBF Kernel."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#k-nearest-neighbours",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#k-nearest-neighbours",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-nearest neighbours",
    "text": "\\(k\\)-nearest neighbours\n\nClassifies an object based on the majority label among its \\(k\\) closest neighbors.\nMain hyperparameter: \\(k\\) or n_neighbors in sklearn\nDistance Metrics: Euclidean\nStrengths: simple and intuitive, can learn complex decision boundaries\nChallenges: Sensitive to the choice of distance metric and scaling (coming up)."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#curse-of-dimensionality",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#curse-of-dimensionality",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\n\nAs dimensionality increases, the volume of the space increases exponentially, making the data sparse.\nDistance metrics lose meaning\n\nAccidental similarity swamps out meaningful similarity\nAll points become almost equidistant.\n\nOverfitting becomes likely: Harder to generalize with high-dimensional data.\nHow to deal with this?\n\nDimensionality reduction (PCA) (not covered in this course)\nFeature selection techniques."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#svms-with-rbf-kernel",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#svms-with-rbf-kernel",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "SVMs with RBF kernel",
    "text": "SVMs with RBF kernel\n\nRBF Kernel: Radial Basis Function, a way to transform data into higher dimensions implicitly.\nStrengths\n\nEffective in high-dimensional and sparse data\nGood performance on non-linear problems.\n\nHyperparameters:\n\n\\(C\\): Regularization parameter (trade-off between correct classification of training examples and maximization of the decision margin).\ngamma (\\(\\gamma\\)): controls how fast the similarity decays with distance"
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#intuition-of-c-and-gamma-in-svm-rbf",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#intuition-of-c-and-gamma-in-svm-rbf",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Intuition of C and gamma in SVM RBF",
    "text": "Intuition of C and gamma in SVM RBF\n\nC (Regularization): Controls the trade-off between perfect training accuracy and having a simpler decision boundary.\n\nHigh C: Strict, complex boundary (overfitting risk).\nLow C: More errors allowed, smoother boundary (generalizes better).\n\nGamma (Kernel Width): Controls the influence of individual data points.\n\nHigh Gamma: Points have local impact, complex boundary.\nLow Gamma: Points affect broader areas, smoother boundary.\n\nKey trade-off: Proper balance between C and gamma is crucial for avoiding overfitting or underfitting."
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#supervised-models-we-have-seen",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#supervised-models-we-have-seen",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Supervised models we have seen",
    "text": "Supervised models we have seen\n\nDecision trees: Split data into subsets based on feature values to create decision rules\n\\(k\\)-NNs: Classify based on the majority vote from \\(k\\) nearest neighbors\nSVM RBFs: Create a boundary using an RBF kernel to separate classes"
  },
  {
    "objectID": "slides/slides-04-kNNs-SVM-RBF.html#comparison-of-models-activity",
    "href": "slides/slides-04-kNNs-SVM-RBF.html#comparison-of-models-activity",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Comparison of models (activity)",
    "text": "Comparison of models (activity)\n\n\n\n\n\n\n\n\n\nModel\nParameters and hyperparameters\nStrengths\nWeaknesses\n\n\n\n\nDecision Trees\n\n\n\n\n\nKNNs\n\n\n\n\n\nSVM RBF"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to CPSC330! Here, you‚Äôll find slides for CPSC 330 Section 102. These slides are based on the notes present here.\n\nClass times üïò 11 am to 12:20 pm\nWhere? üìç HUGH DEMPSTER PAVILION (DMP) 310, 6245 Agronomy Rd, Vancouver, BC V6T 1Z4"
  },
  {
    "objectID": "lecture.html",
    "href": "lecture.html",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision Trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs.¬†regression, inference vs.¬†prediction, accuracy vs.¬†error, baselines, intuition of decision trees\n\n\n\n\nLecture 3: ML fundamentals\n\n\nGeneralization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule\n\n\n\n\nLecture 4: \\(k\\)-nearest neighbours and SVM RBFs\n\n\nIntroduction to KNNs, hyperparameter n_neighbours or \\(k\\), C and gamma hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.\n\n\n\n\nLecture 5: Preprocessing and sklearn pipelines\n\n\nPreprocessing motivation, Common transformations in sklearn, sklearn transformers vs.¬†Estimators, The golden rule in the feature transformations, sklearn pipelines\n\n\n\n\nLecture 6: sklearn column transformer and text fearutres\n\n\nColumn transformer, arguments of OHE, encoding text features, incorporating text features in an ML pipeline\n\n\n\n\nLecture 7: Linear models\n\n\nIntuition behind linear models, linear regression and logistic regression, scikit-learn‚Äôs Ridge model, prediction probabilities, interpret model predictions using coefficients learned by a linear model, parametric vs.¬†non-parametric models\n\n\n\n\nLecture 8: Hyperparameter Optimization\n\n\nmotivation for hyperparameter optimization, hyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV, optimization bias\n\n\n\n\nLecture 9: Classification metrics\n\n\nconfusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance\n\n\n\n\nLecture 10: Regression metrics\n\n\nClassification metrics recap, ridge and RidgeCV, alpha hyperparameter of ridge, MSE, RMSE, MAPE, log transformations on the target\n\n\n\n\nLecture 11: Ensembles\n\n\n¬†\n\n\n\n\nLecture 12: Feature importances\n\n\n¬†\n\n\n\n\nLecture 13: Feature engineering and selection\n\n\nMotivation for feature engineering, preliminary feature engineering on text data, general concept of feature selection, model-based feature selection vs.¬†RFE\n\n\n\n\nLecture 14: K Means\n\n\nUnsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset, importance of input data representation in clustering.\n\n\n\n\nLecture 15: DBSCAN and hierarchical\n\n\nLimitations of K-Means, DBSCAN, ierarchical clustering, dendrograms, omparing and contrasting different clustering methods\n\n\n\n\nLecture 16: Recommender systems\n\n\n¬†\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "lecture.html#schedule",
    "href": "lecture.html#schedule",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\nLecture 1: Course introduction\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\nLecture 2: Terminology, baselines, decision Trees\n\n\nSupervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs.¬†regression, inference vs.¬†prediction, accuracy vs.¬†error, baselines, intuition of decision trees\n\n\n\n\nLecture 3: ML fundamentals\n\n\nGeneralization, data splitting, cross-validation, overfitting, underfitting, the fundamental tradeoff, the golden rule\n\n\n\n\nLecture 4: \\(k\\)-nearest neighbours and SVM RBFs\n\n\nIntroduction to KNNs, hyperparameter n_neighbours or \\(k\\), C and gamma hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.\n\n\n\n\nLecture 5: Preprocessing and sklearn pipelines\n\n\nPreprocessing motivation, Common transformations in sklearn, sklearn transformers vs.¬†Estimators, The golden rule in the feature transformations, sklearn pipelines\n\n\n\n\nLecture 6: sklearn column transformer and text fearutres\n\n\nColumn transformer, arguments of OHE, encoding text features, incorporating text features in an ML pipeline\n\n\n\n\nLecture 7: Linear models\n\n\nIntuition behind linear models, linear regression and logistic regression, scikit-learn‚Äôs Ridge model, prediction probabilities, interpret model predictions using coefficients learned by a linear model, parametric vs.¬†non-parametric models\n\n\n\n\nLecture 8: Hyperparameter Optimization\n\n\nmotivation for hyperparameter optimization, hyperparameter optimization using sklearn‚Äôs GridSearchCV and RandomizedSearchCV, optimization bias\n\n\n\n\nLecture 9: Classification metrics\n\n\nconfusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance\n\n\n\n\nLecture 10: Regression metrics\n\n\nClassification metrics recap, ridge and RidgeCV, alpha hyperparameter of ridge, MSE, RMSE, MAPE, log transformations on the target\n\n\n\n\nLecture 11: Ensembles\n\n\n¬†\n\n\n\n\nLecture 12: Feature importances\n\n\n¬†\n\n\n\n\nLecture 13: Feature engineering and selection\n\n\nMotivation for feature engineering, preliminary feature engineering on text data, general concept of feature selection, model-based feature selection vs.¬†RFE\n\n\n\n\nLecture 14: K Means\n\n\nUnsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset, importance of input data representation in clustering.\n\n\n\n\nLecture 15: DBSCAN and hierarchical\n\n\nLimitations of K-Means, DBSCAN, ierarchical clustering, dendrograms, omparing and contrasting different clustering methods\n\n\n\n\nLecture 16: Recommender systems\n\n\n¬†\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Lectures",
      "Overview"
    ]
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#announcements",
    "href": "slides/slides-05-preprocessing-pipelines.html#announcements",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Announcements",
    "text": "Announcements\n\nHW1 grades will be posted soon.\nSyllabus quiz due date is September 19th, 11:59 pm.\nHomework 3 (hw3) has been released (Due: Sept 29th, 11:59 pm)\n\nYou can work in pairs for this assignment."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#recap",
    "href": "slides/slides-05-preprocessing-pipelines.html#recap",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Recap",
    "text": "Recap\n\nDecision trees: Split data into subsets based on feature values to create decision rules\n\\(k\\)-NNs: Classify based on the majority vote from \\(k\\) nearest neighbors\nSVM RBFs: Create a boundary using an RBF kernel to separate classes"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#iclicker-4.2",
    "href": "slides/slides-05-preprocessing-pipelines.html#iclicker-4.2",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "iClicker 4.2",
    "text": "iClicker 4.2\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-NN may perform poorly in high-dimensional space (say, d &gt; 1000).\n\n\nIn sklearn‚Äôs SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score.\n\n\nIf we increase both gamma and C, we can‚Äôt be certain if the model becomes more complex or less complex."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#comparison-of-models-activity",
    "href": "slides/slides-05-preprocessing-pipelines.html#comparison-of-models-activity",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Comparison of models (activity)",
    "text": "Comparison of models (activity)\n\n\n\n\n\n\n\n\n\nModel\nParameters and hyperparameters\nStrengths\nWeaknesses\n\n\n\n\nDecision Trees\n\n\n\n\n\nKNNs\n\n\n\n\n\nSVM RBF"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#preprocessing-motivation-example",
    "href": "slides/slides-05-preprocessing-pipelines.html#preprocessing-motivation-example",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Preprocessing motivation: example",
    "text": "Preprocessing motivation: example\nYou‚Äôre trying to find a suitable date based on:\n\nAge (closer to yours is better).\nNumber of Facebook Friends (closer to your social circle is ideal)."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#preprocessing-motivation-example-1",
    "href": "slides/slides-05-preprocessing-pipelines.html#preprocessing-motivation-example-1",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Preprocessing motivation: example",
    "text": "Preprocessing motivation: example\n\nYou are 30 years old and have 250 Facebook friends.\n\n\n\n\n\n\n\n\n\n\n\nPerson\nAge\n#FB Friends\nEuclidean Distance Calculation\nDistance\n\n\n\n\nA\n25\n400\n\\(\\sqrt{5^2 + 150^2}\\)\n150.08\n\n\nB\n27\n300\n\\(\\sqrt{3^2 + 50^2}\\)\n50.09\n\n\nC\n30\n500\n\\(\\sqrt{0^2 + 250^2}\\)\n250.00\n\n\nD\n60\n250\n\\(\\sqrt{30^2 + 0^2}\\)\n30.00\n\n\n\nBased on the distances, the two nearest neighbors (2-NN) are:\n\nPerson D (Distance: 30.00)\nPerson B (Distance: 50.09)\n\nWhat‚Äôs the problem here?"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#what-is-preprocessing",
    "href": "slides/slides-05-preprocessing-pipelines.html#what-is-preprocessing",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "What is preprocessing?",
    "text": "What is preprocessing?\n\nPreprocessing is about making the raw dataset ready for machine learning."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.1",
    "href": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.1",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.1",
    "text": "(iClicker) Exercise 5.1\nTake a guess: In your machine learning project, how much time will you typically spend on data preparation and transformation?\n\n\n~80% of the project time\n\n\n~20% of the project time\n\n\n~50% of the project time\n\n\nNone. Most of the time will be spent on model building\n\n\nThe question is adapted from here."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#class-demo",
    "href": "slides/slides-05-preprocessing-pipelines.html#class-demo",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Class demo",
    "text": "Class demo\n\nLet‚Äôs walk through strategies for handling missing values, categorical variables, text, scaling, and irrelevant features using a class demo."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.2",
    "href": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.2",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.2",
    "text": "(iClicker) Exercise 5.2\nSelect all of the following statements which are TRUE.\n\n\nStandardScaler ensures a fixed range (i.e., minimum and maximum values) for the features.\n\n\nStandardScaler calculates mean and standard deviation for each feature separately.\n\n\nIn general, it‚Äôs a good idea to apply scaling on numeric features before training \\(k\\)-NN or SVM RBF models.\n\n\nThe transformed feature values might be hard to interpret for humans.\n\n\nAfter applying SimpleImputer The transformed data has a different shape than the original data."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.3",
    "href": "slides/slides-05-preprocessing-pipelines.html#iclicker-exercise-5.3",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "(iClicker) Exercise 5.3",
    "text": "(iClicker) Exercise 5.3\nSelect all of the following statements which are TRUE.\n\n\nYou can have scaling of numeric features, one-hot encoding of categorical features, and scikit-learn estimator within a single pipeline.\n\n\nOnce you have a scikit-learn pipeline object with an estimator as the last step, you can call fit, predict, and score on it.\n\n\nYou can carry out data splitting within scikit-learn pipeline.\n\n\nWe have to be careful of the order we put each transformation and model in a pipeline."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#imputation-fill-the-gaps",
    "href": "slides/slides-05-preprocessing-pipelines.html#imputation-fill-the-gaps",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Imputation: Fill the gaps! (üü© üüß üü¶)",
    "text": "Imputation: Fill the gaps! (üü© üüß üü¶)\nFill in missing data using a chosen strategy:\n\nMean: Replace missing values with the average of the available data.\nMedian: Use the middle value.\nMost Frequent: Use the most common value (mode).\nKNN Imputation: Fill based on similar neighbors.\n\nExample:\nImputation is like filling in your average or median or most frequent grade for an assessment you missed.\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#scaling-everything-to-the-same-range",
    "href": "slides/slides-05-preprocessing-pipelines.html#scaling-everything-to-the-same-range",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Scaling: Everything to the same range! (üìâ üìà)",
    "text": "Scaling: Everything to the same range! (üìâ üìà)\nEnsure all features have a comparable range.\n\nStandardScaler: Mean = 0, Standard Deviation = 1.\n\nExample:\nScaling is like adjusting the number of everyone‚Äôs Facebook friends so that both the number of friends and their age are on a comparable scale. This way, one feature doesn‚Äôt dominate the other when making comparisons.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#one-hot-encoding-1-0-0",
    "href": "slides/slides-05-preprocessing-pipelines.html#one-hot-encoding-1-0-0",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "One-Hot encoding: üçé ‚Üí 1Ô∏è‚É£ 0Ô∏è‚É£ 0Ô∏è‚É£",
    "text": "One-Hot encoding: üçé ‚Üí 1Ô∏è‚É£ 0Ô∏è‚É£ 0Ô∏è‚É£\nConvert categorical features into binary columns.\n\nCreates new binary columns for each category.\nUseful for handling categorical data in machine learning models.\n\nExample:\nTurn ‚ÄúApple, Banana, Orange‚Äù into binary columns:\n\n\n\nFruit\nüçé\nüçå\nüçä\n\n\n\n\nApple üçé\n1\n0\n0\n\n\nBanana üçå\n0\n1\n0\n\n\nOrange üçä\n0\n0\n1\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nX_encoded = encoder.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#ordinal-encoding-ranking-matters-3",
    "href": "slides/slides-05-preprocessing-pipelines.html#ordinal-encoding-ranking-matters-3",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Ordinal encoding: Ranking matters! (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ‚Üí 3Ô∏è‚É£)",
    "text": "Ordinal encoding: Ranking matters! (‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è ‚Üí 3Ô∏è‚É£)\nConvert categories into integer values that have a meaningful order.\n\nAssign integers based on order or rank.\nUseful when there is an inherent ranking in the data.\n\nExample:\nTurn ‚ÄúPoor, Average, Good‚Äù into 1, 2, 3:\n\n\n\nRating\nOrdinal\n\n\n\n\nPoor\n1\n\n\nAverage\n2\n\n\nGood\n3\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nX_ordinal = encoder.fit_transform(X)"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#transformers",
    "href": "slides/slides-05-preprocessing-pipelines.html#transformers",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Transformers",
    "text": "Transformers\n\nAre used to transform or preprocess data.\nImplement the fit and transform methods.\n\nfit(X): Learns parameters from the data.\ntransform(X): Applies the learned transformation to the data.\n\nExamples:\n\nImputation (SimpleImputer): Fills missing values.\nScaling (StandardScaler): Standardizes features."
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#estimators",
    "href": "slides/slides-05-preprocessing-pipelines.html#estimators",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Estimators",
    "text": "Estimators\n\nUsed to make predictions.\nImplement fit and predict methods.\n\nfit(X, y): Learns from labeled data.\npredict(X): Makes predictions on new data.\n\nExamples: DecisionTreeClassifier, SVC, KNeighborsClassifier"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#the-golden-rule-in-feature-transformations",
    "href": "slides/slides-05-preprocessing-pipelines.html#the-golden-rule-in-feature-transformations",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "The golden rule in feature transformations",
    "text": "The golden rule in feature transformations\n\nNever transform the entire dataset at once!\nWhy? It leads to data leakage ‚Äî using information from the test set in your training process, which can artificially inflate model performance.\nFit transformers like scalers and imputers on the training set only.\nApply the transformations to both the training and test sets separately.\n\nExample:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "slides/slides-05-preprocessing-pipelines.html#sklearn-pipelines",
    "href": "slides/slides-05-preprocessing-pipelines.html#sklearn-pipelines",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "sklearn Pipelines",
    "text": "sklearn Pipelines\n\nPipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\nSimplify the code and improves readability.\nReduce the risk of data leakage by ensuring proper transformation of the training and test sets.\nAutomatically apply transformations in sequence.\n\nExample:\nChaining a StandardScaler with a KNeighborsClassifier model.\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#focus-on-the-breath",
    "href": "slides/slides-12-feature-importances.html#focus-on-the-breath",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Focus on the breath!",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#announcements",
    "href": "slides/slides-12-feature-importances.html#announcements",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Announcements",
    "text": "Announcements\n\nHW4 grades are released\nHW5 is due next week Monday. Make use of office hours and tutorials this week.\n\nMidterm grading in progress. We should be able to return the score later this week."
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#iclicker",
    "href": "slides/slides-12-feature-importances.html#iclicker",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "iClicker",
    "text": "iClicker\nHow did you feel about the exam last week?\n\n\nI felt well-prepared and it went smoothly\n\n\nI think it went okay. We‚Äôll see when grades come back\n\n\nI struggled and didn‚Äôt feel fully prepared\n\n\nI noticed some gaps between what we practiced and what appeared on the exam\n\n\nIt was a stressful experience for me üòî"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#scenario-1-which-model-would-you-pick-why",
    "href": "slides/slides-12-feature-importances.html#scenario-1-which-model-would-you-pick-why",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Scenario 1: Which model would you pick? Why?",
    "text": "Scenario 1: Which model would you pick? Why?\nPredicting whether a patient is likely to develop diabetes based on features such as age, blood pressure, glucose levels, and BMI. You have two models:\n\nLGBM which results in 0.9 f1 score\nLogistic regression which results in 0.84 f1 score"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#scenario-2-which-model-would-you-pick-why",
    "href": "slides/slides-12-feature-importances.html#scenario-2-which-model-would-you-pick-why",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Scenario 2: Which model would you pick? Why?",
    "text": "Scenario 2: Which model would you pick? Why?\nYou‚Äôre building a model to predict whether a user will make their next purchase based on their browsing history, past purchases, and click behaviour. You have two candidate models:\n\nLGBM which results in 0.9 F1 score\nLogistic regression which results in 0.84 F1 score"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#transparency",
    "href": "slides/slides-12-feature-importances.html#transparency",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Transparency",
    "text": "Transparency\nIn many domains understanding the relationship between features and predictions is critical for trust and regulatory compliance."
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#feature-importances",
    "href": "slides/slides-12-feature-importances.html#feature-importances",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Feature importances",
    "text": "Feature importances\n\nHow does the output depend upon the input?\nHow do the predictions change as a function of a particular feature?\nHow can we quantify and visualize feature importances?"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#correlations",
    "href": "slides/slides-12-feature-importances.html#correlations",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Correlations",
    "text": "Correlations\n\n\n\n\n\nWhat are some limitations of correlations?"
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#interepreting-coefficients",
    "href": "slides/slides-12-feature-importances.html#interepreting-coefficients",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Interepreting coefficients",
    "text": "Interepreting coefficients\n\nLinear models are interpretable because you get coefficients associated with different features.\nEach coefficient represents the estimated impact of a feature on the target variable, assuming all other features are held constant.\nIn a Ridge model,\n\nA positive coefficient indicates that as the feature‚Äôs value increases, the predicted value also increases.\n\nA negative coefficient indicates that an increase in the feature‚Äôs value leads to a decrease in the predicted value."
  },
  {
    "objectID": "slides/slides-12-feature-importances.html#interepreting-coefficients-1",
    "href": "slides/slides-12-feature-importances.html#interepreting-coefficients-1",
    "title": "CPSC 330 Lecture 12: Feature importances",
    "section": "Interepreting coefficients",
    "text": "Interepreting coefficients\n\nWhen we have different types of preprocessed features, what challenges you might face in interpreting them?\n\nOrdinally encoded features\nOne-hot encoded features\nScaled numeric features"
  },
  {
    "objectID": "slides/slides-14-k-means.html#focus-on-the-breath",
    "href": "slides/slides-14-k-means.html#focus-on-the-breath",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Focus on the breath!",
    "text": "Focus on the breath!"
  },
  {
    "objectID": "slides/slides-14-k-means.html#announcements",
    "href": "slides/slides-14-k-means.html#announcements",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Announcements",
    "text": "Announcements\n\nMidterm 1 grades were released last week\nHW5 was due yesterday\nHW6 has been released. It‚Äôs due next week Monday.\n\nComputationally intensive\nYou need to install many packages"
  },
  {
    "objectID": "slides/slides-14-k-means.html#iclicker-midterm-poll",
    "href": "slides/slides-14-k-means.html#iclicker-midterm-poll",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "(iClicker) Midterm poll",
    "text": "(iClicker) Midterm poll\nSelect all of the following statements which are TRUE.\n\n\nI‚Äôm happy with my progress and learning in this course.\n\n\nI find the course content interesting, but the pace is a bit overwhelming. Balancing this course with other responsibilities is challenging\n\n\nI‚Äôm doing okay, but I feel stressed and worried about upcoming assessments.\n\n\nI‚Äôm confused about some concepts and would appreciate more clarification or review sessions.\n\n\nI‚Äôm struggling to keep up with the material. I am not happy with my learning in this course and my morale is low :(."
  },
  {
    "objectID": "slides/slides-14-k-means.html#supervised-learning",
    "href": "slides/slides-14-k-means.html#supervised-learning",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nTraining data comprises a set of observations (\\(X\\)) and their corresponding targets (\\(y\\)).\nWe wish to find a model function \\(f\\) that relates \\(X\\) to \\(y\\).\nThen use that model function to predict the targets of new examples.\nWe have been working with this set up so far."
  },
  {
    "objectID": "slides/slides-14-k-means.html#unsupervised-learning",
    "href": "slides/slides-14-k-means.html#unsupervised-learning",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nTraining data consists of observations (\\(X\\)) without any corresponding targets.\nUnsupervised learning could be used to group similar things together in \\(X\\) or to find underlying structure in the data."
  },
  {
    "objectID": "slides/slides-14-k-means.html#clustering-activity",
    "href": "slides/slides-14-k-means.html#clustering-activity",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Clustering activity",
    "text": "Clustering activity\n\n\nCategorize the food items in the image and write your categories. Do you think there is one correct way to cluster these images? Why or why not?\nIf you want to build a machine learning model to cluster such images how would you represent such images?\nWrite your answers here: https://docs.google.com/document/d/12GXA9Efi_19WiRnCi8FiOmdpxM-I_wq2nz97LjvXILg/edit?usp=sharing"
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce",
    "href": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The ‚Äúperfect‚Äù spaghetti sauce",
    "text": "The ‚Äúperfect‚Äù spaghetti sauce\nSuppose you are a hypothetical spaghetti sauce company and you‚Äôre asked to create the ‚Äúperfect‚Äù spaghetti sauce which makes all your customers happy. The truth is humans are diverse and there is no ‚Äúperfect‚Äù spaghetti sauce. There are ‚Äúperfect‚Äù spaghetti sauces that cater to different tastes!"
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce-1",
    "href": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce-1",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The ‚Äúperfect‚Äù spaghetti sauce",
    "text": "The ‚Äúperfect‚Äù spaghetti sauce\nHoward Moskowitz found out that Americans fall into one of the following three categories:\n\npeople who like their spaghetti sauce plain\npeople who like their spaghetti sauce spicy\npeople who like their spaghetti sauce extra chunky\n\n\n\n\n\n\nReference: Malcolm Gladwell‚Äôs Ted talk"
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce-2",
    "href": "slides/slides-14-k-means.html#the-perfect-spaghetti-sauce-2",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The ‚Äúperfect‚Äù spaghetti sauce",
    "text": "The ‚Äúperfect‚Äù spaghetti sauce\n\nIf one ‚Äúperfect‚Äù authentic sauce satisfies 60%, of the people on average, creating several tailored sauce clusters could increase average happiness to between 75% to 78%.\nCan we apply this concept of clustering and tailoring solutions to specific groups in machine learning?"
  },
  {
    "objectID": "slides/slides-14-k-means.html#k-means-clustering-algorithm",
    "href": "slides/slides-14-k-means.html#k-means-clustering-algorithm",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "K-Means clustering algorithm",
    "text": "K-Means clustering algorithm\n\nSelect K initial centroids.\nAssign each data point to the nearest centroid.\nRecalculate centroids based on assigned points.\nRepeat until centroids stabilize or reach a maximum number of iterations."
  },
  {
    "objectID": "slides/slides-14-k-means.html#initialization",
    "href": "slides/slides-14-k-means.html#initialization",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Initialization",
    "text": "Initialization\nIn K-Means, we start with random initial cluster centers. Who would like to be cluster centers?\n\n\n\nSource: Image created by ChatGPT 5.0\n\nLet‚Äôs pretend you are data points scattered across a 2D feature space."
  },
  {
    "objectID": "slides/slides-14-k-means.html#cluster-assignment",
    "href": "slides/slides-14-k-means.html#cluster-assignment",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Cluster assignment",
    "text": "Cluster assignment\nWhich cluster do you belong to?\n\n\n\nSource: Image created by ChatGPT 5.0\n\nLook around and decide which cluster center is closest to you, just by eyeballing distance in the room."
  },
  {
    "objectID": "slides/slides-14-k-means.html#recompute-cluster-centers",
    "href": "slides/slides-14-k-means.html#recompute-cluster-centers",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Recompute cluster centers",
    "text": "Recompute cluster centers\n\nNow each cluster needs to find its new mean location. Let‚Äôs compute new centroids!\nFor each cluster, identify the ‚Äúaverage‚Äù position of your members. For example, pick someone roughly in the middle or calculate the average row/column number.\nMove your old ‚Äúcluster center‚Äù to these new locations."
  },
  {
    "objectID": "slides/slides-14-k-means.html#repeat-cluster-assignment",
    "href": "slides/slides-14-k-means.html#repeat-cluster-assignment",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Repeat cluster assignment",
    "text": "Repeat cluster assignment\nNow that cluster centers have moved, you may need to switch clusters! Check again: which center are you closest to now?"
  },
  {
    "objectID": "slides/slides-14-k-means.html#discussion-questions",
    "href": "slides/slides-14-k-means.html#discussion-questions",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nDid anyone switch clusters in the second round?\nWhen do you think K-Means stops?\nCould we end up in a different final clustering if we started with different centers?"
  },
  {
    "objectID": "slides/slides-14-k-means.html#k-means-example",
    "href": "slides/slides-14-k-means.html#k-means-example",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "K-Means example",
    "text": "K-Means example"
  },
  {
    "objectID": "slides/slides-14-k-means.html#k-means-pros-and-cons",
    "href": "slides/slides-14-k-means.html#k-means-pros-and-cons",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "K-Means pros and cons",
    "text": "K-Means pros and cons\n\nAdvantages:\n\nSimple and efficient for large datasets.\nWorks well with spherical clusters.\n\nLimitations:\n\nNeeds pre-defined \\(k\\).\nSensitive to outliers and initial centroid placement."
  },
  {
    "objectID": "slides/slides-14-k-means.html#iclicker-exercise-14.1",
    "href": "slides/slides-14-k-means.html#iclicker-exercise-14.1",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "iClicker Exercise 14.1",
    "text": "iClicker Exercise 14.1\nSelect all of the following statements which are True\n\n\nK-Means algorithm always converges to the same solution.\n\n\n\\(K\\) in K-Means should always be \\(\\leq\\) # of features.\n\n\nIn K-Means, it makes sense to have \\(K\\) \\(\\leq\\) # of examples.\n\n\nIn K-Means, in some iterations some points may be left unassigned."
  },
  {
    "objectID": "slides/slides-14-k-means.html#iclicker-exercise-14.2",
    "href": "slides/slides-14-k-means.html#iclicker-exercise-14.2",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "iClicker Exercise 14.2",
    "text": "iClicker Exercise 14.2\nSelect all of the following statements which are True\n\n\nK-Means is sensitive to initialization and the solution may change depending upon the initialization.\n\n\nK-means terminates when the number of clusters does not increase between iterations.\n\n\nK-means terminates when the centroid locations do not change between iterations.\n\n\nK-Means is guaranteed to find the optimal solution."
  },
  {
    "objectID": "slides/slides-14-k-means.html#how-many-clusters",
    "href": "slides/slides-14-k-means.html#how-many-clusters",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "How many clusters?",
    "text": "How many clusters?\n\nClustering is not about perfect scores, but insight.\nTypically we do not want too few or way too many clusters.\nThere‚Äôs no single ‚Äúcorrect‚Äù value of \\(k\\) ‚Äì only useful ones that make sense for your problem.\nSome preferred values may exist (e.g., based on domain knowledge).\nThe goal is to find interpretable, meaningful clusters. So pick \\(k\\) that helps tell a useful story about your data.\nWe typically go with quantitative and qualitative approaches."
  },
  {
    "objectID": "slides/slides-14-k-means.html#section",
    "href": "slides/slides-14-k-means.html#section",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "",
    "text": "Quantitative methods\n\nThe Elbow plot\n\nPlot inertia vs.¬†K\n\nLook for the ‚Äúelbow‚Äù ‚Äî diminishing returns\n\n\nSilhouette score\n\nMeasures cluster cohesion and separation\n\nHigher score: better-defined clusters\n\n\n\nQualitative methods\n\nManually inspect clusters:\n\nDo they make sense?\nAre they interpretable and actionable?\n\nThe ultimate goal:\nHuman-understandable themes for real-world decisions.\nThere‚Äôs no escape from manual interpretation!"
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-elbow-method",
    "href": "slides/slides-14-k-means.html#the-elbow-method",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The Elbow method",
    "text": "The Elbow method\n\nPurpose: Identify the optimal number of clusters \\(k\\).\nHow it Works:\n\nPlot intra-cluster distances for different values of \\(k\\).\nLook for the ‚Äúelbow‚Äù point where the intra-cluster reduction slows.\n\nInterpretation:\n\nThe point of diminishing returns suggests a good \\(k\\)."
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-elbow-method-example",
    "href": "slides/slides-14-k-means.html#the-elbow-method-example",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The Elbow method example",
    "text": "The Elbow method example\n\n\n\n\nWhat would be the intracluster distance when\n\n\\(k=1\\)\n\\(k=\\) # number of data points?"
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-silhouette-method",
    "href": "slides/slides-14-k-means.html#the-silhouette-method",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The Silhouette method",
    "text": "The Silhouette method\n\nSilhouette Score: Measures how well data points fit within their cluster.\n\n\\(s(i) = \\frac{b(i) - a(i)}{\\max (a(i), b(i))}\\)\n\n\\(a(i)\\): Mean distance to other points in the same cluster.\n\\(b(i)\\): Mean distance to points in the nearest neighboring cluster."
  },
  {
    "objectID": "slides/slides-14-k-means.html#the-silhouette-method-1",
    "href": "slides/slides-14-k-means.html#the-silhouette-method-1",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "The Silhouette method",
    "text": "The Silhouette method\n\nRange: -1 to 1\n\n1: Perfect clustering.\n0: Overlapping clusters.\nNegative: Poor clustering.\n\nHigher average silhouette score indicates ‚Äúbetter‚Äù clustering."
  },
  {
    "objectID": "slides/slides-14-k-means.html#silhouette-plot-example",
    "href": "slides/slides-14-k-means.html#silhouette-plot-example",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "Silhouette plot example",
    "text": "Silhouette plot example"
  },
  {
    "objectID": "slides/slides-14-k-means.html#iclicker-exercise-14.3",
    "href": "slides/slides-14-k-means.html#iclicker-exercise-14.3",
    "title": "CPSC 330 Lecture 14: K-Means",
    "section": "iClicker Exercise 14.3",
    "text": "iClicker Exercise 14.3\nSelect all of the following statements which are True\n\n\nIf you train K-Means with n_clusters= the number of examples, the inertia value will be 0.\n\n\nThe elbow plot shows the tradeoff between within cluster distance and the number of clusters.\n\n\nUnlike the Elbow method, the Silhouette method is not dependent on the notion of cluster centers.\n\n\nThe elbow plot is not a reliable method to obtain the optimal number of clusters in all cases.\n\n\nThe Silhouette scores ranges between -1 and 1 where higher scores indicates better cluster assignments."
  },
  {
    "objectID": "lecture-01.html",
    "href": "lecture-01.html",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#slides",
    "href": "lecture-01.html#slides",
    "title": "Lecture 1: Course introduction",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-01.html#outline",
    "href": "lecture-01.html#outline",
    "title": "Lecture 1: Course introduction",
    "section": "Outline",
    "text": "Outline\n\nWhat is machine learning\nTypes of machine learning\nLearning to navigate through the course materials\nGetting familiar with the course policies",
    "crumbs": [
      "Lectures",
      "Lecture 1: Course introduction"
    ]
  },
  {
    "objectID": "lecture-03.html",
    "href": "lecture-03.html",
    "title": "Lecture 3: ML fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-03.html#slides",
    "href": "lecture-03.html#slides",
    "title": "Lecture 3: ML fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-03.html#outline",
    "href": "lecture-03.html#outline",
    "title": "Lecture 3: ML fundamentals",
    "section": "Outline",
    "text": "Outline\n\nGeneralization, data splitting\nCross-validation\nOverfitting, underfitting, the fundamental tradeoff\nThe golden rule",
    "crumbs": [
      "Lectures",
      "Lecture 3: ML fundamentals"
    ]
  },
  {
    "objectID": "lecture-05.html",
    "href": "lecture-05.html",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-05.html#slides",
    "href": "lecture-05.html#slides",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-05.html#outline",
    "href": "lecture-05.html#outline",
    "title": "Lecture 5: Preprocessing and sklearn pipelines",
    "section": "Outline",
    "text": "Outline\n\nRecap\nPreprocessing motivation\nCommon transformations in sklearn\nsklearn transformers vs.¬†Estimators\nThe golden rule in the feature transformations\nsklearn pipelines",
    "crumbs": [
      "Lectures",
      "Lecture 5: Preprocessing and sklearn pipelines"
    ]
  },
  {
    "objectID": "lecture-07.html",
    "href": "lecture-07.html",
    "title": "Lecture 7: Linear models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-07.html#slides",
    "href": "lecture-07.html#slides",
    "title": "Lecture 7: Linear models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-07.html#outline",
    "href": "lecture-07.html#outline",
    "title": "Lecture 7: Linear models",
    "section": "Outline",
    "text": "Outline\n\nIntuition behind linear models\nLinear regression and logistic regression\nscikit-learn‚Äôs Ridge model\nPrediction probabilities\nInterpret model predictions using coefficients learned by a linear model\nParametric vs.¬†non-parametric models",
    "crumbs": [
      "Lectures",
      "Lecture 7: Linear models"
    ]
  },
  {
    "objectID": "lecture-09.html",
    "href": "lecture-09.html",
    "title": "Lecture 9: Classification metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-09.html#slides",
    "href": "lecture-09.html#slides",
    "title": "Lecture 9: Classification metrics",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-09.html#outline",
    "href": "lecture-09.html#outline",
    "title": "Lecture 9: Classification metrics",
    "section": "Outline",
    "text": "Outline\n\nIssues with using accuracy\nComponents of a confusion matrix\nPrecision, recall, and f1-score and use them to evaluate different classifiers\nPrecision-recall curves\nAverage precision score\nROC curves and ROC AUC using scikit-learn\nDealing with class imbalance\nModel performance on specific groups in a dataset.",
    "crumbs": [
      "Lectures",
      "Lecture 9: Classification metrics"
    ]
  },
  {
    "objectID": "lecture-11.html",
    "href": "lecture-11.html",
    "title": "Lecture 11: Ensembles",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 11: Ensembles"
    ]
  },
  {
    "objectID": "lecture-11.html#slides",
    "href": "lecture-11.html#slides",
    "title": "Lecture 11: Ensembles",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 11: Ensembles"
    ]
  },
  {
    "objectID": "lecture-11.html#outline",
    "href": "lecture-11.html#outline",
    "title": "Lecture 11: Ensembles",
    "section": "Outline",
    "text": "Outline\n\nMotivation\nRandom forests\nGradient boosting\nAveraging\nStacking",
    "crumbs": [
      "Lectures",
      "Lecture 11: Ensembles"
    ]
  },
  {
    "objectID": "lecture-13.html",
    "href": "lecture-13.html",
    "title": "Lecture 13: Feature engineering and selection",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 13: Feature engineering and selection"
    ]
  },
  {
    "objectID": "lecture-13.html#slides",
    "href": "lecture-13.html#slides",
    "title": "Lecture 13: Feature engineering and selection",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 13: Feature engineering and selection"
    ]
  },
  {
    "objectID": "lecture-13.html#outline",
    "href": "lecture-13.html#outline",
    "title": "Lecture 13: Feature engineering and selection",
    "section": "Outline",
    "text": "Outline\n\nMotivation for feature engineering\nPreliminary feature engineering on text data\nGeneral concept of feature selection.\nModel-based feature selection vs.¬†RFE",
    "crumbs": [
      "Lectures",
      "Lecture 13: Feature engineering and selection"
    ]
  },
  {
    "objectID": "lecture-15.html",
    "href": "lecture-15.html",
    "title": "Lecture 15: DBSCAN and hierarchical",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 15: DBSCAN and hierarchical"
    ]
  },
  {
    "objectID": "lecture-15.html#slides",
    "href": "lecture-15.html#slides",
    "title": "Lecture 15: DBSCAN and hierarchical",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "Lecture 15: DBSCAN and hierarchical"
    ]
  },
  {
    "objectID": "lecture-15.html#outline",
    "href": "lecture-15.html#outline",
    "title": "Lecture 15: DBSCAN and hierarchical",
    "section": "Outline",
    "text": "Outline\n\nLimitations of K-Means\nDBSCAN\nHierarchical clustering, dendrograms\nComparing and contrasting different clustering methods",
    "crumbs": [
      "Lectures",
      "Lecture 15: DBSCAN and hierarchical"
    ]
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#announcements",
    "href": "slides/slides-03-ml-fundamentals.html#announcements",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 2 (hw2) has been released (Due: Sept 16, 11:59pm)\n\nYou are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.\nGroup submissions are not allowed for this assignment.\n\nAdvice on keeping up with the material\n\nPractice!\nStart early on homework assignments.\n\nIf you are still on the waitlist, it‚Äôs your responsibility to keep up with the material and submit assignments.\nLast day to drop without a W standing: Sept 15"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#iclicker-3.1",
    "href": "slides/slides-03-ml-fundamentals.html#iclicker-3.1",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "iClicker 3.1",
    "text": "iClicker 3.1\nClicker cloud join link: https://join.iclicker.com/FZMQ\nSelect all of the following statements which are TRUE.\n\n\nA decision tree model with no depth (the default max_depth in sklearn) is likely to perform very well on the deployment data.\n\n\nData splitting helps us assess how well our model would generalize.\n\n\nDeployment data is scored only once.\n\n\nValidation data could be used for hyperparameter optimization.\n\n\nIt‚Äôs recommended that data be shuffled before splitting it into train and test sets."
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#iclicker-3.2",
    "href": "slides/slides-03-ml-fundamentals.html#iclicker-3.2",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "iClicker 3.2",
    "text": "iClicker 3.2\nClicker cloud join link: https://join.iclicker.com/FZMQ\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-fold cross-validation calls fit \\(k\\) times\n\n\nWe use cross-validation to get a more robust estimate of model performance.\n\n\nIf the mean train accuracy is much higher than the mean cross-validation accuracy it‚Äôs likely to be a case of overfitting.\n\n\nThe fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n\n\nA decision stump on a complicated classification problem is likely to underfit."
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#recap-from-videos",
    "href": "slides/slides-03-ml-fundamentals.html#recap-from-videos",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Recap from videos",
    "text": "Recap from videos\n\nWhy do we split the data? What are train/valid/test splits?\nWhat are the benefits of cross-validation?\nWhat is underfitting and overfitting?\nWhat‚Äôs the fundamental trade-off in supervised machine learning?\nWhat is the golden rule of machine learning?"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#summary-of-train-validation-test-and-deployment-data",
    "href": "slides/slides-03-ml-fundamentals.html#summary-of-train-validation-test-and-deployment-data",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Summary of train, validation, test, and deployment data",
    "text": "Summary of train, validation, test, and deployment data\n\n\n\n\nfit\nscore\npredict\n\n\n\n\nTrain\n‚úîÔ∏è\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nValidation\n\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nTest\n\nonce\nonce\n\n\nDeployment\n\n\n‚úîÔ∏è"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#cross-validation",
    "href": "slides/slides-03-ml-fundamentals.html#cross-validation",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#cross-validation-1",
    "href": "slides/slides-03-ml-fundamentals.html#cross-validation-1",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#overfitting-and-underfitting",
    "href": "slides/slides-03-ml-fundamentals.html#overfitting-and-underfitting",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\n\n\nSource\n\n\nAn overfit model matches the training set so closely that it fails to make correct predictions on new unseen data.\n\nAn underfit model is too simple and does not even make good predictions on the training data"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#the-fundamental-tradeoff",
    "href": "slides/slides-03-ml-fundamentals.html#the-fundamental-tradeoff",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "The fundamental tradeoff",
    "text": "The fundamental tradeoff\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.\n\n\n\n\n\n\n\n\n\nUnderfitting: Both accuracies rise\nSweet spot: Validation accuracy peaks\nOverfitting: Training \\(\\uparrow\\), Validation \\(\\downarrow\\)\nTradeoff: Balance complexity to avoid both"
  },
  {
    "objectID": "slides/slides-03-ml-fundamentals.html#the-golden-rule",
    "href": "slides/slides-03-ml-fundamentals.html#the-golden-rule",
    "title": "CPSC 330 Lecture 3: ML fundamentals",
    "section": "The golden rule",
    "text": "The golden rule\n\nAlthough our primary concern is the model‚Äôs performance on the test data, this data should not influence the training process in any way.\n\n\n\n Source: Image generated by ChatGPT 5\n\n\nTest data = final exam\n\nYou can practice all you want with training/validation data\nBut never peek at the test set before evaluation\nOtherwise, it‚Äôs like sneaking answers before the exam \\(\\rightarrow\\) not a real assessment of your learning."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#announcements",
    "href": "slides/slides-07-linear-models.html#announcements",
    "title": "Lecture 7: Linear models",
    "section": "Announcements",
    "text": "Announcements\n\nImportant information about midterm 1\n\nhttps://piazza.com/class/mekbcze4gyber/post/162\n\nWhere to find slides?\n\nhttps://kvarada.github.io/cpsc330-slides/lecture.html\n\nHW3 is due next week Monday, Sept 29th, 11:59 pm."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#recap-dealing-with-text-features",
    "href": "slides/slides-07-linear-models.html#recap-dealing-with-text-features",
    "title": "Lecture 7: Linear models",
    "section": "Recap: Dealing with text features",
    "text": "Recap: Dealing with text features\n\nPreprocessing text to fit into machine learning models using text vectorization.\nBag of words representation"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#recap-sklearn-countvectorizer",
    "href": "slides/slides-07-linear-models.html#recap-sklearn-countvectorizer",
    "title": "Lecture 7: Linear models",
    "section": "Recap: sklearn CountVectorizer",
    "text": "Recap: sklearn CountVectorizer\n\nUse scikit-learn‚Äôs CountVectorizer to encode text data\nCountVectorizer: Transforms text into a matrix of token counts\nImportant parameters:\n\nmax_features: Control the number of features used in the model\nmax_df, min_df: Control document frequency thresholds\nngram_range: Defines the range of n-grams to be extracted\nstop_words: Enables the removal of common words that are typically uninformative in most applications, such as ‚Äúand‚Äù, ‚Äúthe‚Äù, etc."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#iclicker-exercise-6.2",
    "href": "slides/slides-07-linear-models.html#iclicker-exercise-6.2",
    "title": "Lecture 7: Linear models",
    "section": "(iClicker) Exercise 6.2",
    "text": "(iClicker) Exercise 6.2\nSelect all of the following statements which are TRUE.\n\n\nhandle_unknown=\"ignore\" would treat all unknown categories equally.\n\n\nAs you increase the value for max_features hyperparameter of CountVectorizer the training score is likely to go up.\n\n\nSuppose you are encoding text data using CountVectorizer. If you encounter a word in the validation or the test split that‚Äôs not available in the training data, we‚Äôll get an error.\n\n\nIn the code below, inside cross_validate, each fold might have slightly different number of features (columns) in the fold.\n\n\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#linear-models",
    "href": "slides/slides-07-linear-models.html#linear-models",
    "title": "Lecture 7: Linear models",
    "section": "Linear models",
    "text": "Linear models\n\n\n\nLinear models make an assumption that the relationship between X and y is linear.\nIn this case, with only one feature, our model is a straight line.\nWhat do we need to represent a line?\n\nSlope (\\(w_1\\)): Determines the angle of the line.\nY-intercept (\\(w_0\\)): Where the line crosses the y-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking predictions: \\(y_{hat} = w_1 \\times \\text{\\# hours studied} + w_0\\)"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#ridge-vs.-linearregression",
    "href": "slides/slides-07-linear-models.html#ridge-vs.-linearregression",
    "title": "Lecture 7: Linear models",
    "section": "Ridge vs.¬†LinearRegression",
    "text": "Ridge vs.¬†LinearRegression\n\nOrdinary linear regression is sensitive to multicolinearity and overfitting\nMulticolinearity: Overlapping and redundant features. Most of the real-world datasets have colinear features.\n\nLinear regression may produce large and unstable coefficients in such cases.\nRidge adds a parameter to control the complexity of a model. Finds a line that balances fit and prevents overly large coefficients."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#when-to-use-what",
    "href": "slides/slides-07-linear-models.html#when-to-use-what",
    "title": "Lecture 7: Linear models",
    "section": "When to use what?",
    "text": "When to use what?\n\nLinearRegression\n\nWhen interpretability is key, and no multicollinearity exists\n\nRidge\n\nWhen you have multicollinearity (highly correlated features).\nWhen you want to prevent overfitting in linear models.\n\nIn this course, we‚Äôll use Ridge."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#logistic-regression",
    "href": "slides/slides-07-linear-models.html#logistic-regression",
    "title": "Lecture 7: Linear models",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nSuppose your target is binary: pass or fail\nLogistic regression is used for such binary classification tasks.\n\nLogistic regression predicts a probability that the given example belongs to a particular class.\nIt uses Sigmoid function to map any real-valued input into a value between 0 and 1, representing the probability of a specific outcome.\nA threshold (usually 0.5) is applied to the predicted probability to decide the final class label."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#logistic-regression-decision-boundary",
    "href": "slides/slides-07-linear-models.html#logistic-regression-decision-boundary",
    "title": "Lecture 7: Linear models",
    "section": "Logistic regression: Decision boundary",
    "text": "Logistic regression: Decision boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\nSigmoid Function: \\(\\hat{y} = \\sigma(w^\\top x_i + b) = \\frac{1}{1 + e^{-(w^\\top x_i + b)}}\\)\nThe decision boundary is the point on the x-axis where the corresponding predicted probability on the y-axis is 0.5."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#sentiment-analysis-example",
    "href": "slides/slides-07-linear-models.html#sentiment-analysis-example",
    "title": "Lecture 7: Linear models",
    "section": "Sentiment analysis example",
    "text": "Sentiment analysis example\n\n\n\n\n\nLogistic regression learns coefficients for each word from training data.\n\nPositive coefficients \\(\\rightarrow\\) push prediction toward positive class.\n\nNegative coefficients \\(\\rightarrow\\) push prediction toward negative class.\n\nIn this example, positive words (fun, rewarding) outweigh the negative word (long), so the overall sentiment is likely positive."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#parametric-vs.-non-parametric-models-high-level",
    "href": "slides/slides-07-linear-models.html#parametric-vs.-non-parametric-models-high-level",
    "title": "Lecture 7: Linear models",
    "section": "Parametric vs.¬†non-Parametric models (high-level)",
    "text": "Parametric vs.¬†non-Parametric models (high-level)\n\nImagine you are training a logistic regression model. For each of the following scenarios, identify how many parameters (weights and biases) will be learned.\nScenario 1: 100 features and 1,000 examples\nScenario 2: 100 features and 1 million examples"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#parametric-vs.-non-parametric-models-high-level-1",
    "href": "slides/slides-07-linear-models.html#parametric-vs.-non-parametric-models-high-level-1",
    "title": "Lecture 7: Linear models",
    "section": "Parametric vs.¬†non-Parametric models (high-level)",
    "text": "Parametric vs.¬†non-Parametric models (high-level)\n\n\nParametric\n\nExamples: Logistic regression, linear regression, linear SVM\n\nModels with a fixed number of parameters, regardless of the dataset size\nSimple, computationally efficient, less prone to overfitting\nLess flexible, may not capture complex relationships\n\n\nNon parametric\n\nExamples: KNN, SVM RBF, Decision tree with no specific depth specified\nModels where the number of parameters grows with the dataset size. They do not assume a fixed form for the functions being learned.\nFlexible, can adapt to complex patterns\nComputationally expensive, risk of overfitting with noisy data"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#iclicker-exercise-7.1",
    "href": "slides/slides-07-linear-models.html#iclicker-exercise-7.1",
    "title": "Lecture 7: Linear models",
    "section": "(iClicker) Exercise 7.1",
    "text": "(iClicker) Exercise 7.1\nSelect all of the following statements which are TRUE.\n\n\nIncreasing the hyperparameter alpha of Ridge is likely to decrease model complexity.\n\n\nRidge can be used with datasets that have multiple features.\n\n\nWith Ridge, we learn one coefficient per training example.\n\n\nIf you train a linear regression model on a 2-dimensional problem (2 features), the model will learn 3 parameters: one for each feature and one for the bias term."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#iclicker-exercise-7.2",
    "href": "slides/slides-07-linear-models.html#iclicker-exercise-7.2",
    "title": "Lecture 7: Linear models",
    "section": "(iClicker) Exercise 7.2",
    "text": "(iClicker) Exercise 7.2\nSelect all of the following statements which are TRUE.\n\n\nIncreasing logistic regression‚Äôs C hyperparameter increases model complexity.\n\n\nThe raw output score can be used to calculate the probability score for a given prediction.\n\n\nFor linear classifier trained on \\(d\\) features, the decision boundary is a \\(d-1\\)-dimensional hyperparlane.\n\n\nA linear model is likely to be uncertain about the data points close to the decision boundary."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#softmax-function-for-probabilities",
    "href": "slides/slides-07-linear-models.html#softmax-function-for-probabilities",
    "title": "Lecture 7: Linear models",
    "section": "Softmax Function for Probabilities",
    "text": "Softmax Function for Probabilities\nGiven an input, the probability that it belongs to class \\(j \\in \\{1, 2, \\dots, K\\}\\) is calculated using the softmax function:\n\\(P(y = j \\mid x_i) = \\frac{e^{w_j^\\top x_i + b_j}}{\\sum_{k=1}^{K} e^{w_k^\\top x_i + b_k}}\\)\n\n\\(x_i\\) is the \\(i^{th}\\) example\n\\(w_j\\) is the weight vector for class \\(j\\).\n\\(b_j\\) is the bias term for class \\(j\\).\n\\(K\\) is the total number of classes."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#making-predictions",
    "href": "slides/slides-07-linear-models.html#making-predictions",
    "title": "Lecture 7: Linear models",
    "section": "Making Predictions",
    "text": "Making Predictions\n\nCompute Probabilities:\nFor each class \\(j\\), compute the probability \\(P(y = j \\mid x_i)\\) using the softmax function.\nSelect the Class with the Highest Probability:\nThe predicted class \\(\\hat{y}\\) is:\n\\(\\hat{y} = \\arg \\max_{j \\in \\{1, \\dots, K\\}} P(y = j \\mid x_i)\\)"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#binary-vs-multinomial-logistic-regression",
    "href": "slides/slides-07-linear-models.html#binary-vs-multinomial-logistic-regression",
    "title": "Lecture 7: Linear models",
    "section": "Binary vs multinomial logistic regression",
    "text": "Binary vs multinomial logistic regression\n\n\n\n\n\n\n\n\nAspect\nBinary Logistic Regression\nMultinomial Logistic Regression\n\n\n\n\nTarget variable\n2 classes (binary)\nMore than 2 classes (multi-class)\n\n\nGetting probabilities\nSigmoid\nSoftmax\n\n\nparameters\n\\(d\\) weights, one per feature and the bias term\n\\(d\\) weights and a bias term per class\n\n\nOutput\nSingle probability\nProbability distribution over classes\n\n\nUse case\nBinary classification (e.g., spam detection)\nMulti-class classification (e.g., flower species)"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#activity-time-permitting",
    "href": "slides/slides-07-linear-models.html#activity-time-permitting",
    "title": "Lecture 7: Linear models",
    "section": "Activity (time-permitting)",
    "text": "Activity (time-permitting)\n\nSo far, we have worked with various transformers and supervised machine learning models. The goal of this activity is collaboratively complete tables that provide an overview of\n\nthe strengths, weaknesses, key hyperparameters of different machine learning models\nthe pupose, use cases, and key considerations of various transformers\n\n(This will serve as a handy reference for your upcoming exam and beyond!)"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#activity-description",
    "href": "slides/slides-07-linear-models.html#activity-description",
    "title": "Lecture 7: Linear models",
    "section": "Activity description",
    "text": "Activity description\n\nYour task is to engage in group discussions and fill in the designated row in this Google document.\nFor strengths and weaknesses, some things to consider are:\n\nconcerns about underfitting\nconcerns about overfitting\nspeed\nscalability for large data sets\n\ninterpretability\neffectiveness on sparse data\nease of use for multi-class classification\n\nability to represent uncertainty\ntime/space complexity\netc."
  },
  {
    "objectID": "slides/slides-07-linear-models.html#estimators",
    "href": "slides/slides-07-linear-models.html#estimators",
    "title": "Lecture 7: Linear models",
    "section": "Estimators",
    "text": "Estimators\nFill in the following table with at least one entry per box.\n\n\n\n\n\n\n\n\n\nModel\nStrengths\nWeaknesses\nKey hyperparameters\n\n\n\n\ndecision tree\n\n\n\n\n\n\\(k\\)-NN\n\n\n\n\n\nRBF SVM\n\n\n\n\n\nlinear models"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#transformers",
    "href": "slides/slides-07-linear-models.html#transformers",
    "title": "Lecture 7: Linear models",
    "section": "Transformers",
    "text": "Transformers\n\n\n\n\n\n\n\n\n\nTransformation\nPurpose\nUse cases\nKey consideration\n\n\n\n\nImputation\n\n\n\n\n\nScaling\n\n\n\n\n\nOne-hot encoding\n\n\n\n\n\nOrdinal encoding\n\n\n\n\n\nBag-of-words encoding"
  },
  {
    "objectID": "slides/slides-07-linear-models.html#coming-up",
    "href": "slides/slides-07-linear-models.html#coming-up",
    "title": "Lecture 7: Linear models",
    "section": "Coming up",
    "text": "Coming up\nA few big questions remain:\n\nHow do we tune hyperparameters?\nHow do we choose our features? (feature selection, dimensionality reduction, regularization etc.)\nHow do we come up with new useful features (feature engineering)\nHow do we choose between different models? (different evaluation metrics, what happens if we‚Äôre not happy with our test error?)\nHow to deal with class imbalance?\nWhat do we do if we do not have targets (unsupervised learning)\nHow to build models for more interesting data such as images, user preferences, or sequential data?)"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#recap-countvectorizer-input",
    "href": "slides/slides-06-hyperparameter-optimization.html#recap-countvectorizer-input",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Recap: CountVectorizer input",
    "text": "Recap: CountVectorizer input\n\nPrimarily designed to accept either a pandas.Series of text data or a 1D numpy array. It can also process a list of string data directly.\nUnlike many transformers that handle multiple features (DataFrame or 2D numpy array), CountVectorizer a single text column at a time.\nIf your dataset contains multiple text columns, you will need to instantiate separate CountVectorizer objects for each text feature.\nThis approach ensures that the unique vocabulary and tokenization processes are correctly applied to each specific text column without interference."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#hyperparameter-optimization-motivation",
    "href": "slides/slides-06-hyperparameter-optimization.html#hyperparameter-optimization-motivation",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Hyperparameter optimization motivation",
    "text": "Hyperparameter optimization motivation"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#data",
    "href": "slides/slides-06-hyperparameter-optimization.html#data",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Data",
    "text": "Data\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\ntrain_df.head(4)\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\n3130\nspam\nLookAtMe!: Thanks for your purchase of a video...\n\n\n106\nham\nAight, I'll hit you up when I get some cash\n\n\n4697\nham\nDon no da:)whats you plan?\n\n\n856\nham\nGoing to take your babe out ?"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#model-building",
    "href": "slides/slides-06-hyperparameter-optimization.html#model-building",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Model building",
    "text": "Model building\n\nLet‚Äôs define a pipeline\n\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\n\nSuppose we want to try out different hyperparameter values.\n\n\nparameters = {\n    \"max_features\": [100, 200, 400],\n    \"gamma\": [0.01, 0.1, 1.0],\n    \"C\": [0.01, 0.1, 1.0],\n}"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#hyperparameter-optimization-with-loops",
    "href": "slides/slides-06-hyperparameter-optimization.html#hyperparameter-optimization-with-loops",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Hyperparameter optimization with loops",
    "text": "Hyperparameter optimization with loops\n\nDefine a parameter space.\nIterate through possible combinations.\nEvaluate model performance.\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#sklearn-methods",
    "href": "slides/slides-06-hyperparameter-optimization.html#sklearn-methods",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "sklearn methods",
    "text": "sklearn methods\n\nsklearn provides two main methods for hyperparameter optimization\n\nGrid Search\nRandom Search"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#grid-search",
    "href": "slides/slides-06-hyperparameter-optimization.html#grid-search",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Grid Search",
    "text": "Grid Search\n\nCovers all possible combinations from the provided grid.\nCan be parallelized easily.\nIntegrates cross-validation."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#grid-search-example",
    "href": "slides/slides-06-hyperparameter-optimization.html#grid-search-example",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Grid search example",
    "text": "Grid search example\n\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(CountVectorizer(), SVC())\n\nparam_grid = {\n    \"countvectorizer__max_features\": [100, 200, 400],\n    \"svc__gamma\": [0.01, 0.1, 1.0],\n    \"svc__C\": [0.01, 0.1, 1.0],\n}\ngrid_search = GridSearchCV(pipe_svm, \n                  param_grid = param_grid, \n                  n_jobs=-1, \n                  return_train_score=True\n                 )\ngrid_search.fit(X_train, y_train)\ngrid_search.best_score_\n\nnp.float64(0.9782606272997375)"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#random-search",
    "href": "slides/slides-06-hyperparameter-optimization.html#random-search",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Random Search",
    "text": "Random Search\n\nMore efficient than grid search when dealing with large hyperparameter spaces.\nSamples a given number of parameter settings from distributions."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#random-search-example",
    "href": "slides/slides-06-hyperparameter-optimization.html#random-search-example",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Random search example",
    "text": "Random search example\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\npipe_svc = make_pipeline(CountVectorizer(), SVC())\n\nparam_dist = {\n    \"countvectorizer__max_features\": randint(100, 2000), \n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\nrandom_search = RandomizedSearchCV(pipe_svm,                                    \n                  param_distributions = param_dist, \n                  n_iter=10, \n                  n_jobs=-1, \n                  return_train_score=True)\n\n# Carry out the search\nrandom_search.fit(X_train, y_train)\nrandom_search.best_score_\n\nnp.float64(0.982648262796441)"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#pizza-baking-competition-example",
    "href": "slides/slides-06-hyperparameter-optimization.html#pizza-baking-competition-example",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Pizza baking competition example",
    "text": "Pizza baking competition example\nImagine that you participate in pizza baking competition.\n\n\nTraining phase: Collecting recipes and practicing\nValidation phase: Inviting a group of friends and getting feedback"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#overfitting-on-the-validation-set",
    "href": "slides/slides-06-hyperparameter-optimization.html#overfitting-on-the-validation-set",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Overfitting on the validation set",
    "text": "Overfitting on the validation set\n\n\n\nYour friends love your pineapple pizza you hesitantly tried out.\nEncouraged by their enthusiasm, you decide to focus on perfecting this recipe, believing it to be a crowd-pleaser\n\n\n\nCompetition day!\n\nYou confidently present your perfected pineapple pizza, expecting it to be a hit\nThe judges are not impressed. They criticize the choice of pineapple, pointing out that it might not appeal to a general audience."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#overfitting-on-the-validation-set-1",
    "href": "slides/slides-06-hyperparameter-optimization.html#overfitting-on-the-validation-set-1",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Overfitting on the validation set",
    "text": "Overfitting on the validation set\n\nBy focusing solely on the positive feedback from your pineapple-loving friends, you‚Äôve overfitted your pizza to their tastes. This group, however, was not representative of the broader preferences of the competition judges or the general public.\nThe pizza, while perfect for your validation group, failed to generalize across a broader range of tastes, leading to disappointing results in the competition where diverse preferences were expected."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#optimization-bias-1",
    "href": "slides/slides-06-hyperparameter-optimization.html#optimization-bias-1",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Optimization bias",
    "text": "Optimization bias\n\nWhy do we need separate validation and test datasets?"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#mitigating-optimization-bias.",
    "href": "slides/slides-06-hyperparameter-optimization.html#mitigating-optimization-bias.",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Mitigating optimization bias.",
    "text": "Mitigating optimization bias.\n\nCross-validation\nEnsembles\nRegularization and choosing a simpler model"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#iclicker-exercise-6.1",
    "href": "slides/slides-06-hyperparameter-optimization.html#iclicker-exercise-6.1",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "(iClicker) Exercise 6.1",
    "text": "(iClicker) Exercise 6.1\niClicker cloud join link: https://join.iclicker.com/YWOJ\nSelect all of the following statements which are TRUE.\n\n\nIf you get best results at the edges of your parameter grid, it might be a good idea to adjust the range of values in your parameter grid.\n\n\nGrid search is guaranteed to find the best hyperparameter values.\n\n\nIt is possible to get different hyperparameters in different runs of RandomizedSearchCV."
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#questions-for-you",
    "href": "slides/slides-06-hyperparameter-optimization.html#questions-for-you",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Questions for you",
    "text": "Questions for you\n\nYou have a dataset and you give me 1/10th of it. The dataset given to me is rather small and so I split it into 96% train and 4% validation split. I carry out hyperparameter optimization using a single 4% validation split and report validation accuracy of 0.97. Would it classify the rest of the data with similar accuracy?\n\nProbably\nProbably not"
  },
  {
    "objectID": "slides/slides-06-hyperparameter-optimization.html#questions-for-class-discussion",
    "href": "slides/slides-06-hyperparameter-optimization.html#questions-for-class-discussion",
    "title": "DSCI 571 Lecture 6: Hyperparameter Optimization",
    "section": "Questions for class discussion",
    "text": "Questions for class discussion\n\nSuppose you have 10 hyperparameters, each with 4 possible values. If you run GridSearchCV with this parameter grid, how many cross-validation experiments will be carried out?\nSuppose you have 10 hyperparameters and each takes 4 values. If you run RandomizedSearchCV with this parameter grid with n_iter=20, how many cross-validation experiments will be carried out?"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#announcements",
    "href": "slides/slides-06-column-transformer-text-feats.html#announcements",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Announcements",
    "text": "Announcements\n\nHW1 grades have been returned.\nWhere to find slides?\n\nhttps://kvarada.github.io/cpsc330-slides/lecture.html\n\nHW3 is due next week Monday, Sept 29th, 11:59 pm.\n\nIf you‚Äôve started yet, start now.\nYou can work in pairs for this assignment."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#data",
    "href": "slides/slides-06-column-transformer-text-feats.html#data",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Data",
    "text": "Data\n\nX, y = make_blobs(n_samples=100, centers=3, random_state=12, cluster_std=5) # make synthetic data\nX_train_toy, X_test_toy, y_train_toy, y_test_toy = train_test_split(\n    X, y, random_state=5, test_size=0.4) # split it into training and test sets\n# Visualize the training data\nplt.scatter(X_train_toy[:, 0], X_train_toy[:, 1], label=\"Training set\", s=60)\nplt.scatter(\n    X_test_toy[:, 0], X_test_toy[:, 1], color=mglearn.cm2(1), label=\"Test set\", s=60\n)\nplt.legend(loc=\"upper right\")"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-1",
    "href": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 1",
    "text": "Bad methodology 1\n\nWhat‚Äôs wrong with scaling data separately?\n\n\nscaler = StandardScaler() # Creating a scalert object \nscaler.fit(X_train_toy) # Calling fit on the training data \ntrain_scaled = scaler.transform(\n    X_train_toy\n)  # Transforming the training data using the scaler fit on training data\n\nscaler = StandardScaler()  # Creating a separate object for scaling test data\nscaler.fit(X_test_toy)  # Calling fit on the test data\ntest_scaled = scaler.transform(\n    X_test_toy\n)  # Transforming the test data using the scaler fit on test data\n\nknn = KNeighborsClassifier()\nknn.fit(train_scaled, y_train_toy)\nprint(f\"Training score: {knn.score(train_scaled, y_train_toy):.2f}\")\nprint(f\"Test score: {knn.score(test_scaled, y_test_toy):.2f}\") # misleading scores\n\nTraining score: 0.63\nTest score: 0.60"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#scaling-train-and-test-data-separately",
    "href": "slides/slides-06-column-transformer-text-feats.html#scaling-train-and-test-data-separately",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Scaling train and test data separately",
    "text": "Scaling train and test data separately"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-2",
    "href": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 2",
    "text": "Bad methodology 2\n\nWhat‚Äôs wrong with scaling the data together\n\n\n# join the train and test sets back together\nXX = np.vstack((X_train_toy, X_test_toy))\n\nscaler = StandardScaler()\nscaler.fit(XX)\nXX_scaled = scaler.transform(XX)\n\nXX_train = XX_scaled[:X_train_toy.shape[0]]\nXX_test = XX_scaled[X_train_toy.shape[0]:]\n\nknn = KNeighborsClassifier()\nknn.fit(XX_train, y_train_toy)\nprint(f\"Training score: {knn.score(XX_train, y_train_toy):.2f}\")  # Misleading score\nprint(f\"Test score: {knn.score(XX_test, y_test_toy):.2f}\")  # Misleading score\n\nTraining score: 0.63\nTest score: 0.55"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-3",
    "href": "slides/slides-06-column-transformer-text-feats.html#bad-methodology-3",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Bad methodology 3",
    "text": "Bad methodology 3\n\nWhat‚Äôs wrong here?\n\n\nknn = KNeighborsClassifier()\n\nscaler = StandardScaler()\nscaler.fit(X_train_toy)\nX_train_scaled = scaler.transform(X_train_toy)\nX_test_scaled = scaler.transform(X_test_toy)\ncross_val_score(knn, X_train_scaled, y_train_toy)\n\narray([0.25      , 0.5       , 0.58333333, 0.58333333, 0.41666667])"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#improper-preprocessing",
    "href": "slides/slides-06-column-transformer-text-feats.html#improper-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Improper preprocessing",
    "text": "Improper preprocessing"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#proper-preprocessing",
    "href": "slides/slides-06-column-transformer-text-feats.html#proper-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Proper preprocessing",
    "text": "Proper preprocessing"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#common-scenarios-of-data-leakage",
    "href": "slides/slides-06-column-transformer-text-feats.html#common-scenarios-of-data-leakage",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Common scenarios of data leakage ‚õîÔ∏è",
    "text": "Common scenarios of data leakage ‚õîÔ∏è\n\nFitting preprocessing on validation or test data: The preprocessing ‚Äúlearns‚Äù from data it shouldn‚Äôt see, giving your model unfair information.\nFitting preprocessing on the full dataset (train + validation/test):  Even a small peek at validation/test data contaminates the training process.\nFitting preprocessing before cross-validation:  The preprocessing step has already ‚Äúseen‚Äù all the data, so each validation fold is no longer truly unseen."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#avoiding-data-leakage",
    "href": "slides/slides-06-column-transformer-text-feats.html#avoiding-data-leakage",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Avoiding data leakage ‚úÖ",
    "text": "Avoiding data leakage ‚úÖ\n\nEnsure the training process remains completely independent of the validation/test data.\nFit preprocessing steps only on the training data (e.g., imputer learns medians from training set).\nApply the same preprocessing steps to both training and validation/test data.\n\nUse the validation set only for hyperparameter tuning. It must not contribute to model training.\n\nUse sklearn pipelines to automatically handle preprocessing and modeling without breaking the golden rule."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#recap-sklearn-pipelines",
    "href": "slides/slides-06-column-transformer-text-feats.html#recap-sklearn-pipelines",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Recap: sklearn Pipelines",
    "text": "Recap: sklearn Pipelines\n\nPipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\nIf you build a pipeline with preprocessing + model, then during cross-validation:\n\nEach preprocessing step fits and transforms only on the training fold.\nThe same preprocessing steps are then applied (transform only) to the validation fold.\nFinally, the model is trained on the preprocessed training data and evaluated on the preprocessed validation data."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#recap-sklearn-pipelines-1",
    "href": "slides/slides-06-column-transformer-text-feats.html#recap-sklearn-pipelines-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Recap: sklearn Pipelines",
    "text": "Recap: sklearn Pipelines\n\nSimplify the code and improves readability.\nReduce the risk of data leakage by ensuring proper transformation of the training and test sets.\nAutomatically apply transformations in sequence.\nExample:\n\nChaining a StandardScaler with a KNeighborsClassifier model.\n\n\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\n# Correct way to do cross validation without breaking the golden rule. \ncross_val_score(pipe_knn, X_train_toy, y_train_toy) \n\narray([0.25      , 0.5       , 0.5       , 0.58333333, 0.41666667])"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#sklearns-columntransformer",
    "href": "slides/slides-06-column-transformer-text-feats.html#sklearns-columntransformer",
    "title": "Lecture 6: Column transformer and text features",
    "section": "sklearn‚Äôs ColumnTransformer",
    "text": "sklearn‚Äôs ColumnTransformer\n\nUse ColumnTransformer to build all our transformations together into one object\n\n\n\nUse a column transformer with sklearn pipelines."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#iclicker-exercise-6.1",
    "href": "slides/slides-06-column-transformer-text-feats.html#iclicker-exercise-6.1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "(iClicker) Exercise 6.1",
    "text": "(iClicker) Exercise 6.1\nSelect all of the following statements which are TRUE.\n\n\nYou could carry out cross-validation by passing a ColumnTransformer object to cross_validate.\n\n\nAfter applying column transformer, the order of the columns in the transformed data has to be the same as the order of the columns in the original data.\n\n\nAfter applying a column transformer, the transformed data is always going to be of different shape than the original data.\n\n\nWhen you call fit_transform on a ColumnTransformer object, you get a numpy ndarray."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#remarks-on-preprocessing",
    "href": "slides/slides-06-column-transformer-text-feats.html#remarks-on-preprocessing",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Remarks on preprocessing",
    "text": "Remarks on preprocessing\n\nThere is no one-size-fits-all solution in data preprocessing, and decisions often involve a degree of subjectivity.\n\nExploratory data analysis and domain knowledge inform these decisions\n\nAlways consider the specific goals of your project when deciding how to encode features."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#alternative-methods-for-scaling",
    "href": "slides/slides-06-column-transformer-text-feats.html#alternative-methods-for-scaling",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Alternative methods for scaling",
    "text": "Alternative methods for scaling\n\nStandardScaler\n\nGood choice when the column follows a normal distribution or a distribution somewhat like a normal distribution.\n\nMinMaxScaler: Transform each feature to a desired range. Appropriate when\n\nGood choice for features such as human age, where there is a fixed range of values and the feature is uniformly distributed across the range\n\nNormalizer: Works on rows rather than columns. Normalize examples individually to unit norm.\n\nGood choice for frequency-type data\n\nLog scaling\n\nGood choice for features following power law distribution (e.g., counts of followers, salary or income )\n\n‚Ä¶"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding",
    "href": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nOrdinal Encoding: Encodes categorical features as an integer array.\nOne-hot Encoding: Creates binary columns for each category‚Äôs presence.\nSometimes how we encode a specific feature depends upon the context."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding-1",
    "href": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding-1",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nConsider weather feature and its four categories: Sunny (‚òÄÔ∏è), Cloudy (üå•Ô∏è), Rainy (‚õàÔ∏è), Snowy (‚ùÑÔ∏è)\nWhich encoding would you use in each of the following scenarios?\n\nPredicting traffic volume\nPredicting severity of weather-related road incidents"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding-2",
    "href": "slides/slides-06-column-transformer-text-feats.html#ordinal-encoding-vs.-one-hot-encoding-2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Ordinal encoding vs.¬†One-hot encoding",
    "text": "Ordinal encoding vs.¬†One-hot encoding\n\nConsider weather feature and its four categories: Sunny (‚òÄÔ∏è), Cloudy (üå•Ô∏è), Rainy (‚õàÔ∏è), Snowy (‚ùÑÔ∏è)\nPredicting traffic volume: Using one-hot encoding would make sense here because the impact of different weather conditions on traffic volume does not necessarily follow a clear order and different weather conditions could have very distinct effects.\nPredicting severity of weather-related road incidents: An ordinal encoding might be more appropriate if you define your weather categories from least to most severe as this could correlate directly with the likelihood or severity of incidents."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#how-to-deal-with-unseen-categories",
    "href": "slides/slides-06-column-transformer-text-feats.html#how-to-deal-with-unseen-categories",
    "title": "Lecture 6: Column transformer and text features",
    "section": "How to deal with unseen categories?",
    "text": "How to deal with unseen categories?\n\nIn sklearn we can use handle_unknown='ignore' with OneHotEncoder to safely ignore unseen categories during transform.\nIn each of the following scenarios, identify whether it‚Äôs a reasonable strategy or not.\n\nExample 1: Suppose you are building a model to predict customer behavior (e.g., purchase likelihood) based on features like location, device_type, and product_category. During training, you have observed a set of categories for product_category, but in the future, new product categories might be added.\nExample 2: You‚Äôre building a model to predict disease diagnosis based on symptoms, where each symptom is categorized (e.g., fever, headache, nausea)."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#handle_unknown-ignore-of-onehotencoder",
    "href": "slides/slides-06-column-transformer-text-feats.html#handle_unknown-ignore-of-onehotencoder",
    "title": "Lecture 6: Column transformer and text features",
    "section": "handle_unknown = \"ignore\" of OneHotEncoder",
    "text": "handle_unknown = \"ignore\" of OneHotEncoder\n\nReasonable use: When unseen categories are less likely to impact the model‚Äôs prediction accuracy (e.g., product categories in e-commerce), and you prefer to avoid breaking the model.\nNot-so-reasonable use: When unseen categories could provide critical new information that could significantly alter predictions (e.g., in medical diagnostics), ignoring them could result in a poor or dangerous outcome."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#dropif_binary-argument-of-onehotencoder",
    "href": "slides/slides-06-column-transformer-text-feats.html#dropif_binary-argument-of-onehotencoder",
    "title": "Lecture 6: Column transformer and text features",
    "section": "drop=\"if_binary\" argument of OneHotEncoder",
    "text": "drop=\"if_binary\" argument of OneHotEncoder\n\ndrop=‚Äòif_binary‚Äô argument in OneHotEncoder:\nReduces redundancy by dropping one of the columns if the feature is binary."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#categorical-variables-with-too-many-categories",
    "href": "slides/slides-06-column-transformer-text-feats.html#categorical-variables-with-too-many-categories",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Categorical variables with too many categories",
    "text": "Categorical variables with too many categories\n\nStrategies for categorical variables with too many categories:\n\nDimensionality reduction techniques\nBucketing categories into ‚Äòothers‚Äô\nClustering or grouping categories manually\nOnly considering top-N categories\n‚Ä¶"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#dealing-with-text-features",
    "href": "slides/slides-06-column-transformer-text-feats.html#dealing-with-text-features",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Dealing with text features",
    "text": "Dealing with text features\n\nPreprocessing text to fit into machine learning models using text vectorization.\nBag of words representation"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#sklearn-countvectorizer",
    "href": "slides/slides-06-column-transformer-text-feats.html#sklearn-countvectorizer",
    "title": "Lecture 6: Column transformer and text features",
    "section": "sklearn CountVectorizer",
    "text": "sklearn CountVectorizer\n\nUse scikit-learn‚Äôs CountVectorizer to encode text data\nCountVectorizer: Transforms text into a matrix of token counts\nImportant parameters:\n\nmax_features: Control the number of features used in the model\nmax_df, min_df: Control document frequency thresholds\nngram_range: Defines the range of n-grams to be extracted\nstop_words: Enables the removal of common words that are typically uninformative in most applications, such as ‚Äúand‚Äù, ‚Äúthe‚Äù, etc."
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#incorporating-text-features-in-a-machine-learning-pipeline",
    "href": "slides/slides-06-column-transformer-text-feats.html#incorporating-text-features-in-a-machine-learning-pipeline",
    "title": "Lecture 6: Column transformer and text features",
    "section": "Incorporating text features in a machine learning pipeline",
    "text": "Incorporating text features in a machine learning pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipeline = make_pipeline(\n    CountVectorizer(),\n    SVC()\n)"
  },
  {
    "objectID": "slides/slides-06-column-transformer-text-feats.html#iclicker-exercise-6.2",
    "href": "slides/slides-06-column-transformer-text-feats.html#iclicker-exercise-6.2",
    "title": "Lecture 6: Column transformer and text features",
    "section": "(iClicker) Exercise 6.2",
    "text": "(iClicker) Exercise 6.2\nSelect all of the following statements which are TRUE.\n\n\nhandle_unknown=\"ignore\" would treat all unknown categories equally.\n\n\nAs you increase the value for max_features hyperparameter of CountVectorizer the training score is likely to go up.\n\n\nSuppose you are encoding text data using CountVectorizer. If you encounter a word in the validation or the test split that‚Äôs not available in the training data, we‚Äôll get an error.\n\n\nIn the code below, inside cross_validate, each fold might have slightly different number of features (columns) in the fold.\n\n\npipe = (CountVectorizer(), SVC())\ncross_validate(pipe, X_train, y_train)"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#learning-outcomes",
    "href": "slides/slides-02-terminology-decision-trees.html#learning-outcomes",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "üéØ Learning Outcomes",
    "text": "üéØ Learning Outcomes\nBy the end of this lesson, you will be able to:\n\nDefine key machine learning terminology:\nfeatures, targets, predictions, training, error, classification vs.¬†regression, supervised vs.¬†unsupervised learning, hyperparameters vs.¬†parameters, baselines, decision boundaries\nBuild a simple machine learning model in scikit-learn, explaining the fit‚Äìpredict workflow and evaluating performance with the score method\nDescribe at a high level how decision trees are trained (fitting) and how they make predictions\nImplement and visualize decision trees in scikit-learn using DecisionTreeClassifier and DecisionTreeRegressor"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#announcements",
    "href": "slides/slides-02-terminology-decision-trees.html#announcements",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Announcements",
    "text": "Announcements\n\nThings due this week\n\nHomework 1 (hw1): Due Sept 09 11:59pm\n\nHomework 2 (hw2) has been released (Due: Sept 15, 11:59pm)\n\nThere is some autograding in this homework.\n\nYou can find the tentative due dates for all deliverables here.\nPlease monitor Piazza (especially pinned posts and instructor posts) for announcements.\nI‚Äôll assume that you‚Äôve watched the pre-lecture videos."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#recap-what-is-ml",
    "href": "slides/slides-02-terminology-decision-trees.html#recap-what-is-ml",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: What is ML?",
    "text": "Recap: What is ML?\n\nML uses data to build models that find patterns, make predictions, or generate content.\nIt helps computers learn from data to make decisions.\nNo one model works for every situation."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#iclicker-2.1-ml-or-not",
    "href": "slides/slides-02-terminology-decision-trees.html#iclicker-2.1-ml-or-not",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.1: ML or not",
    "text": "iClicker 2.1: ML or not\niClicker join link: https://join.iclicker.com/FZMQ\nSelect all of the following statements which are suitable problems for machine learning.\n\n\nIdentifying objects within digital images, such as facial recognition in security systems or categorizing images based on content.\n\n\nDetermining if individuals meet the necessary criteria for government or financial services based on strict guidelines.\n\n\nIdentifying unusual patterns that may indicate fraudulent transactions in banking and finance.\n\n\nAutomatically analyzing images from MRIs, CT scans, or X-rays to detect abnormalities like tumors or fractures.\n\n\nAddressing mental health issues where human empathy, understanding, and adaptability are key."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#therapists-using-chatgpt-secretly",
    "href": "slides/slides-02-terminology-decision-trees.html#therapists-using-chatgpt-secretly",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Therapists using ChatGPT secretly üòî",
    "text": "Therapists using ChatGPT secretly üòî"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#recap-when-is-ml-suitable",
    "href": "slides/slides-02-terminology-decision-trees.html#recap-when-is-ml-suitable",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: When is ML suitable?",
    "text": "Recap: When is ML suitable?\n\nML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\nRule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making.\nHuman experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#recap-supervised-learning",
    "href": "slides/slides-02-terminology-decision-trees.html#recap-supervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: Supervised learning",
    "text": "Recap: Supervised learning\n\nWe wish to find a model function \\(f\\) that relates \\(X\\) to \\(y\\).\nWe use the model function to predict targets of new examples.\n\n\n\n\n\n\nIn the first part of this course, we‚Äôll focus on supervised machine learning."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#unsupervised-learning",
    "href": "slides/slides-02-terminology-decision-trees.html#unsupervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nTraining data consists of observations \\(X\\) without any corresponding targets.\nUnsupervised learning could be used to group similar things together in \\(X\\) or to find underlying structure in the data."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#iclicker-2.2-supervised-vs-unsupervised",
    "href": "slides/slides-02-terminology-decision-trees.html#iclicker-2.2-supervised-vs-unsupervised",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.2: Supervised vs unsupervised",
    "text": "iClicker 2.2: Supervised vs unsupervised\nClicker cloud join link:\nSelect all of the following statements which are examples of supervised machine learning\n\n\nFinding groups of similar properties in a real estate data set.\n\n\nPredicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n\n\nGrouping articles on different topics from different news sources (something like the Google News app).\n\n\nDetecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n\n\nGiven some measure of employee performance, identify the key factors which are likely to influence their performance."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#iclicker-2.3-classification-vs.-regression",
    "href": "slides/slides-02-terminology-decision-trees.html#iclicker-2.3-classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.3: Classification vs.¬†Regression",
    "text": "iClicker 2.3: Classification vs.¬†Regression\nClicker cloud join link:\nSelect all of the following statements which are examples of regression problems\n\n\nPredicting the price of a house based on features such as number of bedrooms and the year built.\n\n\nPredicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n\n\nPredicting percentage grade in CPSC 330 based on past grades.\n\n\nPredicting whether you should bicycle tomorrow or not based on the weather forecast.\n\n\nPredicting appropriate thermostat temperature based on the wind speed and the number of people in a room."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#todays-focus",
    "href": "slides/slides-02-terminology-decision-trees.html#todays-focus",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Today‚Äôs focus",
    "text": "Today‚Äôs focus\n\nML Terminology\nUsing sklearn to build a simple supervised ML model\nIntuition of Decision Trees"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#framework",
    "href": "slides/slides-02-terminology-decision-trees.html#framework",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Framework",
    "text": "Framework\n\nThere are many frameworks to do do machine learning.\nWe‚Äôll mainly be using scikit-learn framework."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#running-example",
    "href": "slides/slides-02-terminology-decision-trees.html#running-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Running example",
    "text": "Running example\nImagine you‚Äôre in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people.\n\nCan you think of relevant features for this problem?"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#toy-job-happinees-dataset",
    "href": "slides/slides-02-terminology-decision-trees.html#toy-job-happinees-dataset",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Toy job happinees dataset",
    "text": "Toy job happinees dataset\nHere are the first few rows of a toy dataset.\n\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#features-target-example",
    "href": "slides/slides-02-terminology-decision-trees.html#features-target-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Features, target, example",
    "text": "Features, target, example\n\nTerminologyData\n\n\n\nWhat are the features \\(X\\)?\n\nfeatures = inputs = predictors = explanatory variables = regressors = independent variables = covariates\n\nWhat‚Äôs the target \\(y\\)?\n\ntarget = output = outcome = response variable = dependent variable = labels\n\nWhat is an example?\n\n\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#classification-vs.-regression",
    "href": "slides/slides-02-terminology-decision-trees.html#classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs.¬†Regression",
    "text": "Classification vs.¬†Regression\n\nIs this a classification problem or a regression problem?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#optional-inference-vs.-prediction",
    "href": "slides/slides-02-terminology-decision-trees.html#optional-inference-vs.-prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "(Optional) Inference vs.¬†Prediction",
    "text": "(Optional) Inference vs.¬†Prediction\n\n\n\nInference asks: Why does something happen?\n\nGoal: understand and quantify the relationship between variables\n\nOften involves estimating model parameters and testing hypotheses\n\nExample: Which factors influence happiness, and by how much?\n\n\n\n\nPrediction asks: What will happen?\n\nGoal: accurately predict the target without needing to fully explain the relationships\n\nExample: Will you be happy in a particular job?\n\n\n\nOf course these goals are related, and in many situations we need both."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#training",
    "href": "slides/slides-02-terminology-decision-trees.html#training",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training",
    "text": "Training\n\nIn supervised ML, the goal is to learn a function that maps input features (\\(X\\)) to a target (\\(y\\)).\nThe relationship between \\(X\\) and \\(y\\) is often complex, making it difficult to define mathematically.\nWe use algorithms to approximate this complex relationship between \\(X\\) and \\(y\\).\nTraining is the process of applying an algorithm to learn the best function (or model) that maps \\(X\\) to \\(y\\).\nIn this course, I‚Äôll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#error-and-accuracy",
    "href": "slides/slides-02-terminology-decision-trees.html#error-and-accuracy",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Error and accuracy",
    "text": "Error and accuracy\n\nMachine learning models are not perfect‚Äîthey will make mistakes.\n\nTo judge whether a model is useful, we need to track its performance.\n\nFor classification problems, the most common (and default in sklearn) metric is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of examples}}\n\\]"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#separating-x-and-y",
    "href": "slides/slides-02-terminology-decision-trees.html#separating-x-and-y",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Separating \\(X\\) and \\(y\\)",
    "text": "Separating \\(X\\) and \\(y\\)\n\nIn order to train a model we need to separate \\(X\\) and \\(y\\) from the dataframe.\n\n\nX = toy_happiness_df.drop(columns=[\"happy?\"]) # Extract the feature set by removing the target column \"happy?\"\ny = toy_happiness_df[\"happy?\"] # Extract the target variable \"happy?\""
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#baseline",
    "href": "slides/slides-02-terminology-decision-trees.html#baseline",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Baseline",
    "text": "Baseline\n\nLet‚Äôs try a simplest algorithm of predicting the most popular target!\n\n\nfrom sklearn.dummy import DummyClassifier\nmodel = DummyClassifier(strategy=\"most_frequent\") # Initialize the DummyClassifier to always predict the most frequent class\nmodel.fit(X, y) # Train the model on the feature set X and target variable y\ntoy_happiness_df['dummy_predictions'] = model.predict(X) # Add the predicted values as a new column in the dataframe\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\ndummy_predictions\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\nHappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\nHappy\n\n\n2\n1\n80000\n1\n0\nHappy\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy\nHappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#which-question-is-more-effective",
    "href": "slides/slides-02-terminology-decision-trees.html#which-question-is-more-effective",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Which question is more effective?",
    "text": "Which question is more effective?\n\nVisualData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#what-are-we-trying-to-learn",
    "href": "slides/slides-02-terminology-decision-trees.html#what-are-we-trying-to-learn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "What are we trying to learn?",
    "text": "What are we trying to learn?\n\nWe want to learn which questions to ask and in what order.\nHow many possible questions could we ask with these features?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#decision-tree-training-high-level",
    "href": "slides/slides-02-terminology-decision-trees.html#decision-tree-training-high-level",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree Training (high level)",
    "text": "Decision tree Training (high level)\n\nTraining a decision tree is a search process: we look for the ‚Äúbest‚Äù tree among many possible ones.\nThere are different algorithms for learning trees. Check this out.\nAt each step, we evaluate candidate questions using measures such as:\n\nInformation gain\nGini index\n\nThe goal is to split the data into groups with greater certainty (more homogeneous outcomes)."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#decision-tree-with-sklearn",
    "href": "slides/slides-02-terminology-decision-trees.html#decision-tree-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree with sklearn",
    "text": "Decision tree with sklearn\nLet‚Äôs train a simple decision tree on our toy dataset using sklearn\n\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1) # Create a class object\nmodel.fit(X, y)\nplot_tree(model, filled=True, feature_names = X.columns, class_names=[\"Happy\", \"Unhappy\"], impurity = False, fontsize=12);"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#prediction",
    "href": "slides/slides-02-terminology-decision-trees.html#prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction",
    "text": "Prediction\n\nGiven a new example, how does a decision tree predict the class of this example?\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#prediction-with-sklearn",
    "href": "slides/slides-02-terminology-decision-trees.html#prediction-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction with sklearn",
    "text": "Prediction with sklearn\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, free_coffee = 0, vegan_boss = 1,\n\n\n\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\nplot_tree(model, filled=True, feature_names = X.columns, class_names = [\"Happy\", \"Unhappy\"], impurity = False, fontsize=9);\n\nModel prediction:  ['Unhappy']"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#parameters-vs.-hyperparameters",
    "href": "slides/slides-02-terminology-decision-trees.html#parameters-vs.-hyperparameters",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Parameters vs.¬†Hyperparameters",
    "text": "Parameters vs.¬†Hyperparameters\n\nParameters\n\nThe questions (features and thresholds) used to split the data at each node.\nExample: salary &lt;= 75000 at the root node\n\n\nHyperparameters\n\nSettings that control tree growth, like max_depth, which limits how deep the tree can go."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#decision-boundary",
    "href": "slides/slides-02-terminology-decision-trees.html#decision-boundary",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary",
    "text": "Decision boundary\n\nA decision boundary is the line, curve, or surface that separates classes.\nPoints on one side \\(\\rightarrow\\) Model predicts Class Happy\n\nPoints on the other side \\(\\rightarrow\\) Model predicts Class Unhappy"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#decision-boundary-with-max_depth1",
    "href": "slides/slides-02-terminology-decision-trees.html#decision-boundary-with-max_depth1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=1",
    "text": "Decision boundary with max_depth=1"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#decision-boundary-with-max_depth2",
    "href": "slides/slides-02-terminology-decision-trees.html#decision-boundary-with-max_depth2",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=2",
    "text": "Decision boundary with max_depth=2"
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#iclicker-2.4-baselines-and-decision-trees",
    "href": "slides/slides-02-terminology-decision-trees.html#iclicker-2.4-baselines-and-decision-trees",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "iClicker 2.4: Baselines and Decision trees",
    "text": "iClicker 2.4: Baselines and Decision trees\niClicker cloud join link: https://join.iclicker.com/FZMQ\nSelect all of the following statements which are TRUE.\n\n\nChange in features (i.e., binarizing features above) would change DummyClassifier predictions.\n\n\npredict takes only X as argument whereas fit and score take both X and y as arguments.\n\n\nFor the decision tree algorithm to work, the feature values must be binary.\n\n\nThe prediction in a decision tree works by routing the example from the root to the leaf."
  },
  {
    "objectID": "slides/slides-02-terminology-decision-trees.html#summary",
    "href": "slides/slides-02-terminology-decision-trees.html#summary",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Summary",
    "text": "Summary\n\nTerminology\nsklearn basic steps\nDecision tree intuition"
  }
]